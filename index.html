<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-MoE-Rag-Peft-RLHF-DPO" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T18:16:27.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/">MoE_Rag_Peft_RLHF_DPO</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="零"><a href="#零" class="headerlink" title="零"></a>零</h1><p>本篇就是个杂记,杂烩涉猎一番,不精不深, 故而也不分不分章节序号了</p>
<h1 id="MoE-Mixture-of-Experts"><a href="#MoE-Mixture-of-Experts" class="headerlink" title="MoE(Mixture of Experts)"></a>MoE(Mixture of Experts)</h1><blockquote>
<p>混合专家模型（Mixture of Experts，简称MOE）是一种机器学习的集成技术，由多个专家模型（Experts）和一个门控网络（Gating Network）组成。这种模型的核心思想是将复杂的问题分解成多个子问题，每个子问题由一个专家模型处理，而门控网络则负责决定每个输入样本应该由哪个专家处理。</p>
</blockquote>
<p><img src="./img/MoE.png" alt="MoE"></p>
<p>MoE的具体实现可以是多个FFN, 也可以更复杂(比如垒出层级的gate+FFN). 同时由此可见, expert层确实是模型权重的大头. </p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><blockquote>
<ul>
<li><p>能够在远少于稠密模型所需的计算资源下进行有效的预训练。</p>
</li>
<li><p>这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。</p>
</li>
<li><p>特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p>
</li>
</ul>
</blockquote>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul>
<li>在微调阶段往往面临<strong>泛化能力不足</strong>的问题，长期以来易于引发<strong>过拟合</strong>现象。</li>
<li><strong>推理时参数量过大</strong>(所有expert都得上内存), 因此对内存的需求非常高</li>
<li><strong>batch大小的不均匀分配</strong>(不会均匀分给每一个expert)和<strong>资源利用效率不高</strong>(稠密模型时刻负载都会很高, 而MoE由于其特性很难做到)</li>
</ul>
<h2 id="典型解决办法"><a href="#典型解决办法" class="headerlink" title="典型解决办法"></a>典型解决办法</h2><ul>
<li><strong>更强的内部正则化</strong>:<ul>
<li><blockquote>
<p>例如，可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p>
</blockquote>
</li>
<li>Router z-loss <strong>(回头仔细看看)</strong><blockquote>
<p>在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 logits 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>微调阶段</strong>:<ul>
<li><strong>不应</strong>冻结所有非专家层的权重。实践中，这会导致性能大幅下降(MoE层占据了网络的主要部分, 和Qwen-VL类似, 直接把vision model冻了可能把模型调“坏”);<br><strong>应</strong>仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</li>
<li>较小的batchsize大小和较高的lr</li>
</ul>
</li>
<li><p><strong>推理时参数量过大</strong>:</p>
<ul>
<li><blockquote>
<p>预先蒸馏: 将 MoE 模型蒸馏回其对应的稠密模型。</p>
</blockquote>
</li>
<li><blockquote>
<p>专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。</p>
</blockquote>
</li>
<li><blockquote>
<p>任务级别路由: 路由器被修改为将整个句子或任务直接路由到一个专家。</p>
</blockquote>
<p>(最后一条有点费解,<strong>回头仔细看</strong>)</p>
</li>
</ul>
</li>
<li><p><strong>资源利用效率不高</strong></p>
<ul>
<li><blockquote>
<p>在experts并行中，experts被放置在不同的node上，每个node处理不同批次的训练样本。对于非 MoE 层，experts并行的行为与数据并行相同。对于 MoE 层，序列中的tokens被发送到拥有所需expert的节点。(有点类似model parallel)</p>
</blockquote>
<p><img src="./img/MoE-expert_parallel.png" alt="expert parallel"></p>
</li>
<li><blockquote>
<p>FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p>
</blockquote>
</li>
<li><blockquote>
<p>Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>batch大小的不均匀分配</strong><br>采用一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E)，门控网络通常为一个带有softmax函数的简单网络。</p>
<ul>
<li>MOE层<br>$y = \sum_{i=1}^n G(x)_i E_i(x)$</li>
<li>门控网络<br>$G_\sigma(x) = \text{Softmax}(x \cdot W_g)$</li>
<li>tok门控<ul>
<li>添加噪声<br>$H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_\text{noise})_i)$</li>
<li>保留前K值<br>$\text{KeepTopK}(v, k)_i =\begin{cases} v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v, \-\infty &amp; \text{otherwise.}\end{cases}$</li>
<li>门控网络<br>G(x) &amp;= \text{Softmax}(\text{KeepTopK}(H(x), k))</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" data-id="cm4boxx5g0002ykfyd0g740cd" data-title="MoE_Rag_Peft_RLHF_DPO" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Olmo及dolma实践" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T15:13:57.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/">Olmo及dolma实践</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="零、导言"><a href="#零、导言" class="headerlink" title="零、导言"></a>零、导言</h3><h4 id="0-0-模型训练的总体流程"><a href="#0-0-模型训练的总体流程" class="headerlink" title="0.0 模型训练的总体流程"></a><strong>0.0 模型训练的总体流程</strong></h4><p>模型训练的完整流程通常包括以下几个步骤：</p>
<ol>
<li><strong>数据准备</strong>：将原始数据通过数据清洗与处理，转化为可用于模型训练的高质量训练数据。Dolma 工具在此阶段发挥重要作用，确保数据经过标记、去重、过滤等处理步骤，形成高质量的训练语料。</li>
<li><strong>词元切分</strong>：对训练数据进行 Tokenization，将文本数据转换为模型能够理解和处理的 tokens，生成可供训练的输入数据。</li>
<li><strong>预训练</strong>：使用训练数据对 OLMo 模型进行预训练，模型在大量数据上学习语言的基本规律和特征。</li>
<li><strong>指令微调</strong>：对预训练后的模型进行指令微调，使其能够更好地理解和执行特定任务指令。</li>
<li><strong>模型评测</strong>：使用 OpenCompass 工具对模型进行全面评测，包括对模型的语言理解、生成能力等多方面的评价，以便了解模型性能并进行进一步优化。</li>
</ol>
<h3 id="一、Docker使用与环境搭建"><a href="#一、Docker使用与环境搭建" class="headerlink" title="一、Docker使用与环境搭建"></a><strong>一、Docker使用与环境搭建</strong></h3><h4 id="1-1-Docker简介"><a href="#1-1-Docker简介" class="headerlink" title="1.1 Docker简介"></a><strong>1.1 Docker简介</strong></h4><p><strong>容器技术的定义</strong>：容器技术通过在物理主机操作系统上创建一个个孤立的分组（即容器），将应用程序及其依赖项打包在一个独立的容器中，使其能够在任何支持容器的环境中运行，而不受底层系统的影响。Docker 是目前主流的容器技术之一，其他类似的容器技术还有 Kubernetes (K8s) 等。</p>
<p><strong>Docker的特点</strong>：</p>
<ul>
<li><strong>轻量级</strong>：容器共享主机的操作系统内核，避免了虚拟机的资源开销，启动更快，占用更少资源。</li>
<li><strong>可移植性</strong>：容器技术允许应用程序在不同的云平台和数据中心中轻松迁移，确保在不同环境中运行时不会出现兼容性问题。</li>
<li><strong>一致性</strong>：容器保证了应用程序在不同环境中的一致性运行，减少了“在我的机器上能够运行”的问题，使得开发、测试、生产等环境的统一性得以保证。</li>
</ul>
<h4 id="1-2-Docker的基本概念"><a href="#1-2-Docker的基本概念" class="headerlink" title="1.2 Docker的基本概念"></a><strong>1.2 Docker的基本概念</strong></h4><p>Docker 包括三个基本概念：</p>
<ol>
<li><strong>镜像（Image）</strong>：Docker 镜像相当于一个 root 文件系统。例如，官方镜像 <code>ubuntu:16.04</code> 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。镜像是静态的，作为容器的模板存在。</li>
<li><strong>容器（Container）</strong>：容器是镜像的运行实例，类似于面向对象编程中的类和对象。镜像是静态的定义，容器是镜像在运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</li>
<li><strong>仓库（Repository）</strong>：仓库是用于保存镜像的地方，类似于代码控制中心。在仓库中，可以存储、获取、分发 Docker 镜像。</li>
</ol>
<h4 id="1-3-Docker安装与使用"><a href="#1-3-Docker安装与使用" class="headerlink" title="1.3 Docker安装与使用"></a><strong>1.3 Docker安装与使用</strong></h4><p><strong>安装Docker</strong>：请参考官方网站的安装指南，或者访问 <a target="_blank" rel="noopener" href="https://www.runoob.com/docker/ubuntu-docker-install.html">菜鸟教程Docker安装</a> 完成 Docker 的安装。</p>
<p><strong>常用操作命令</strong>：</p>
<ol>
<li><p><strong>下载镜像</strong>：通过命令 <code>docker pull &lt;镜像名&gt;</code> 下载所需的 Docker 镜像。例如，<code>docker pull ubuntu:16.04</code>。</p>
</li>
<li><p><strong>查看镜像</strong>：使用 <code>docker images</code> 查看已经下载到本地的镜像列表。</p>
</li>
<li><p><strong>启动容器</strong>：使用 <code>docker run [OPTIONS] &lt;image&gt; [COMMAND] [ARG...]</code> 从镜像启动一个新的容器。</p>
<ul>
<li>常用的 <code>OPTIONS</code> 参数：<ul>
<li><code>--gpus</code>：配置容器内部使用的 GPU，示例如 <code>--gpus all</code>。</li>
<li><code>-v &lt;host目录&gt;:&lt;容器目录&gt;</code>：挂载主机目录到容器中，例如挂载数据集等。在容器内部修改挂载的目录或文件会直接反映到宿主机。</li>
<li><code>--privileged</code>：使用该参数，容器内的 root 用户拥有真正的 root 权限。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="1-4-容器内部环境的安装与配置"><a href="#1-4-容器内部环境的安装与配置" class="headerlink" title="1.4 容器内部环境的安装与配置"></a><strong>1.4 容器内部环境的安装与配置</strong></h4><p>在启动并进入容器后，可以对容器内部进行各种环境的配置，例如安装 Anaconda 和其他必要的工具。</p>
<p><strong>步骤</strong>：</p>
<ol>
<li><strong>安装Anaconda</strong>：下载 Anaconda 安装文件，然后运行 <code>bash Anaconda****.sh</code> 进行安装。</li>
<li><strong>创建 Python 环境</strong>：使用 Anaconda 创建一个指定 Python 版本的环境，例如 <code>conda create –name myenv python=3.8</code>，然后使用 <code>conda activate myenv</code> 激活环境。</li>
<li><strong>安装Dolma</strong>：在已激活的 Anaconda 环境中，运行 <code>pip install dolma</code> 安装 Dolma 工具包。</li>
<li><strong>安装OLMo</strong>：参照 OLMo 的 GitHub 指导文档进行安装，并确保所使用的 PyTorch 版本与 CUDA 版本匹配，以获得最佳性能。</li>
</ol>
<p><strong>环境安装验证</strong>：完成所有安装后，可运行相关命令（例如 <code>python</code>、<code>pip list</code> 等）检查环境配置是否正确。</p>
<h4 id="1-5-容器的管理与镜像保存"><a href="#1-5-容器的管理与镜像保存" class="headerlink" title="1.5 容器的管理与镜像保存"></a><strong>1.5 容器的管理与镜像保存</strong></h4><p><strong>将容器放到后台运行</strong>：</p>
<ul>
<li>使用快捷键 <code>Ctrl + P + Q</code> 将当前容器放到后台运行，而不停止容器的执行。</li>
</ul>
<p><strong>查看正在运行的容器</strong>：</p>
<ul>
<li>通过 <code>docker ps</code> 查看当前运行的容器。如果容器已停止，可使用 <code>docker ps -a</code> 查看所有容器。</li>
</ul>
<p><strong>重新启动和进入容器</strong>：</p>
<ul>
<li>使用 <code>docker start &lt;容器ID&gt;</code> 启动已停止的容器。</li>
<li>使用 <code>docker attach &lt;容器ID&gt;</code> 进入正在运行的容器。</li>
</ul>
<p><strong>保存容器为镜像</strong>：</p>
<ul>
<li>通过 <code>docker commit &lt;容器ID&gt; &lt;镜像名&gt;</code> 将已配置好环境的容器保存为一个新的镜像。</li>
</ul>
<p><strong>保存镜像为离线文件</strong>：</p>
<ul>
<li>使用 <code>docker save -o &lt;文件名&gt;.tar &lt;镜像名&gt;:&lt;tag&gt;</code> 将镜像保存为 <code>.tar</code> 文件，便于在其他设备上使用。</li>
</ul>
<p><strong>加载离线镜像</strong>：</p>
<ul>
<li>使用 <code>docker load -i &lt;文件名&gt;.tar</code> 载入离线镜像，方便在新的环境中快速搭建所需环境。</li>
</ul>
<h3 id="二、数据收集与清洗"><a href="#二、数据收集与清洗" class="headerlink" title="二、数据收集与清洗"></a><strong>二、数据收集与清洗</strong></h3><h4 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a><strong>2.1 数据准备</strong></h4><ul>
<li><strong>原料</strong>：原始文本数据是进行数据清洗与处理的基础，来源包括各种大型语料库和数据集。</li>
<li><strong>工具</strong>：Dolma 是一个高性能的数据清洗工具包，提供了完整的数据处理与清洗流程。</li>
<li><strong>步骤</strong>：<ol>
<li><strong>收集数据</strong>：从不同来源收集原始数据，例如 Common Crawl、The Stack、Wikipedia、Wikibooks 等。</li>
<li><strong>数据处理与清洗</strong>：使用 Dolma 工具对数据进行标记、去重、筛选与混合等处理。</li>
<li><strong>词元切分</strong>：将清洗后的文本进行分词，生成可供模型训练的 tokens。</li>
</ol>
</li>
<li><strong>结果</strong>：完成数据清洗与处理后，最终获得可供模型训练使用的 tokens，规模约在 100B 左右。</li>
</ul>
<h4 id="2-2-Dolma的特点"><a href="#2-2-Dolma的特点" class="headerlink" title="2.2 Dolma的特点"></a><strong>2.2 Dolma的特点</strong></h4><p>Dolma 是一款强大且灵活的数据处理工具包，具备以下主要特点：</p>
<ul>
<li><strong>高性能</strong>：能够并行处理包含数十亿个文档的数据集，支持高效的数据清洗与预处理。</li>
<li><strong>可移植性</strong>：Dolma 可在单机、集群或云计算环境中运行，灵活性强。</li>
<li><strong>内置标记器</strong>：内置多种标记器，包括语言检测、毒性检测、困惑度评分等，提供对文本的多维度分析和过滤。</li>
<li><strong>快速重复数据删除</strong>：采用 Rust 实现的 Bloom 过滤器，能够高效去除数据集中的重复文档，比其他方法快得多。</li>
<li><strong>可扩展</strong>：Dolma 设计为可扩展，可以根据需要使用自定义标记器。</li>
<li><strong>云支持</strong>：Dolma 支持从本地磁盘和 AWS S3 兼容位置读取和写入数据，便于云端与本地数据的交互。</li>
</ul>
<h4 id="2-3-Dolma数据处理流程"><a href="#2-3-Dolma数据处理流程" class="headerlink" title="2.3 Dolma数据处理流程"></a><strong>2.3 Dolma数据处理流程</strong></h4><p>使用 Dolma 进行数据集管理通常分为四个步骤：</p>
<ol>
<li><strong>标记数据</strong>：使用内置标记器对数据集中的文档进行标记，例如标记文档的语言、毒性等属性。</li>
<li><strong>重复数据删除</strong>：根据文档内容或元数据对文档进行去重。</li>
<li><strong>混合数据</strong>：根据文档的属性值，删除或过滤不符合要求的文档，并按比例混合不同来源的数据。</li>
<li><strong>词元切分</strong>：使用与 HuggingFace 兼容的 Tokenizer 对文档进行分词处理，生成可供训练使用的 tokens。</li>
</ol>
<h4 id="2-4-数据处理“菜谱”"><a href="#2-4-数据处理“菜谱”" class="headerlink" title="2.4 数据处理“菜谱”"></a><strong>2.4 数据处理“菜谱”</strong></h4><p>Dolma 工具包针对不同数据类型和数据来源提供了特定的数据处理“菜谱”：</p>
<ul>
<li><p><strong>网页（CC or C4）</strong>：</p>
<ul>
<li><strong>语种</strong>：先检测语言，确保是目标语种。</li>
<li><strong>去重</strong>：两次去重处理，包括段落去重和 URL 去重。</li>
<li><strong>质量过滤</strong>：过滤掉没有终止符的文本、Gopher 过滤规则、重复 token 超过 100 次的文档。</li>
<li><strong>内容过滤</strong>：检测并过滤掉毒性内容和 PII（Personal Identifiable Information）信息。</li>
</ul>
</li>
<li><p><strong>论坛（Reddit）</strong>：</p>
<ul>
<li><strong>语种检测</strong>：确保文本属于目标语言。</li>
<li><strong>质量过滤</strong>：过滤掉评论过短、点赞数少的内容。</li>
<li><strong>内容过滤</strong>：检测并去除包含被禁止、毒性、NSFW 内容的评论。</li>
</ul>
</li>
<li><p><strong>论文（Semantic Scholar）</strong>：</p>
<ul>
<li><strong>语种检测</strong>：确保内容是目标语言。</li>
<li><strong>质量过滤</strong>：过滤掉含有重复 token 超过 100 次的文档。</li>
</ul>
</li>
<li><p><strong>代码（The Stack）</strong>：</p>
<ul>
<li><strong>编程语种</strong>：根据需要筛选特定的编程语言。</li>
<li><strong>质量过滤</strong>：移除序言部分的版权声明，遵循 RedPajama v1 和 Starcoder 过滤规则。</li>
<li><strong>内容过滤</strong>：检测 PII 信息（如邮箱、电话、IP 地址），若文档中包含 6 个以上 PII 信息，移除整个语料。</li>
</ul>
</li>
<li><p><strong>维基（Wikipedia &amp; Wikibooks）</strong>：</p>
<ul>
<li>移除少于 25 个 utf-8 字符的网页。</li>
</ul>
</li>
<li><p><strong>书籍（Gutenberg）</strong>：</p>
<ul>
<li>对每段内容进行语言分类，如果整本书的平均语言得分低于 0.5，则移除。</li>
<li>移除少于 25 个 utf-8 字符的网页，并过滤掉重复 token 超过 100 次的内容。</li>
</ul>
</li>
</ul>
<h4 id="2-5-数据格式"><a href="#2-5-数据格式" class="headerlink" title="2.5 数据格式"></a><strong>2.5 数据格式</strong></h4><p>Dolma 对数据格式有严格要求，以确保数据在处理和训练中具有一致性：</p>
<ol>
<li><p><strong>id字段</strong>：该字段非常重要，用于追溯每个版本的每个文档到原始源文档，确保文档在不同版本中保持唯一性。id 在同一数据源内必须唯一。</p>
</li>
<li><p><strong>metadata字段</strong>：包含任何特定于源的信息，例如代码许可证、论文标识符（DOI、arXiv ID 等）。应尽可能保留源特定标识符，以确保数据的完整性和可追溯性。</p>
</li>
</ol>
<p>Dolma 的标准格式如下，数据清洗前应符合以下结构：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;12345&quot;</span><span class="punctuation">,</span>           <span class="comment">// 必填：来源特定的唯一标识符，用于标记该文档</span></span><br><span class="line">    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;示例文本内容&quot;</span><span class="punctuation">,</span>  <span class="comment">// 必填：文档的文本内容</span></span><br><span class="line">    <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;common-crawl&quot;</span><span class="punctuation">,</span> <span class="comment">// 必填：数据来源，例如 peS2o、common-crawl 等</span></span><br><span class="line">    <span class="attr">&quot;added&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2024-09-23T12:34:56Z&quot;</span><span class="punctuation">,</span> <span class="comment">// 可选：数据被 AI2 获取的时间戳</span></span><br><span class="line">    <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2024-01-01T08:00:00Z&quot;</span><span class="punctuation">,</span> <span class="comment">// 可选：原始文档创建的时间戳（如不可用，最好做出合理的推断）</span></span><br><span class="line">    <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span>            <span class="comment">// 可选：与数据源相关的特定元数据信息</span></span><br><span class="line">        <span class="attr">&quot;author&quot;</span><span class="punctuation">:</span> <span class="string">&quot;张三&quot;</span><span class="punctuation">,</span>    <span class="comment">// 示例：作者信息</span></span><br><span class="line">        <span class="attr">&quot;keywords&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;示例&quot;</span><span class="punctuation">,</span> <span class="string">&quot;文本&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="comment">// 示例：关键词</span></span><br><span class="line">        <span class="attr">&quot;license&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CC BY-SA&quot;</span> <span class="comment">// 示例：数据的许可信息</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>Dolma流程格式：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;attributes&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;toxicity&quot;</span><span class="punctuation">:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></p>
<h4 id="2-6-数据处理与过滤命令"><a href="#2-6-数据处理与过滤命令" class="headerlink" title="2.6 数据处理与过滤命令"></a><strong>2.6 数据处理与过滤命令</strong></h4><p>在实际数据清洗中，Dolma 提供了一系列的命令用于处理和过滤数据【40】：</p>
<ol>
<li><strong>dolma tag</strong>：使用标记器（taggers）对文档进行标记，例如标记语言、质量等属性。</li>
<li><strong>dolma dedupe</strong>：创建 Bloom 过滤器索引文件，对文本进行去重处理。</li>
<li><strong>dolma mix</strong>：根据标签筛选并按比例混合训练数据。</li>
<li><strong>dolma tokens</strong>：使用 Huggingface Tokenizer 对文档进行分词处理。</li>
</ol>
<h4 id="2-7-词元切分（Tokenization）"><a href="#2-7-词元切分（Tokenization）" class="headerlink" title="2.7 词元切分（Tokenization）"></a><strong>2.7 词元切分（Tokenization）</strong></h4><ul>
<li><strong>原料准备</strong>：训练语料一般为 JSONL 格式文件，使用 gz 等形式进行压缩。</li>
<li><strong>Tokenizer</strong>：可重用已有的 tokenizer，或者使用 BPE、SentencePiece 等重新训练一个新的 tokenizer。</li>
<li><strong>输出</strong>：以 npy 格式保存生成的 tokens。</li>
</ul>
<p><strong>词元切分的流程与问题解决</strong>：</p>
<ul>
<li><strong>中文支持</strong>：Dolma 默认的 Tokenizer 对中文支持较差，建议选择其他支持中文的 Tokenizer，如 Qwen2，或自行训练一个。</li>
<li><strong>数据格式问题</strong>：当使用新词表且词表数量超过 65536 时，可能会造成 id 溢出，需要手动修改 numpy 保存和训练的 dtype 为 uint32。存储空间会因此扩大一倍。</li>
<li><strong>特殊 Tokens</strong>：确保 Tokenizer 设置、Tokenize 过程和训练设置的 End of sentence (EOS) 等特殊 tokens 保持一致。</li>
<li><strong>建议</strong>：处理完成后，查看生成的内容，确保能够正常解码。</li>
</ul>
<h3 id="三、大模型预训练与OLMo框架"><a href="#三、大模型预训练与OLMo框架" class="headerlink" title="三、大模型预训练与OLMo框架"></a><strong>三、大模型预训练与OLMo框架</strong></h3><h4 id="3-1-OLMo简介"><a href="#3-1-OLMo简介" class="headerlink" title="3.1 OLMo简介"></a><strong>3.1 OLMo简介</strong></h4><ul>
<li><strong>OLMo</strong> 是由 AI2（Allen Institute for AI）开源的一个大型语言模型（LLM），全称为 <strong>Open Language Model</strong>。</li>
<li><strong>完全开源</strong>：OLMo 不仅开源了模型的权重，还开源了整个训练过程中的数据、训练代码和评估代码，方便用户了解和复现模型训练的全过程。</li>
<li><strong>集成性好</strong>：只需要具备 GPU 资源，任何用户都可以利用 OLMo 从零开始快速训练自己的 LLM，这极大地降低了大模型预训练的门槛，方便研究人员和开发者进行探索和实践。</li>
</ul>
<h4 id="3-2-OLMo安装"><a href="#3-2-OLMo安装" class="headerlink" title="3.2 OLMo安装"></a><strong>3.2 OLMo安装</strong></h4><p>首先根据您的操作系统的指示安装 <a target="_blank" rel="noopener" href="https://pytorch.org">PyTorch</a>。</p>
<p>如果希望从源代码进行安装，请运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/allenai/OLMo.git</span><br><span class="line"><span class="built_in">cd</span> OLMo</span><br><span class="line">pip install -e .[all]</span><br></pre></td></tr></table></figure>
<p>否则，您可以直接从 PyPI 安装模型代码，使用以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ai2-olmo</span><br></pre></td></tr></table></figure>
<h4 id="3-3-OLMo预训练"><a href="#3-3-OLMo预训练" class="headerlink" title="3.3 OLMo预训练"></a><strong>3.3 OLMo预训练</strong></h4><p>用于训练官方 OLMo 模型的配置文件可以在 <a target="_blank" rel="noopener" href="https://github.com/allenai/OLMo/blob/main/configs/official"><code>configs/official/</code></a> 目录中找到。</p>
<p>在更新配置文件中的数据路径后，您可以使用 <code>torchrun</code> 启动训练。</p>
<p>例如，要在单个 8x GPU 节点上启动 1B 模型的训练，请运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml</span><br></pre></td></tr></table></figure>
<p>要从检查点恢复训练，可以将路径（本地或 URL）传递给 <code>scripts/train.py</code>，并使用 <code>--load_path</code> 参数。</p>
<p>例如，要从 OLMo 1B 运行的第 1000 步恢复训练：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml --load_path=https</span><br><span class="line"></span><br><span class="line">://olmo-checkpoints.org/ai2-llm/olmo-small/w1r5xfzt/step1000-unsharded</span><br></pre></td></tr></table></figure>
<h4 id="3-3-推理"><a href="#3-3-推理" class="headerlink" title="3.3 推理"></a><strong>3.3 推理</strong></h4><p>您可以使用 Hugging Face 来对 OLMo Transformers 检查点运行推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">olmo = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;allenai/OLMo-7B-0724-hf&quot;</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;allenai/OLMo-7B-0724-hf&quot;</span>)</span><br><span class="line"></span><br><span class="line">message = [<span class="string">&quot;Language modeling is &quot;</span>]</span><br><span class="line">inputs = tokenizer(message, return_tensors=<span class="string">&#x27;pt&#x27;</span>, return_token_type_ids=<span class="literal">False</span>)</span><br><span class="line">response = olmo.generate(**inputs, max_new_tokens=<span class="number">100</span>, do_sample=<span class="literal">True</span>, top_k=<span class="number">50</span>, top_p=<span class="number">0.95</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(response, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>或者，使用 Hugging Face 的 pipeline ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line">olmo_pipe = pipeline(<span class="string">&quot;text-generation&quot;</span>, model=<span class="string">&quot;allenai/OLMo-7B-0724-hf&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(olmo_pipe(<span class="string">&quot;Language modeling is&quot;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="四、指令微调与LLaMA-Factory框架"><a href="#四、指令微调与LLaMA-Factory框架" class="headerlink" title="四、指令微调与LLaMA-Factory框架"></a><strong>四、指令微调与LLaMA-Factory框架</strong></h3><h4 id="4-1-指令微调简介"><a href="#4-1-指令微调简介" class="headerlink" title="4.1 指令微调简介"></a><strong>4.1 指令微调简介</strong></h4><p><strong>指令微调（Instruction Tuning）</strong> 是对预训练后的大语言模型（LLM）进行参数微调的过程，也被称为<strong>有监督微调（Supervised Fine-tuning, SFT）</strong>。在指令微调的过程中，首先需要收集或构建包含指令信息的训练实例，然后通过有监督的方式对模型的参数进行微调。经过指令微调后的大语言模型具备较强的指令遵循能力，能够通过零样本学习（Zero-Shot Learning）的方式解决多种下游任务。</p>
<h4 id="4-2-训练流程"><a href="#4-2-训练流程" class="headerlink" title="4.2 训练流程"></a><strong>4.2 训练流程</strong></h4><ol>
<li><strong>选择基座模型</strong>：选择已预训练的基座模型，或者使用自己预训练的模型。</li>
<li><strong>构建高质量数据集</strong>：根据微调策略构建高质量的指令数据，注意数据质量与多样性，以及不同领域数据的比例。</li>
<li><strong>训练模型</strong>：根据硬件资源和训练目标，配置训练超参数、数据配置等。</li>
<li><strong>监控与评估</strong>：通过 LLaMA Board 等工具实时监控训练过程，观察 loss 曲线，调整训练策略。</li>
<li><strong>评测与推理</strong>：对微调后的模型进行评测，验证其在特定任务上的性能。<h4 id="4-3-指令数据的构建"><a href="#4-3-指令数据的构建" class="headerlink" title="4.3 指令数据的构建"></a><strong>4.3 指令数据的构建</strong></h4></li>
</ol>
<p><strong>指令数据的组成</strong>：</p>
<ul>
<li><strong>任务描述（指令）</strong>：描述模型需要完成的任务，例如回答问题、生成代码等。</li>
<li><strong>任务输入</strong>：提供给模型的任务输入数据，模型需要根据输入生成对应的输出。</li>
<li><strong>任务输出</strong>：预期的任务结果或答案。</li>
<li><strong>历史记录</strong>：在多轮对话中，历史记录用来保持上下文信息，帮助模型理解对话的连续性。</li>
</ul>
<p>在实际应用中，指令微调数据集可以包含<strong>单轮对话</strong>（无历史记录）和<strong>多轮对话</strong>（包含历史记录）。</p>
<p><strong>指令数据集示例</strong>：</p>
<ul>
<li><strong>OpenHermes</strong>、<strong>Alpaca</strong>、<strong>Codeforces-Python-Submissions-SFT</strong> 等数据集，涵盖了通用领域、代码生成、数学推理等不同类型的数据。推荐的数据集比例为通用数据与领域数据保持平衡，例如 5:1。</li>
</ul>
<h4 id="4-4-选择-SFT-训练框架：LLaMA-Factory"><a href="#4-4-选择-SFT-训练框架：LLaMA-Factory" class="headerlink" title="4.4 选择 SFT 训练框架：LLaMA-Factory"></a><strong>4.4 选择 SFT 训练框架：LLaMA-Factory</strong></h4><p><strong>LLaMA-Factory</strong> 是一个统一、高效的微调框架，集成了多种高效训练方法，支持对 100 多种大型语言模型（LLMs）的灵活微调。该框架的特点和优势包括：</p>
<ul>
<li><strong>易于使用</strong>：通过内置的 Web 用户界面 LlamaBoard，无需编码即可灵活地定制模型微调过程。</li>
<li><strong>多模型支持</strong>：支持多种模型，包括 LLaMA、LLaVA、Mistral 等，且提供多种训练方法，如继续预训练、多模态监督微调、奖励建模、PPO、DPO、KTO、ORPO 等。</li>
<li><strong>高效性</strong>：支持全参数微调（Full Fine Tuning, FFT）、参数高效微调（PEFT），以及多种量化技术和参数效率算法，如 LoRA（Low-Rank Adaptation）、Q-LoRA、FlashAttention-2 等，减少 GPU 内存使用并提高训练速度。</li>
<li><strong>实验监控</strong>：集成了 LlamaBoard、TensorBoard、Wandb、MLflow 等工具，方便跟踪训练进度和性能。</li>
</ul>
<h4 id="4-5-微调过程与配置"><a href="#4-5-微调过程与配置" class="headerlink" title="4.5 微调过程与配置"></a><strong>4.5 微调过程与配置</strong></h4><p><strong>数据集注册</strong>：</p>
<ul>
<li>将数据集放在 <code>data</code> 文件夹下，并在 <code>data_info.json</code> 中注册数据集的信息，包括数据集名称、地址和格式。</li>
</ul>
<p><strong>训练配置示例</strong>：</p>
<ul>
<li>以下是对模型进行微调的配置文件示例：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line"><span class="attr">model_name_or_path:</span> <span class="string">meta-llama/Meta-Llama-3-8B-Instruct</span></span><br><span class="line"><span class="comment"># method</span></span><br><span class="line"><span class="attr">stage:</span> <span class="string">sft</span></span><br><span class="line"><span class="attr">do_train:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">finetuning_type:</span> <span class="string">full</span></span><br><span class="line"><span class="comment"># ddp</span></span><br><span class="line"><span class="attr">ddp_timeout:</span> <span class="number">180000000</span></span><br><span class="line"><span class="attr">deepspeed:</span> <span class="string">examples/deepspeed/ds_z3_config.json</span></span><br><span class="line"><span class="comment"># dataset</span></span><br><span class="line"><span class="attr">dataset:</span> <span class="string">identity,alpaca_gpt4_en</span></span><br><span class="line"><span class="attr">template:</span> <span class="string">llama3</span></span><br><span class="line"><span class="attr">cutoff_len:</span> <span class="number">1024</span></span><br><span class="line"><span class="attr">max_samples:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">val_size:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">overwrite_cache:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">preprocessing_num_workers:</span> <span class="number">16</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="attr">output_dir:</span> <span class="string">saves/llama3-8b/full/sft</span></span><br><span class="line"><span class="attr">logging_steps:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">save_steps:</span> <span class="number">500</span></span><br><span class="line"><span class="attr">plot_loss:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">overwrite_output_dir:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>其中包含了模型名称、训练方法、数据集名称、数据预处理进程数、输出路径等信息。该配置文件可以灵活调整，以适应不同的训练需求。</li>
</ul>
<p><strong>训练与评估设置</strong>：</p>
<ul>
<li>单卡 batch size、梯度累积步数、学习率、训练 epoch、精度（如 FP16、BF16）等参数均可根据硬件资源进行调整。</li>
</ul>
<h4 id="4-6-训练策略"><a href="#4-6-训练策略" class="headerlink" title="4.6 训练策略"></a><strong>4.6 训练策略</strong></h4><p><strong>全量微调（Full Fine Tuning, FFT）</strong>：</p>
<ul>
<li>对全量参数进行训练，具有更高的训练成本，但能获得更全面的模型能力。然而，FFT 可能导致“灾难性遗忘”（Catastrophic Forgetting）现象，即在特定领域数据上微调后，模型在其他领域的表现可能下降。</li>
</ul>
<p><strong>参数高效微调（Parameter-Efficient Fine Tuning, PEFT）</strong>：</p>
<ul>
<li>仅对部分参数进行训练，常用的技术是 LoRA（Low-Rank Adaptation），通过添加旁路线性层 A 和 B，模型只需训练降维和升维的矩阵，实现高效微调。PEFT 能够以较低的训练成本提高模型在特定任务上的性能。</li>
</ul>
<h4 id="4-7-微调实例与训练过程"><a href="#4-7-微调实例与训练过程" class="headerlink" title="4.7 微调实例与训练过程"></a><strong>4.7 微调实例与训练过程</strong></h4><p><strong>训练实例</strong>：</p>
<ul>
<li>通过以下命令对 Llama3-8B-Instruct 模型进行微调、推理和合并操作：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli chat examples/inference/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli <span class="built_in">export</span> examples/merge_lora/llama3_lora_sft.yaml</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>可视化训练过程</strong>：</p>
<ul>
<li>使用 LLaMA Board（由 Gradio 驱动）实现训练过程的可视化，方便监控训练的实时状态：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="五、模型评测与OpenCompass"><a href="#五、模型评测与OpenCompass" class="headerlink" title="五、模型评测与OpenCompass"></a><strong>五、模型评测与OpenCompass</strong></h3><h4 id="5-1-OpenCompass简介"><a href="#5-1-OpenCompass简介" class="headerlink" title="5.1 OpenCompass简介"></a><strong>5.1 OpenCompass简介</strong></h4><p><strong>OpenCompass（司南）</strong> 是由上海人工智能实验室发布的一个开源大模型评测体系，旨在为大模型提供一个公平、开放和可复制的评估基准，已经成为目前权威的大型模型评估平台。OpenCompass 支持丰富的模型和数据集，提供分布式高效评测，具备多样化的评估范式、模块化设计和较强的可扩展性，同时具备实验管理和报告生成机制。</p>
<ul>
<li><strong>官方链接</strong>：<ul>
<li>GitHub 项目地址: <a target="_blank" rel="noopener" href="https://github.com/open-compass/opencompass">https://github.com/open-compass/opencompass</a></li>
<li>排行榜网站: <a target="_blank" rel="noopener" href="https://rank.opencompass.org.cn/home">https://rank.opencompass.org.cn/home</a></li>
</ul>
</li>
</ul>
<h4 id="5-2-OpenCompass安装"><a href="#5-2-OpenCompass安装" class="headerlink" title="5.2 OpenCompass安装"></a><strong>5.2 OpenCompass安装</strong></h4><p>安装 OpenCompass 的步骤如下：</p>
<ol>
<li><strong>创建 Conda 虚拟环境</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y</span><br><span class="line">conda activate opencompass</span><br></pre></td></tr></table></figure></li>
<li><strong>克隆 OpenCompass 项目源码</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/open-compass/opencompass opencompass</span><br><span class="line"><span class="built_in">cd</span> opencompass</span><br></pre></td></tr></table></figure></li>
<li><strong>安装依赖</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r ./requirements/runtime.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure></li>
<li><strong>下载测评数据</strong>：使用以下命令下载官方测评数据，并解压到 <code>opencompass</code> 目录下的 <code>data</code> 目录：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip</span><br><span class="line">unzip OpenCompassData-core-20231110.zip</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="5-3-OpenCompass整体流程"><a href="#5-3-OpenCompass整体流程" class="headerlink" title="5.3 OpenCompass整体流程"></a><strong>5.3 OpenCompass整体流程</strong></h4><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：</p>
<ul>
<li><strong>配置</strong>：配置评估过程，包括选择模型、数据集、评估策略、计算后端等，还可以定义结果的显示方式。</li>
<li><strong>推理与评估</strong>：推理阶段让模型从数据集产生输出，评估阶段则衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率。</li>
<li><strong>可视化</strong>：评估完成后，OpenCompass 会将结果整理成易读的表格，并保存为 CSV 和 TXT 文件，还可以在飞书客户端中及时获得评测状态报告。</li>
</ul>
<h4 id="5-4-OpenCompass配置文件"><a href="#5-4-OpenCompass配置文件" class="headerlink" title="5.4 OpenCompass配置文件"></a><strong>5.4 OpenCompass配置文件</strong></h4><ul>
<li><strong>配置文件命名</strong>：一般命名为 <code>eval_xxx.py</code>。</li>
<li><strong>主要配置项</strong>：<ul>
<li><strong>datasets</strong>：指定使用的数据集列表。</li>
<li><strong>models</strong>：指定使用的模型、分词器和推理方法（如 ppl、gen）。</li>
<li><strong>infer</strong>：指定测评任务的分割方式（如 <code>SizePartitioner</code>、<code>NaivePartitioner</code>）。</li>
<li><strong>meta_template</strong>：需要与 SFT 阶段使用的模板保持一致。</li>
</ul>
</li>
</ul>
<p>如使用 vllm 版本，可使用 <code>gpu_memory_utilization</code> 等额外参数，若发生显存不足则可调整相关参数。</p>
<h4 id="5-5-OpenCompass模型准备"><a href="#5-5-OpenCompass模型准备" class="headerlink" title="5.5 OpenCompass模型准备"></a><strong>5.5 OpenCompass模型准备</strong></h4><p>在进行模型评测之前，首先需要将模型转化为适合评测的格式：</p>
<ul>
<li><strong>分布式训练结果转换</strong>：将分布式训练的结果转换为 unsharded 形式：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python OLMo/scripts/unshard.py &lt;sharded_checkpoint_dir&gt; &lt;unsharded_checkpoint_dir&gt;</span><br></pre></td></tr></table></figure></li>
<li><strong>将检查点转换为 HuggingFace 格式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python /OLMo/scripts/convert_olmo_to_hf_new.py --input_dir=&lt;unsharded_checkpoint_dir&gt; --output_dir=&lt;hf_checkpoint_dir&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>如果使用 <code>llama-factory</code> 或 <code>trl</code> 等包进行模型训练/微调，则无需转换。使用 OLMo 训练的模型在进行 SFT 或评测前均需完成 unsharded 格式转换与 HuggingFace 格式转换。</p>
<h4 id="5-6-OpenCompass运行与输出"><a href="#5-6-OpenCompass运行与输出" class="headerlink" title="5.6 OpenCompass运行与输出"></a><strong>5.6 OpenCompass运行与输出</strong></h4><p>在 <code>opencompass</code> 目录下，运行以下命令启动评测：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python run.py ./configs/eval_olmo.py &gt; nohup.out &amp;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>运行过程中，会在 <code>&lt;workdir&gt;</code> 目录下生成一个以时间戳命名的文件夹，其中包含 <code>configs</code>、<code>logs</code>、<code>predictions</code>、<code>results</code>、<code>summary</code> 等文件夹。</li>
<li>如果在评测过程中遇到出错的项目，OpenCompass 会自动跳过；若错误过多，可查看对应的 <code>log</code> 文件进行排查。</li>
<li>评测完成后，可以在 <code>summary</code> 文件夹中找到每个测评数据集的得分（以 TXT 和 CSV 两种形式记录）。</li>
</ul>
<p><strong>停止进程</strong>：如需终止评测进程，可执行以下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -elf | grep opencompass | awk <span class="string">&#x27;&#123;print $4&#125;&#x27;</span> | xargs <span class="built_in">kill</span></span><br></pre></td></tr></table></figure></p>
<h4 id="5-7-OpenCompass评测数据集"><a href="#5-7-OpenCompass评测数据集" class="headerlink" title="5.7 OpenCompass评测数据集"></a><strong>5.7 OpenCompass评测数据集</strong></h4><ul>
<li><strong>MMLU（Multilingual Massively Multilingual Understanding）</strong>：多选问答任务的英文评测数据集，涵盖 STEM、人文、社会科学等领域的 57 个学科，共 15908 个问题，评估模型的多任务处理能力。</li>
<li><strong>CMMLU</strong>：综合性的中文评估基准，专门用于评估模型在中文语境下的知识和推理能力，涵盖 67 个主题，共 11528 个问题，包含少样本和测试集部分。</li>
<li><strong>C-Eval</strong>：中文 LLM 评估基准，包含 13948 个多项选择问题，涉及 52 个学科，分为初中、高中、大学和专业四个难度级别，并设有 C-Eval Hard 子集用于高级推理能力的评估。</li>
<li><strong>GSM8K</strong>：中小学数学应用题的英文数据集，包含 8.5K 高质量数学问题，考察模型在多步骤数学推理任务中的性能。</li>
<li><strong>MATH</strong>：数学竞赛问题数据集，包含 12500 道题目，提供了完整的分步骤解答方案，评估模型在数学推理和解题方面的能力。</li>
<li><strong>MBPP（Mostly Basic Python Problems）</strong>：由约 1401 个 Python 编程问题组成，旨在评估模型的编程基础和标准库功能。</li>
<li><strong>HumanEval</strong>：由 OpenAI 发布的编程问题数据集，包含 164 个编程任务，评估模型在代码生成和编写方面的能力。</li>
<li><strong>BBH（Big Bench Hard）</strong>：评估模型在高难度推理任务方面的性能，共 23 个具有挑战性的 BIG-Bench 任务，每类约 250 个测试案例。</li>
<li><strong>GAOKAO-BENCH</strong>：利用中国高考试题作为数据集，评估模型的语言理解和逻辑推理能力。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/" data-id="cm4boxx5h0005ykfybv0hb0uz" data-title="Olmo及dolma实践" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-DeepSpeed相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:23:53.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/">DeepSpeed相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Accelerate</strong> 集成了 <strong>DeepSpeed ZeRO</strong> 的所有功能。这包括 <strong>ZeRO</strong> 的第 1、2 和 3 阶段，以及 <strong>ZeRO-Offload</strong>、<strong>ZeRO-Infinity</strong>（可以卸载到 Disk/NVMe）和 <strong>ZeRO++</strong>。</p>
<p>Huggingface中的 <strong>Accelerate</strong> 通过两种选项集成了 DeepSpeed：</p>
<ol>
<li><strong>通过 deepspeed 配置文件</strong></li>
<li><strong>通过 deepspeed_plugin</strong></li>
</ol>
</blockquote>
<hr>
<h2 id="如何通过Accelerate使用DeepSpeed"><a href="#如何通过Accelerate使用DeepSpeed" class="headerlink" title="如何通过Accelerate使用DeepSpeed"></a>如何通过Accelerate使用DeepSpeed</h2><h3 id="1-通过-deepspeed-plugin"><a href="#1-通过-deepspeed-plugin" class="headerlink" title="1. 通过 deepspeed_plugin"></a>1. 通过 deepspeed_plugin</h3><p>在您的机器上运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate config</span><br></pre></td></tr></table></figure>
<p>并回答所提出的问题。它会询问您是否要为 <strong>DeepSpeed</strong> 使用配置文件，您应该回答否。然后回答以下问题以生成一个基本的 <strong>DeepSpeed</strong> 配置。这将生成一个配置文件，在执行以下命令时将自动使用该配置文件来正确设置默认选项：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch my_script.py --args_to_my_script</span><br></pre></td></tr></table></figure>
<p>例如，这里是如何使用 <strong>DeepSpeed Plugin</strong> 运行 NLP 示例 <code>examples/nlp_example.py</code>（从仓库根目录）：</p>
<h4 id="ZeRO-Stage-2-DeepSpeed-Plugin-示例"><a href="#ZeRO-Stage-2-DeepSpeed-Plugin-示例" class="headerlink" title="ZeRO Stage-2 DeepSpeed Plugin 示例"></a>ZeRO Stage-2 DeepSpeed Plugin 示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">none</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">none</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/nlp_example.py --mixed_precision fp16</span><br></pre></td></tr></table></figure>
<h4 id="ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-Plugin-示例"><a href="#ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-Plugin-示例" class="headerlink" title="ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed Plugin 示例"></a>ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed Plugin 示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero3_save_16bit_model:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/nlp_example.py --mixed_precision fp16</span><br></pre></td></tr></table></figure>
<p>目前，<strong>Accelerate</strong> 通过 CLI 支持以下配置：</p>
<ul>
<li><code>zero_stage</code>: [0] 禁用， [1] 优化器状态分区， [2] 优化器+梯度状态分区， [3] 优化器+梯度+参数分区</li>
<li><code>gradient_accumulation_steps</code>: 在平均和应用梯度之前要累积的训练步骤数。</li>
<li><code>gradient_clipping</code>: 启用梯度裁剪并设定值。</li>
<li><code>offload_optimizer_device</code>: [none] 禁用优化器卸载， [cpu] 将优化器卸载到 CPU， [nvme] 将优化器卸载到 NVMe SSD。仅适用于 ZeRO &gt;= Stage-2。</li>
<li><code>offload_optimizer_nvme_path</code>: 决定将优化器状态卸载到的 NVMe 路径。如果未指定，则默认为 ‘none’。</li>
<li><code>offload_param_device</code>: [none] 禁用参数卸载， [cpu] 将参数卸载到 CPU， [nvme] 将参数卸载到 NVMe SSD。仅适用于 ZeRO Stage-3。</li>
<li><code>offload_param_nvme_path</code>: 决定将参数卸载到的 NVMe 路径。如果未指定，则默认为 ‘none’。</li>
<li><code>zero3_init_flag</code>: 决定是否启用 <code>deepspeed.zero.Init</code> 来构建大规模模型。仅适用于 ZeRO Stage-3。</li>
<li><code>zero3_save_16bit_model</code>: 决定在使用 ZeRO Stage-3 时是否保存 16 位模型权重。</li>
<li><code>mixed_precision</code>: <code>no</code> 表示 FP32 训练， <code>fp16</code> 表示 FP16 混合精度训练， <code>bf16</code> 表示 BF16 混合精度训练。</li>
<li><code>deepspeed_moe_layer_cls_names</code>: 逗号分隔的 transformer Mixture-of-Experts (MoE) 层类名列表（区分大小写），例如 <code>MixtralSparseMoeBlock</code>, <code>Qwen2MoeSparseMoeBlock</code>, <code>JetMoEAttention,JetMoEBlock</code> …</li>
<li><code>deepspeed_hostfile</code>: 用于配置多节点计算资源的 <strong>DeepSpeed</strong> hostfile。</li>
<li><code>deepspeed_exclusion_filter</code>: 使用多节点设置时的 <strong>DeepSpeed</strong> 排除过滤字符串。</li>
<li><code>deepspeed_inclusion_filter</code>: 使用多节点设置时的 <strong>DeepSpeed</strong> 包含过滤字符串。</li>
<li><code>deepspeed_multinode_launcher</code>: 要使用的 <strong>DeepSpeed</strong> 多节点启动器。如果未指定，则默认为 <code>pdsh</code>。</li>
<li><code>deepspeed_config_file</code>: <strong>DeepSpeed</strong> 配置文件的路径，格式为 <code>json</code>。有关更多详细信息，请参见下一节。</li>
</ul>
<p>要调整更多选项，您需要使用 <strong>DeepSpeed</strong> 配置文件。</p>
<h3 id="2-DeepSpeed-配置文件"><a href="#2-DeepSpeed-配置文件" class="headerlink" title="2. DeepSpeed 配置文件"></a>2. DeepSpeed 配置文件</h3><p>在您的机器上运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate config</span><br></pre></td></tr></table></figure>
<p>并回答所提出的问题。它会询问您是否要为 <strong>DeepSpeed</strong> 使用配置文件，您应该回答是并提供 <strong>DeepSpeed 配置文件</strong> 的路径。这将生成一个配置文件，在执行以下命令时将自动使用该配置文件来正确设置默认选项：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch my_script.py --args_to_my_script</span><br></pre></td></tr></table></figure>
<p>例如，这里是如何使用 <strong>DeepSpeed 配置文件</strong> 运行 NLP 示例 <code>examples/by_feature/deepspeed_with_config_support.py</code>（从仓库根目录）：</p>
<h4 id="ZeRO-Stage-2-DeepSpeed-配置文件示例"><a href="#ZeRO-Stage-2-DeepSpeed-配置文件示例" class="headerlink" title="ZeRO Stage-2 DeepSpeed 配置文件示例"></a>ZeRO Stage-2 DeepSpeed 配置文件示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">/home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage2_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>zero_stage2_config.json</strong> 的内容如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;initial_scale_power&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AdamW&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;torch_adam&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;adam_w_mode&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;scheduler&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;WarmupDecayLR&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;warmup_min_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_max_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_num_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;total_num_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;allgather_partitions&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">2e8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">2000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/by_feature/deepspeed_with_config_support.py \</span><br><span class="line">--config_name <span class="string">&quot;gpt2-large&quot;</span> \</span><br><span class="line">--tokenizer_name <span class="string">&quot;gpt2-large&quot;</span> \</span><br><span class="line">--dataset_name <span class="string">&quot;wikitext&quot;</span> \</span><br><span class="line">--dataset_config_name <span class="string">&quot;wikitext-2-raw-v1&quot;</span> \</span><br><span class="line">--block_size 128 \</span><br><span class="line">--output_dir <span class="string">&quot;./clm/clm_deepspeed_stage2_accelerate&quot;</span> \</span><br><span class="line">--learning_rate 5e-4 \</span><br><span class="line">--per_device_train_batch_size 24 \</span><br><span class="line">--per_device_eval_batch_size 24 \</span><br><span class="line">--num_train_epochs 3 \</span><br><span class="line">--with_tracking \</span><br><span class="line">--report_to <span class="string">&quot;wandb&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-配置文件示例"><a href="#ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-配置文件示例" class="headerlink" title="ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed 配置文件示例"></a>ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed 配置文件示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">/home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage3_offload_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>zero_stage3_offload_config.json</strong> 的内容如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;initial_scale_power&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AdamW&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;scheduler&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;WarmupDecayLR&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;warmup_min_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_max_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_num_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;total_num_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cpu&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;pin_memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_param&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cpu&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;pin_memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_prefetch_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_param_persistence_threshold&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;sub_group_size&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_max_live_parameters&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_max_reuse_distance&quot;</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">2000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/by_feature/deepspeed_with_config_support.py \</span><br><span class="line">--config_name <span class="string">&quot;gpt2-large&quot;</span> \</span><br><span class="line">--tokenizer_name <span class="string">&quot;gpt2-large&quot;</span> \</span><br><span class="line">--dataset_name <span class="string">&quot;wikitext&quot;</span> \</span><br><span class="line">--dataset_config_name <span class="string">&quot;wikitext-2-raw-v1&quot;</span> \</span><br><span class="line">--block_size 128 \</span><br><span class="line">--output_dir <span class="string">&quot;./clm/clm_deepspeed_stage3_offload_accelerate&quot;</span> \</span><br><span class="line">--learning_rate 5e-4 \</span><br><span class="line">--per_device_train_batch_size 32 \</span><br><span class="line">--per_device_eval_batch_size 32 \</span><br><span class="line">--num_train_epochs 3 \</span><br><span class="line">--with_tracking \</span><br><span class="line">--report_to <span class="string">&quot;wandb&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>说明：</strong></p>
<ul>
<li><p><strong>ZeRO Stage-2 配置文件</strong> 示例展示了如何配置 <strong>DeepSpeed</strong> 以使用 ZeRO 的第 2 阶段优化，包括梯度累积步数、梯度裁剪、优化器类型等参数。</p>
</li>
<li><p><strong>ZeRO Stage-3 使用 CPU Offload 的配置文件</strong> 示例则进一步展示了如何将优化器状态和模型参数卸载到 CPU，以支持更大规模模型的训练。</p>
</li>
<li><p>运行命令中使用的 <code>--config_name</code>、<code>--tokenizer_name</code>、<code>--dataset_name</code> 等参数保持不变，确保 <strong>Accelerate</strong> 能正确加载和使用指定的配置文件。</p>
</li>
<li><p>通过配置文件方式，用户可以更灵活地调整 <strong>DeepSpeed</strong> 的各种参数，以适应不同的训练需求和硬件环境。</p>
</li>
</ul>
<hr>
<h2 id="ZeRO-配置示例"><a href="#ZeRO-配置示例" class="headerlink" title="ZeRO++ 配置示例"></a><strong>ZeRO++ 配置示例</strong></h2><p>您可以通过使用适当的配置参数来使用 <strong>ZeRO++</strong> 的功能。请注意，<strong>ZeRO++</strong> 是 <strong>ZeRO Stage 3</strong> 的扩展。以下是如何修改配置文件的示例，摘自 <strong>DeepSpeed</strong> 的 <strong>ZeRO++</strong> 教程：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">&quot;zero_quantized_weights&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;zero_hpz_partition_size&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;zero_quantized_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>对于分层分区，<code>zero_hpz_partition_size</code> 的分区大小理想情况下应设置为每个节点的 GPU 数量。（例如，上述配置文件假设每个节点有 8 个 GPU）</p>
<h3 id="使用-DeepSpeed-配置文件时的重要代码更改"><a href="#使用-DeepSpeed-配置文件时的重要代码更改" class="headerlink" title="使用 DeepSpeed 配置文件时的重要代码更改"></a><strong>使用 DeepSpeed 配置文件时的重要代码更改</strong></h3><p><strong>DeepSpeed Optimizers</strong> 和 <strong>Schedulers</strong>。有关更多信息，请参阅 <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/#optimizer">DeepSpeed Optimizers</a> 和 <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/#scheduler">DeepSpeed Schedulers</a> 文档。我们将查看在使用这些时代码需要进行的更改。</p>
<h4 id="a-DS-Optim-DS-Scheduler"><a href="#a-DS-Optim-DS-Scheduler" class="headerlink" title="a. DS Optim + DS Scheduler"></a>a. <strong>DS Optim + DS Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中同时存在 <code>optimizer</code> 和 <code>scheduler</code> 键时。在这种情况下，将使用配置文件中的优化器和调度器，用户需要使用 <code>accelerate.utils.DummyOptim</code> 和 <code>accelerate.utils.DummyScheduler</code> 来替换代码中的 PyTorch/自定义优化器和调度器。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果配置文件中指定了 `optimizer`，则创建 Dummy Optimizer，否则创建 Adam Optimizer</span></span><br><span class="line">optimizer_cls = (</span><br><span class="line">    torch.optim.AdamW</span><br><span class="line">    <span class="keyword">if</span> accelerator.state.deepspeed_plugin <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">or</span> <span class="string">&quot;optimizer&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> accelerator.state.deepspeed_plugin.deepspeed_config</span><br><span class="line">    <span class="keyword">else</span> DummyOptim</span><br><span class="line">)</span><br><span class="line">optimizer = optimizer_cls(optimizer_grouped_parameters, lr=args.learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果配置文件中未指定 `scheduler`，则创建 `args.lr_scheduler_type` Scheduler，否则创建 Dummy Scheduler</span></span><br><span class="line"><span class="keyword">if</span> (</span><br><span class="line">    accelerator.state.deepspeed_plugin <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">or</span> <span class="string">&quot;scheduler&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> accelerator.state.deepspeed_plugin.deepspeed_config</span><br><span class="line">):</span><br><span class="line">    lr_scheduler = get_scheduler(</span><br><span class="line">        name=args.lr_scheduler_type,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        num_warmup_steps=args.num_warmup_steps,</span><br><span class="line">        num_training_steps=args.max_train_steps,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lr_scheduler = DummyScheduler(</span><br><span class="line">        optimizer, total_num_steps=args.max_train_steps, warmup_num_steps=args.num_warmup_steps</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h4 id="b-Custom-Optim-Custom-Scheduler"><a href="#b-Custom-Optim-Custom-Scheduler" class="headerlink" title="b. Custom Optim + Custom Scheduler"></a>b. <strong>Custom Optim + Custom Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中同时缺少 <code>optimizer</code> 和 <code>scheduler</code> 键时。在这种情况下，用户无需更改任何代码，这是使用 <strong>DeepSpeed Plugin</strong> 集成时的情况。在上述示例中，如果配置文件中缺少 <code>optimizer</code> 和 <code>scheduler</code> 键，代码将保持不变。</p>
<h4 id="c-Custom-Optim-DS-Scheduler"><a href="#c-Custom-Optim-DS-Scheduler" class="headerlink" title="c. Custom Optim + DS Scheduler"></a>c. <strong>Custom Optim + DS Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中仅存在 <code>scheduler</code> 键时。在这种情况下，用户必须使用 <code>accelerate.utils.DummyScheduler</code> 来替换代码中的 PyTorch/自定义调度器。</p>
<h4 id="d-DS-Optim-Custom-Scheduler"><a href="#d-DS-Optim-Custom-Scheduler" class="headerlink" title="d. DS Optim + Custom Scheduler"></a>d. <strong>DS Optim + Custom Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中仅存在 <code>optimizer</code> 键时。这将导致错误，因为只能在使用 <strong>DS Optim</strong> 时使用 <strong>DS Scheduler</strong>。</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><p>请注意上述示例 <strong>DeepSpeed</strong> 配置文件中的 <code>auto</code> 值。这些值由 <code>prepare</code> 方法根据模型、数据加载器、Dummy Optimizer 和 Dummy Schedulers 自动处理。仅示例中指定的 <code>auto</code> 字段由 <code>prepare</code> 方法处理，其余字段必须由用户显式指定。</p>
<p><strong>自动值的计算方式如下：</strong></p>
<ul>
<li><code>reduce_bucket_size</code>: <code>hidden_size * hidden_size</code></li>
<li><code>stage3_prefetch_bucket_size</code>: <code>int(0.9 * hidden_size * hidden_size)</code></li>
<li><code>stage3_param_persistence_threshold</code>: <code>10 * hidden_size</code></li>
</ul>
<p>为了使这 3 个配置项的自动功能正常工作，<strong>Accelerate</strong> 将使用 <code>model.config.hidden_size</code> 或 <code>max(model.config.hidden_sizes)</code> 作为 <code>hidden_size</code>。如果这两个都不可用，启动将失败，您需要手动设置这 3 个配置项。请记住，前两个配置项是通信缓冲区——它们越大，通信效率越高，但也会消耗更多的 GPU 内存，因此这是一个可调节的性能权衡。</p>
<hr>
<h3 id="使用-DeepSpeed-配置文件时需要注意的事项"><a href="#使用-DeepSpeed-配置文件时需要注意的事项" class="headerlink" title="使用 DeepSpeed 配置文件时需要注意的事项"></a><strong>使用 DeepSpeed 配置文件时需要注意的事项</strong></h3><p>以下是一个在不同场景下使用 <code>deepspeed_config_file</code> 的示例脚本。</p>
<p><strong>代码 <code>test.py</code>：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> accelerate.state <span class="keyword">import</span> AcceleratorState</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    accelerator = Accelerator()</span><br><span class="line">    accelerator.<span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;AcceleratorState()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h4 id="场景-1：手动修改的-accelerate-配置文件，同时包含-deepspeed-config-file-及其他条目"><a href="#场景-1：手动修改的-accelerate-配置文件，同时包含-deepspeed-config-file-及其他条目" class="headerlink" title="场景 1：手动修改的 accelerate 配置文件，同时包含 deepspeed_config_file 及其他条目"></a><strong>场景 1：手动修改的 accelerate 配置文件，同时包含 <code>deepspeed_config_file</code> 及其他条目</strong></h4><p><strong>accelerate 配置内容：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command_file:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">commands:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero3_save_16bit_model:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">&#x27;ds_config.json&#x27;</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">dynamo_backend:</span> <span class="string">&#x27;NO&#x27;</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">gpu_ids:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">megatron_lm_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_name:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">tpu_zone:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong><code>ds_config.json</code> 内容：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bf16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;none&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_param&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;none&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">2000000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令 <code>accelerate launch test.py</code> 的输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ValueError: When using `deepspeed_config_file`, the following accelerate config variables will be ignored:</span><br><span class="line">[&#x27;gradient_accumulation_steps&#x27;, &#x27;gradient_clipping&#x27;, &#x27;zero_stage&#x27;, &#x27;offload_optimizer_device&#x27;, &#x27;offload_param_device&#x27;,</span><br><span class="line">&#x27;zero3_save_16bit_model&#x27;, &#x27;mixed_precision&#x27;].</span><br><span class="line">Please specify them appropriately in the DeepSpeed config file.</span><br><span class="line">If you are using an accelerate config file, remove other config variables mentioned in the above specified list.</span><br><span class="line">The easiest method is to create a new config following the questionnaire via `accelerate config`.</span><br><span class="line">It will only ask for the necessary config variables when using `deepspeed_config_file`.</span><br></pre></td></tr></table></figure>
<h4 id="场景-2：使用错误解决方案创建新的-accelerate-配置，并检查不再抛出模糊错误"><a href="#场景-2：使用错误解决方案创建新的-accelerate-配置，并检查不再抛出模糊错误" class="headerlink" title="场景 2：使用错误解决方案创建新的 accelerate 配置，并检查不再抛出模糊错误"></a><strong>场景 2：使用错误解决方案创建新的 accelerate 配置，并检查不再抛出模糊错误</strong></h4><p><strong>运行 <code>accelerate config</code>：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ accelerate config</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">In which compute environment are you running?</span><br><span class="line">This machine</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">Which type of machine are you using?</span><br><span class="line">multi-GPU</span><br><span class="line">How many different machines will you use (use more than 1 for multi-node training)? [1]: </span><br><span class="line">Do you wish to optimize your script with torch dynamo?[yes/NO]: </span><br><span class="line">Do you want to use DeepSpeed? [yes/NO]: yes</span><br><span class="line">Do you want to specify a json file to a DeepSpeed config? [yes/NO]: yes</span><br><span class="line">Please enter the path to the json DeepSpeed config file: ds_config.json</span><br><span class="line">Do you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]: yes</span><br><span class="line">How many GPU(s) should be used for distributed training? [1]:4</span><br><span class="line">accelerate configuration saved at ds_config_sample.yaml</span><br></pre></td></tr></table></figure>
<p><strong>新的 accelerate 配置内容：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">ds_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line"><span class="attr">dynamo_backend:</span> <span class="string">&#x27;NO&#x27;</span></span><br><span class="line"><span class="attr">fsdp_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">megatron_lm_config:</span> &#123;&#125;</span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令 <code>accelerate launch test.py</code> 的输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Distributed environment: DEEPSPEED  Backend: nccl</span><br><span class="line">Num processes: 4</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cuda:0</span><br><span class="line">Mixed precision type: bf16</span><br><span class="line">ds_config: &#123;&#x27;bf16&#x27;: &#123;&#x27;enabled&#x27;: True&#125;, &#x27;zero_optimization&#x27;: &#123;&#x27;stage&#x27;: 3, &#x27;stage3_gather_16bit_weights_on_model_save&#x27;: False, &#x27;offload_optimizer&#x27;: &#123;&#x27;device&#x27;: &#x27;none&#x27;&#125;, &#x27;offload_param&#x27;: &#123;&#x27;device&#x27;: &#x27;none&#x27;&#125;&#125;, &#x27;gradient_clipping&#x27;: 1.0, &#x27;train_batch_size&#x27;: &#x27;auto&#x27;, &#x27;train_micro_batch_size_per_gpu&#x27;: &#x27;auto&#x27;, &#x27;gradient_accumulation_steps&#x27;: 10, &#x27;steps_per_print&#x27;: inf, &#x27;fp16&#x27;: &#123;&#x27;enabled&#x27;: False&#125;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="场景-3：在-DeepSpeed-配置文件中将与-DeepSpeed-命令相关的-accelerate-launch-命令参数设置为-“auto”，并检查是否按预期工作"><a href="#场景-3：在-DeepSpeed-配置文件中将与-DeepSpeed-命令相关的-accelerate-launch-命令参数设置为-“auto”，并检查是否按预期工作" class="headerlink" title="场景 3：在 DeepSpeed 配置文件中将与 DeepSpeed 命令相关的 accelerate launch 命令参数设置为 “auto”，并检查是否按预期工作"></a><strong>场景 3：在 DeepSpeed 配置文件中将与 DeepSpeed 命令相关的 accelerate launch 命令参数设置为 “auto”，并检查是否按预期工作</strong></h4><p><strong>新的 <code>ds_config.json</code>，将与 accelerate launch DeepSpeed 命令参数设置为 “auto”：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;bf16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_param&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">2000000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch --mixed_precision=<span class="string">&quot;fp16&quot;</span> --zero_stage=3 --gradient_accumulation_steps=5 --gradient_clipping=1.0 --offload_param_device=<span class="string">&quot;cpu&quot;</span> --offload_optimizer_device=<span class="string">&quot;nvme&quot;</span> --zero3_save_16bit_model=<span class="string">&quot;true&quot;</span> test.py</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Distributed environment: DEEPSPEED  Backend: nccl</span><br><span class="line">Num processes: 4</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cuda:0</span><br><span class="line">Mixed precision type: fp16</span><br><span class="line">ds_config: &#123;&#x27;bf16&#x27;: &#123;&#x27;enabled&#x27;: False&#125;, &#x27;zero_optimization&#x27;: &#123;&#x27;stage&#x27;: 3, &#x27;stage3_gather_16bit_weights_on_model_save&#x27;: True, &#x27;offload_optimizer&#x27;: &#123;&#x27;device&#x27;: &#x27;nvme&#x27;&#125;, &#x27;offload_param&#x27;: &#123;&#x27;device&#x27;: &#x27;cpu&#x27;&#125;&#125;, &#x27;gradient_clipping&#x27;: 1.0, &#x27;train_batch_size&#x27;: &#x27;auto&#x27;, &#x27;train_micro_batch_size_per_gpu&#x27;: &#x27;auto&#x27;, &#x27;gradient_accumulation_steps&#x27;: 5, &#x27;steps_per_print&#x27;: inf, &#x27;fp16&#x27;: &#123;&#x27;enabled&#x27;: True, &#x27;auto_cast&#x27;: True&#125;&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ul>
<li>剩余的 “auto” 值在调用 <code>accelerator.prepare()</code> 时处理，如使用 DeepSpeed 配置文件时重要代码更改中的第 2 点所述。</li>
<li>仅当 <code>gradient_accumulation_steps</code> 为 auto 时，创建 <code>Accelerator</code> 对象时传递的值（如 <code>Accelerator(gradient_accumulation_steps=k)</code>）将被使用。</li>
<li>使用 DeepSpeed Plugin 时，将使用 DeepSpeed Plugin 中的值，并覆盖在创建 <code>Accelerator</code> 对象时传递的值。</li>
</ul>
<h4 id="保存和加载"><a href="#保存和加载" class="headerlink" title="保存和加载"></a><strong>保存和加载</strong></h4><ul>
<li><p><strong>ZeRO Stage-1 和 Stage-2</strong> 的模型保存和加载方式保持不变。</p>
</li>
<li><p><strong>ZeRO Stage-3</strong> 下，<code>state_dict</code> 仅包含占位符，因为模型权重被分区到多个 GPU 上。<strong>ZeRO Stage-3</strong> 有两种选项：</p>
<p>a. <strong>保存整个 16 位模型权重</strong>，以便稍后使用 <code>model.load_state_dict(torch.load(pytorch_model.bin))</code> 直接加载。为此，可以在 DeepSpeed 配置文件中设置 <code>zero_optimization.stage3_gather_16bit_weights_on_model_save</code> 为 <code>True</code>，或在 DeepSpeed Plugin 中设置 <code>zero3_save_16bit_model</code> 为 <code>True</code>。请注意，此选项需要在一个 GPU 上合并权重，可能会很慢且占用大量内存，因此仅在需要时使用。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新代码 #</span></span><br><span class="line"><span class="comment"># 如果 DeepSpeed 配置文件中的 `stage3_gather_16bit_weights_on_model_save` 为 True 或</span></span><br><span class="line"><span class="comment"># DeepSpeed Plugin 中的 `zero3_save_16bit_model` 为 True，则在 ZeRO Stage-3 中将整个/未分区的 fp16 模型保存到输出目录。</span></span><br><span class="line"><span class="comment"># 对于 ZeRO Stage 1 和 2，模型按通常方式保存在输出目录中。</span></span><br><span class="line"><span class="comment"># 保存的模型名称为 `pytorch_model.bin`</span></span><br><span class="line">unwrapped_model.save_pretrained(</span><br><span class="line">    args.output_dir,</span><br><span class="line">    is_main_process=accelerator.is_main_process,</span><br><span class="line">    save_function=accelerator.save,</span><br><span class="line">    state_dict=accelerator.get_state_dict(model),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>b. <strong>获取 32 位权重</strong>，首先使用 <code>model.save_checkpoint()</code> 保存模型。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">success = model.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)</span><br><span class="line">status_msg = <span class="string">f&quot;checkpointing: PATH=<span class="subst">&#123;PATH&#125;</span>, ckpt_id=<span class="subst">&#123;ckpt_id&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">if</span> success:</span><br><span class="line">    logging.info(<span class="string">f&quot;Success <span class="subst">&#123;status_msg&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">f&quot;Failure <span class="subst">&#123;status_msg&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>  这将在检查点目录中创建 ZeRO 模型和优化器分区以及 <code>zero_to_fp32.py</code> 脚本。您可以使用此脚本进行离线合并，无需配置文件或 GPU。以下是其使用示例：</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /path/to/checkpoint_dir</span><br><span class="line">$ ./zero_to_fp32.py . pytorch_model.bin</span><br><span class="line">Processing zero checkpoint at global_step1</span><br><span class="line">Detected checkpoint of <span class="built_in">type</span> zero stage 3, world_size: 2</span><br><span class="line">Saving fp32 state dict to pytorch_model.bin (total_numel=60506624)</span><br></pre></td></tr></table></figure>
<p>  <strong>获取用于保存/推理的 32 位模型，您可以执行以下操作：</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> deepspeed.utils.zero_to_fp32 <span class="keyword">import</span> load_state_dict_from_zero_checkpoint</span><br><span class="line"></span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">fp32_model = load_state_dict_from_zero_checkpoint(unwrapped_model, checkpoint_dir)</span><br></pre></td></tr></table></figure>
<p>  <strong>如果您只对 <code>state_dict</code> 感兴趣，可以执行以下操作：</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> deepspeed.utils.zero_to_fp32 <span class="keyword">import</span> get_fp32_state_dict_from_zero_checkpoint</span><br><span class="line"></span><br><span class="line">state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)</span><br></pre></td></tr></table></figure>
<p>  请注意，所有这些函数需要约 2 倍于最终检查点大小的内存（通用 RAM）。</p>
</li>
</ul>
<h4 id="ZeRO-推理"><a href="#ZeRO-推理" class="headerlink" title="ZeRO 推理"></a><strong>ZeRO 推理</strong></h4><p><strong>DeepSpeed ZeRO Inference</strong> 支持 ZeRO Stage 3 与 <strong>ZeRO-Infinity</strong>。它使用与训练相同的 ZeRO 协议，但不使用优化器和学习率调度器，仅 Stage 3 相关。通过 <strong>Accelerate</strong> 集成，您只需按如下所示准备模型和数据加载器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model, eval_dataloader = accelerator.prepare(model, eval_dataloader)</span><br></pre></td></tr></table></figure>
<h4 id="需要注意的几个问题"><a href="#需要注意的几个问题" class="headerlink" title="需要注意的几个问题"></a><strong>需要注意的几个问题</strong></h4><ul>
<li>当前的集成不支持 <strong>DeepSpeed</strong> 的 <strong>Pipeline Parallelism</strong>。</li>
<li>当前的集成不支持 <strong>mpu</strong>，限制了在 <strong>Megatron-LM</strong> 中支持的张量并行。</li>
<li>当前的集成不支持多个模型。</li>
</ul>
<h4 id="DeepSpeed-资源"><a href="#DeepSpeed-资源" class="headerlink" title="DeepSpeed 资源"></a><strong>DeepSpeed 资源</strong></h4><p>与 DeepSpeed 相关的内部文档可以在这里找到：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">项目的 GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/">使用文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/api/">API 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/blog/">博客文章</a></li>
<li><strong>论文：</strong><ul>
<li><strong>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</strong></li>
<li><strong>ZeRO-Offload: Democratizing Billion-Scale Model Training</strong></li>
<li><strong>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</strong></li>
<li><strong>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</strong></li>
</ul>
</li>
</ul>
<p>最后，请记住，<strong>Accelerate</strong> 仅集成了 <strong>DeepSpeed</strong>，因此如果您在使用 <strong>DeepSpeed</strong> 时遇到任何问题或有任何疑问，请在 <strong>DeepSpeed GitHub</strong> 上提交问题。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/" data-id="cm4boxx5f0001ykfy39y5dlvy" data-title="DeepSpeed相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-DP和DDP" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/DP%E5%92%8CDDP/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:20:09.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/DP%E5%92%8CDDP/">DP和DDP</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p>首先 <strong>DP</strong> 和 <strong>DDP</strong> 都只是 <code>数据并行</code> 并不涉及到 <code>模型权重</code> 的拆分。</p>
</blockquote>
<h2 id="DataParallel-DP"><a href="#DataParallel-DP" class="headerlink" title="DataParallel (DP)"></a>DataParallel (DP)</h2><p>DP是较简单的一种数据并行方式，直接将模型复制到多个GPU上并行计算，每个GPU计算batch中的一部分数据，各自完成前向和反向后，将梯度汇总到主GPU上。其基本流程：</p>
<ol>
<li>加载模型、数据至内存；</li>
<li>创建DP模型；</li>
<li>DP模型的forward过程：<ol>
<li><strong>一个batch的数据均分到不同device</strong>上；</li>
<li>为每个device复制一份模型；</li>
<li>至此，每个device上有模型和一份数据，并行进行前向传播；</li>
<li>收集各个device上的输出；</li>
</ol>
</li>
<li>每个device上的模型反向传播后，收集梯度到主device上，更新主device上的模型，将模型广播到其他device上；</li>
<li>3-4循环。</li>
</ol>
<p>在DP中，只有一个主进程，主进程下有多个线程，每个线程管理一个device的训练。因此，DP中内存中只存在一份数据，各个线程间是共享这份数据的。DP和Parameter Server的方式很像。</p>
<p><strong>Demo:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, target</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx], <span class="variable language_">self</span>.target[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.fc(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一些数据</span></span><br><span class="line">n_sample = <span class="number">100</span></span><br><span class="line">n_dim = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">X = torch.randn(n_sample, n_dim)</span><br><span class="line">Y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (n_sample, )).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">dataset = SimpleDataset(X, Y)</span><br><span class="line">data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===== 注意：刚创建的模型是在 cpu 上的 ===== #</span></span><br><span class="line">device_ids = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">model = SimpleModel(n_dim).to(device_ids[<span class="number">0</span>])</span><br><span class="line">model = nn.DataParallel(model, device_ids=device_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        inputs, targets = inputs.to(<span class="string">&#x27;cuda&#x27;</span>), targets.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        </span><br><span class="line">        loss = nn.BCELoss()(outputs, targets.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;batch_idx&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其中最重要的一行便是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model, device_ids=device_ids)</span><br></pre></td></tr></table></figure>
<p>注意，模型的参数和缓冲区都要放在<code>device_ids[0]</code>上。在执行<code>forward</code>函数时，模型会被复制到各个GPU上，对模型的属性进行更新并不会产生效果，因为前向完后各个卡上的模型就被销毁了。<strong>只有在<code>device_ids[0]</code>上对模型的参数或者buffer进行的更新才会生效！</strong></p>
<h2 id="DistributedDataParallel-DDP"><a href="#DistributedDataParallel-DDP" class="headerlink" title="DistributedDataParallel (DDP)"></a>DistributedDataParallel (DDP)</h2><p><strong>DistributedDataParallel（DDP）</strong> 是 PyTorch 提供的分布式数据并行训练接口，旨在高效地在多 GPU、甚至多机多 GPU 环境下进行训练。与 <code>DataParallel</code>（DP）相比，DDP 具有更高的效率和更好的可扩展性。</p>
<p><strong>DDP 的核心思想：</strong></p>
<ul>
<li><strong>多进程并行</strong>：为每个 GPU 启动一个独立的进程，每个进程负责在其 GPU 上执行模型的前向和反向传播。</li>
<li><strong>梯度同步</strong>：在反向传播过程中，各进程之间通过通信（如 NCCL 后端）同步梯度，确保模型参数在所有进程中保持一致。</li>
<li><strong>数据划分</strong>：使用分布式采样器（<code>DistributedSampler</code>），确保每个进程处理的数据不重叠，实现数据并行。</li>
</ul>
<h3 id="DDP-的执行流程"><a href="#DDP-的执行流程" class="headerlink" title="DDP 的执行流程"></a>DDP 的执行流程</h3><h4 id="1-准备阶段"><a href="#1-准备阶段" class="headerlink" title="1. 准备阶段"></a>1. <strong>准备阶段</strong></h4><h5 id="a-环境初始化"><a href="#a-环境初始化" class="headerlink" title="a. 环境初始化"></a>a. <strong>环境初始化</strong></h5><ul>
<li><strong>初始化进程组</strong>：使用 <code>torch.distributed.init_process_group</code>，指定通信后端（如 NCCL）、进程组名称等。</li>
<li><strong>设置设备</strong>：使用 <code>torch.cuda.set_device(local_rank)</code>，将当前进程绑定到指定的 GPU。</li>
</ul>
<h5 id="b-模型广播"><a href="#b-模型广播" class="headerlink" title="b. 模型广播"></a>b. <strong>模型广播</strong></h5><ul>
<li><strong>创建模型实例</strong>：在各个进程中创建模型实例，并将其移动到对应的 GPU 上。</li>
<li><strong>封装 DDP 模型</strong>：使用 <code>torch.nn.parallel.DistributedDataParallel</code> 封装模型。</li>
<li><strong>模型参数广播</strong>：DDP 会在后台自动将模型的参数和缓冲区从主进程广播到其他进程，确保模型初始状态一致。</li>
</ul>
<h5 id="c-注册梯度钩子"><a href="#c-注册梯度钩子" class="headerlink" title="c. 注册梯度钩子"></a>c. <strong>注册梯度钩子</strong></h5><ul>
<li><strong>Reducer 管理器</strong>：DDP 会为模型参数注册梯度钩子，在反向传播过程中自动进行梯度同步。</li>
</ul>
<h4 id="2-准备数据"><a href="#2-准备数据" class="headerlink" title="2. 准备数据"></a>2. <strong>准备数据</strong></h4><ul>
<li><strong>加载数据集</strong>：使用标准的 PyTorch 数据集或自定义数据集。</li>
<li><strong>创建分布式采样器</strong>：使用 <code>torch.utils.data.distributed.DistributedSampler</code>，确保每个进程加载的数据不重叠。</li>
<li><strong>创建数据加载器</strong>：将采样器传递给数据加载器，以便在每个 epoch 开始时正确地划分数据。</li>
</ul>
<h4 id="3-训练阶段"><a href="#3-训练阶段" class="headerlink" title="3. 训练阶段"></a>3. <strong>训练阶段</strong></h4><h5 id="a-前向传播"><a href="#a-前向传播" class="headerlink" title="a. 前向传播"></a>a. <strong>前向传播</strong></h5><ul>
<li><strong>模型前向计算</strong>：每个进程使用其本地数据执行模型的前向传播。</li>
<li><strong>同步参数和缓冲区</strong>：在初始阶段，DDP 已经同步了参数和缓冲区。在训练过程中，缓冲区（如 BatchNorm 的 <code>running_mean</code> 和 <code>running_var</code>）的更新也会被自动同步。</li>
</ul>
<h5 id="b-计算梯度"><a href="#b-计算梯度" class="headerlink" title="b. 计算梯度"></a>b. <strong>计算梯度</strong></h5><ul>
<li><strong>反向传播</strong>：每个进程独立计算梯度。</li>
<li><strong>梯度同步</strong>：DDP 在后台通过梯度钩子，使用异步的 All-Reduce 操作（如 NCCL）来平均梯度。</li>
<li><strong>更新梯度状态</strong>：当所有参数的梯度都被同步后，DDP 会将平均梯度写回参数的 <code>.grad</code> 属性。</li>
</ul>
<h5 id="c-参数更新"><a href="#c-参数更新" class="headerlink" title="c. 参数更新"></a>c. <strong>参数更新</strong></h5><ul>
<li><strong>优化器更新参数</strong>：使用优化器（如 SGD、Adam）更新模型参数。</li>
<li><strong>参数一致性</strong>：由于梯度已被同步，所有进程中的模型参数在更新后仍然保持一致。</li>
</ul>
<h4 id="4-循环训练"><a href="#4-循环训练" class="headerlink" title="4. 循环训练"></a>4. <strong>循环训练</strong></h4><ul>
<li><strong>重复上述步骤</strong>，直到完成所有的训练迭代。</li>
</ul>
<p><strong>Demo:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 基础模块 ### </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line">        cnt = torch.tensor(<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;cnt&#x27;</span>, cnt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="variable language_">self</span>.cnt += <span class="number">1</span></span><br><span class="line">        <span class="comment"># print(&quot;In forward: &quot;, self.cnt, &quot;Rank: &quot;, self.fc.weight.device)</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.fc(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, target</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx], <span class="variable language_">self</span>.target[idx]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 2. 初始化我们的模型、数据、各种配置  ####</span></span><br><span class="line"><span class="comment">## DDP：从外部得到local_rank参数。从外面得到local_rank参数，在调用DDP的时候，其会自动给出这个参数</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">FLAGS = parser.parse_args()</span><br><span class="line">local_rank = FLAGS.local_rank</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP：DDP backend初始化</span></span><br><span class="line">torch.cuda.set_device(local_rank)</span><br><span class="line">dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 假设我们有一些数据</span></span><br><span class="line">n_sample = <span class="number">100</span></span><br><span class="line">n_dim = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">25</span></span><br><span class="line">X = torch.randn(n_sample, n_dim)  <span class="comment"># 100个样本，每个样本有10个特征</span></span><br><span class="line">Y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (n_sample, )).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">dataset = SimpleDataset(X, Y)</span><br><span class="line">sampler = torch.utils.data.distributed.DistributedSampler(dataset)</span><br><span class="line">data_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 构造模型</span></span><br><span class="line">model = SimpleModel(n_dim).to(local_rank)</span><br><span class="line"><span class="comment">## DDP: Load模型要在构造DDP模型之前，且只需要在master上加载就行了。</span></span><br><span class="line">ckpt_path = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    model.load_state_dict(torch.load(ckpt_path))</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP: 构造DDP model —————— 必须在 init_process_group 之后才可以调用 DDP</span></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP: 要在构造DDP model之后，才能用model初始化optimizer。</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">loss_func = nn.BCELoss().to(local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 网络训练  ###</span></span><br><span class="line">model.train()</span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line">iterator = tqdm(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> iterator:</span><br><span class="line">    <span class="comment"># DDP：设置sampler的epoch，</span></span><br><span class="line">    <span class="comment"># DistributedSampler需要这个来指定shuffle方式，</span></span><br><span class="line">    <span class="comment"># 通过维持各个进程之间的相同随机数种子使不同进程能获得同样的shuffle效果。</span></span><br><span class="line">    data_loader.sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># 后面这部分，则与原来完全一致了。</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_loader:</span><br><span class="line">        data, label = data.to(local_rank), label.to(local_rank)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        prediction = model(data)</span><br><span class="line">        loss = loss_func(prediction, label.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        loss.backward()</span><br><span class="line">        iterator.desc = <span class="string">&quot;loss = %0.3f&quot;</span> % loss</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DDP:</span></span><br><span class="line">    <span class="comment"># 1. save模型的时候，和DP模式一样，有一个需要注意的点：保存的是model.module而不是model。</span></span><br><span class="line">    <span class="comment">#    因为model其实是DDP model，参数是被`model=DDP(model)`包起来的。</span></span><br><span class="line">    <span class="comment"># 2. 只需要在进程0上保存一次就行了，避免多次保存重复的东西。</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> epoch == num_epoch - <span class="number">1</span>:</span><br><span class="line">        torch.save(model.module.state_dict(), <span class="string">&quot;%d.ckpt&quot;</span> % epoch)</span><br></pre></td></tr></table></figure>
<p>结合上面的代码，一个简化版的DDP流程：</p>
<ol>
<li>读取DDP相关的配置，其中最关键的就是：<code>local_rank</code>；</li>
<li>DDP后端初始化：<code>dist.init_process_group</code>；</li>
<li>创建DDP模型，以及数据加载器。注意要为加载器创建分布式采样器（<code>DistributedSampler</code>）；</li>
<li>训练。</li>
</ol>
<p>DDP的通常启动方式：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">CUDA_VISIBLE_DEVICES</span>=<span class="string">&quot;0,1&quot;</span> python -m torch.distributed.launch --nproc_per_node <span class="number">2</span> ddp.py</span><br></pre></td></tr></table></figure>
<h3 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h3><p>以上过程中涉及到一些陌生的概念，其实走一遍DDP的过程就会很好理解：每个进程是一个独立的训练流程，不同进程之间共享同一份数据。为了避免不同进程使用重复的数据训练，以及训练后同步梯度，进程间需要同步。因此，其中一个重点就是每个进程序号，或者说使用的GPU的序号。</p>
<ul>
<li><code>node</code>：节点，可以是物理主机，也可以是容器；</li>
<li><code>rank</code>和<code>local_rank</code>：都表示进程在整个分布式任务中的编号。<code>rank</code>是进程在全局的编号，<code>local_rank</code>是进程在所在节点上的编号。显然，如果只有一个节点，那么二者是相等的。在启动脚本中的<code>--nproc_per_node</code>即指定一个节点上有多少进程；</li>
<li><code>world_size</code>：即整个分布式任务中进程的数量。</li>
</ul>
<p>你好！你对 <strong>DataParallel（DP）</strong> 和 <strong>DistributedDataParallel（DDP）</strong> 的区别做了一个很好的总结。确实，DP 和 DDP 在实现方式、性能和适用场景上都有显著的不同。在分布式训练的实际应用中，涉及到很多复杂的细节，例如梯度的同步方式、数据采样策略以及进程间的通信等。</p>
<p>让我进一步深入探讨你提到的几个关键点，以帮助你更全面地理解 DP 和 DDP 的工作机制。</p>
<hr>
<h2 id="DP-与-DDP-的区别"><a href="#DP-与-DDP-的区别" class="headerlink" title="DP 与 DDP 的区别"></a>DP 与 DDP 的区别</h2><h3 id="1-并行方式"><a href="#1-并行方式" class="headerlink" title="1. 并行方式"></a><strong>1. 并行方式</strong></h3><ul>
<li><p><strong>DataParallel（DP）</strong>：</p>
<ul>
<li><strong>单进程多线程</strong>：在一个进程中使用多线程实现并行计算。</li>
<li><strong>模型复制</strong>：在每个前向传播中，将模型复制到多个 GPU 上。</li>
<li><strong>数据划分</strong>：将输入数据划分成多个子批次，分别送入不同的 GPU。</li>
</ul>
</li>
<li><p><strong>DistributedDataParallel（DDP）</strong>：</p>
<ul>
<li><strong>多进程多线程</strong>：为每个 GPU 启动一个独立的进程。</li>
<li><strong>进程间通信</strong>：通过进程间通信（如 NCCL）来同步梯度和参数。</li>
<li><strong>更高的并行效率</strong>：避免了 Python 全局解释器锁（GIL）的影响，提升了计算效率。</li>
</ul>
</li>
</ul>
<h3 id="2-性能差异的原因"><a href="#2-性能差异的原因" class="headerlink" title="2. 性能差异的原因"></a><strong>2. 性能差异的原因</strong></h3><ul>
<li><p><strong>DP 通常比 DDP 慢，主要原因有：</strong></p>
<ol>
<li><p><strong>单进程的 GIL 限制</strong>：DP 使用多线程并行计算，但由于 Python 的 GIL，无法真正实现并行计算，特别是在计算密集型任务中。</p>
</li>
<li><p><strong>模型复制和数据划分的开销</strong>：DP 在每次前向传播时都需要将模型复制到各个 GPU，并划分数据，这会增加额外的开销。</p>
</li>
<li><p><strong>梯度汇总的瓶颈</strong>：DP 在反向传播时需要将各个 GPU 的梯度汇总到主 GPU，这可能导致通信瓶颈。</p>
</li>
</ol>
</li>
<li><p><strong>DDP 的优势：</strong></p>
<ul>
<li><p><strong>多进程并行，避免 GIL</strong>：每个进程独立运行，GIL 不再成为瓶颈。</p>
</li>
<li><p><strong>高效的梯度同步</strong>：使用 All-Reduce 操作，同步梯度更高效。</p>
</li>
<li><p><strong>通信开销更低</strong>：DDP 支持 Ring-AllReduce，通信成本随着 GPU 数量的增加而 <strong>相对固定</strong>，而 DP 的通信成本则随着 GPU 数量线性增长。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-适用性"><a href="#3-适用性" class="headerlink" title="3. 适用性"></a><strong>3. 适用性</strong></h3><ul>
<li><p><strong>DP 只能在单机上工作</strong>，适用于小规模的多 GPU 训练。</p>
</li>
<li><p><strong>DDP 可以在多机多卡上工作</strong>，适用于大规模的分布式训练。</p>
</li>
</ul>
<h3 id="4-模型并行的结合"><a href="#4-模型并行的结合" class="headerlink" title="4. 模型并行的结合"></a><strong>4. 模型并行的结合</strong></h3><ul>
<li><strong>DDP 可以与模型并行相结合</strong>：在需要模型并行的场景下，可以将模型的不同部分分配到不同的 GPU 上，同时使用 DDP 进行数据并行。</li>
</ul>
<hr>
<h2 id="二、DP-与-DDP-中梯度的回收方式"><a href="#二、DP-与-DDP-中梯度的回收方式" class="headerlink" title="二、DP 与 DDP 中梯度的回收方式"></a><strong>二、DP 与 DDP 中梯度的回收方式</strong></h2><h3 id="1-DP-中的梯度回收"><a href="#1-DP-中的梯度回收" class="headerlink" title="1. DP 中的梯度回收"></a><strong>1. DP 中的梯度回收</strong></h3><ul>
<li><p><strong>梯度计算</strong>：在每个 GPU 上，模型副本计算其子批次数据的梯度。</p>
</li>
<li><p><strong>梯度汇总</strong>：所有 GPU 的梯度会被收集到主 GPU（<code>device_ids[0]</code>）上，进行汇总。</p>
</li>
<li><p><strong>参数更新</strong>：在主 GPU 上更新模型参数。</p>
</li>
<li><p><strong>问题</strong>：</p>
<ul>
<li><p><strong>通信瓶颈</strong>：所有梯度都需要传输到主 GPU，通信量大。</p>
</li>
<li><p><strong>主 GPU 的负载过重</strong>：主 GPU 需要负责梯度汇总和参数更新，可能成为性能瓶颈。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-DDP-中的梯度回收"><a href="#2-DDP-中的梯度回收" class="headerlink" title="2. DDP 中的梯度回收"></a><strong>2. DDP 中的梯度回收</strong></h3><ul>
<li><p><strong>梯度计算</strong>：每个进程独立计算其负责的数据的梯度。</p>
</li>
<li><p><strong>梯度同步（All-Reduce 操作）</strong>：</p>
<ul>
<li><p><strong>All-Reduce</strong>：将所有进程的梯度进行求和，然后平均分发回每个进程。</p>
</li>
<li><p><strong>异步通信</strong>：DDP 采用异步的 All-Reduce 操作，可以与计算重叠，减少等待时间。</p>
</li>
</ul>
</li>
<li><p><strong>参数更新</strong>：每个进程使用同步后的平均梯度，更新本地的模型参数。</p>
</li>
<li><p><strong>优势</strong>：</p>
<ul>
<li><p><strong>通信效率高</strong>：All-Reduce 操作的通信开销相对固定，不会随着 GPU 数量的增加而线性增长。</p>
</li>
<li><p><strong>没有单点瓶颈</strong>：所有进程同时参与通信和计算，避免了主 GPU 的瓶颈。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-通信成本对比"><a href="#3-通信成本对比" class="headerlink" title="3. 通信成本对比"></a><strong>3. 通信成本对比</strong></h3><ul>
<li><p><strong>DP 的通信成本</strong>：</p>
<ul>
<li><p>随着 GPU 数量的增加，通信成本 <strong>线性增长</strong>。</p>
</li>
<li><p>主 GPU 需要收集和广播梯度，通信量大。</p>
</li>
</ul>
</li>
<li><p><strong>DDP 的通信成本</strong>：</p>
<ul>
<li><p>使用 Ring-AllReduce，通信成本 <strong>相对固定</strong>。</p>
</li>
<li><p>通信效率随着 GPU 数量的增加而 <strong>更高效</strong>。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="三、DDP-中数据采样的细节"><a href="#三、DDP-中数据采样的细节" class="headerlink" title="三、DDP 中数据采样的细节"></a><strong>三、DDP 中数据采样的细节</strong></h2><h3 id="1-为什么需要-DistributedSampler"><a href="#1-为什么需要-DistributedSampler" class="headerlink" title="1. 为什么需要 DistributedSampler"></a><strong>1. 为什么需要 <code>DistributedSampler</code></strong></h3><ul>
<li><p><strong>数据划分的必要性</strong>：在 DDP 中，每个进程都独立运行，为了避免不同进程处理相同的数据（数据重叠），需要确保每个进程处理的数据是互不重叠的子集。</p>
</li>
<li><p><strong><code>DistributedSampler</code> 的作用</strong>：</p>
<ul>
<li><p><strong>划分数据集</strong>：将数据集划分为若干份，每个进程处理其中一份。</p>
</li>
<li><p><strong>确保随机性一致</strong>：在每个 epoch 开始时，通过设置相同的随机种子，确保各进程的数据划分方式一致。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-DistributedSampler-的工作机制"><a href="#2-DistributedSampler-的工作机制" class="headerlink" title="2. DistributedSampler 的工作机制"></a><strong>2. <code>DistributedSampler</code> 的工作机制</strong></h3><ul>
<li><p><strong>分割数据集</strong>：根据 <code>world_size</code>（总进程数）和 <code>rank</code>（当前进程编号），计算当前进程应该处理的数据索引范围。</p>
</li>
<li><p><strong>处理数据不重叠</strong>：不同进程处理的数据索引范围不重叠，确保了数据并行。</p>
</li>
<li><p><strong>支持数据随机打乱</strong>：在每个 epoch，可以通过设置不同的随机种子，实现数据的随机打乱。</p>
</li>
</ul>
<h3 id="3-设置-sampler-set-epoch-epoch-的必要性"><a href="#3-设置-sampler-set-epoch-epoch-的必要性" class="headerlink" title="3. 设置 sampler.set_epoch(epoch) 的必要性"></a><strong>3. 设置 <code>sampler.set_epoch(epoch)</code> 的必要性</strong></h3><ul>
<li><p><strong>原因</strong>：</p>
<ul>
<li><p><strong>确保数据乱序的一致性</strong>：在每个 epoch 开始时，需要为 <code>DistributedSampler</code> 设置 epoch，以确保所有进程的数据乱序方式一致。</p>
</li>
<li><p><strong>避免数据重复或遗漏</strong>：不同进程在数据乱序时，如果不设置相同的种子，可能导致数据重复或遗漏，影响模型训练的正确性。</p>
</li>
</ul>
</li>
<li><p><strong>使用方法</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_loader.sampler.set_epoch(epoch)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="四、DDP-中的数据同步操作"><a href="#四、DDP-中的数据同步操作" class="headerlink" title="四、DDP 中的数据同步操作"></a><strong>四、DDP 中的数据同步操作</strong></h2><h3 id="1-模型参数和缓冲区的同步"><a href="#1-模型参数和缓冲区的同步" class="headerlink" title="1. 模型参数和缓冲区的同步"></a><strong>1. 模型参数和缓冲区的同步</strong></h3><ul>
<li><p><strong>初始同步</strong>：</p>
<ul>
<li><strong>参数广播</strong>：在 DDP 初始化时，自动将主进程（<code>rank == 0</code>）的模型参数和缓冲区广播到其他进程，确保所有进程的模型状态一致。</li>
</ul>
</li>
<li><p><strong>缓冲区的同步</strong>：</p>
<ul>
<li><p><strong>自动同步</strong>：在前向和反向传播过程中，DDP 会自动同步模型的缓冲区（如 BatchNorm 的 <code>running_mean</code> 和 <code>running_var</code>）。</p>
</li>
<li><p><strong>确保一致性</strong>：使得模型在所有进程中的缓冲区状态保持一致。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-梯度的同步（All-Reduce-操作）"><a href="#2-梯度的同步（All-Reduce-操作）" class="headerlink" title="2. 梯度的同步（All-Reduce 操作）"></a><strong>2. 梯度的同步（All-Reduce 操作）</strong></h3><ul>
<li><p><strong>注册梯度钩子</strong>：DDP 为每个模型参数注册了梯度钩子，当参数的梯度计算完成后，自动触发 All-Reduce 操作。</p>
</li>
<li><p><strong>All-Reduce 的过程</strong>：</p>
<ul>
<li><p><strong>梯度求和</strong>：将所有进程的对应参数的梯度相加。</p>
</li>
<li><p><strong>梯度平均</strong>：将总和除以进程数，得到平均梯度。</p>
</li>
<li><p><strong>同步更新</strong>：将平均梯度分发回各个进程，更新模型参数。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-通信操作的处理"><a href="#3-通信操作的处理" class="headerlink" title="3. 通信操作的处理"></a><strong>3. 通信操作的处理</strong></h3><ul>
<li><p><strong>通信后端</strong>：通常使用高效的通信库（如 NCCL）进行进程间通信。</p>
</li>
<li><p><strong>通信模式</strong>：</p>
<ul>
<li><p><strong>Broadcast（广播）</strong>：用于初始参数和缓冲区的同步。</p>
</li>
<li><p><strong>All-Reduce</strong>：用于梯度的同步。</p>
</li>
<li><p><strong>异步通信</strong>：DDP 采用异步通信机制，通信和计算可以重叠，减少等待时间。</p>
</li>
</ul>
</li>
<li><p><strong>用户无需干预</strong>：这些通信操作都由 DDP 在后台自动处理，用户不需要手动编写通信代码。</p>
</li>
</ul>
<hr>
<h2 id="五、基于真实需求的实践体会"><a href="#五、基于真实需求的实践体会" class="headerlink" title="五、基于真实需求的实践体会"></a><strong>五、基于真实需求的实践体会</strong></h2><ul>
<li><p><strong>复杂性与细节</strong>：在分布式训练中，涉及到很多复杂的细节，包括通信机制、数据同步、随机性控制等。</p>
</li>
<li><p><strong>实践的重要性</strong>：只有在真实的项目中，面对具体的需求和挑战，才能深入理解并解决分布式训练中的各种问题。</p>
</li>
<li><p><strong>建议</strong>：</p>
<ul>
<li><p><strong>深入学习 PyTorch 官方文档和示例</strong>：了解 DDP 的详细使用方法和注意事项。</p>
</li>
<li><p><strong>从小规模实验开始</strong>：先在单机多 GPU 环境下实践 DDP，熟悉其工作机制。</p>
</li>
<li><p><strong>逐步扩展到多机环境</strong>：在熟悉基本原理后，可以尝试在多机多卡的环境下进行训练，处理更多的实际问题。</p>
</li>
<li><p><strong>关注性能优化</strong>：在实践中，可以针对通信开销、数据加载效率、模型并行等方面进行优化，提升训练性能。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a><strong>六、总结</strong></h2><ul>
<li><p><strong>DP 与 DDP 的主要区别</strong>在于并行方式、通信机制和性能表现。</p>
</li>
<li><p><strong>DP 的局限性</strong>：</p>
<ul>
<li><p>受限于 GIL，无法充分利用多核 CPU 和多 GPU 的计算能力。</p>
</li>
<li><p>通信开销随着 GPU 数量线性增长，主 GPU 可能成为瓶颈。</p>
</li>
</ul>
</li>
<li><p><strong>DDP 的优势</strong>：</p>
<ul>
<li><p>采用多进程并行，避开 GIL 限制，充分利用硬件资源。</p>
</li>
<li><p>使用高效的 All-Reduce 操作，同步梯度和参数，通信开销低。</p>
</li>
<li><p>支持多机多卡，具有良好的扩展性。</p>
</li>
</ul>
</li>
<li><p><strong>实践中需要注意的细节</strong>：</p>
<ul>
<li><p>正确设置数据采样器，确保数据不重叠。</p>
</li>
<li><p>理解梯度同步和参数更新的机制。</p>
</li>
<li><p>熟悉 DDP 的启动和配置方法。</p>
</li>
</ul>
</li>
</ul>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/DP%E5%92%8CDDP/" data-id="cm4boxx5h0004ykfybz46f5ll" data-title="DP和DDP" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-千问相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:19:29.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/">千问相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="Qwen2-1-5B-模型结构"><a href="#Qwen2-1-5B-模型结构" class="headerlink" title="Qwen2-1.5B 模型结构"></a><code>Qwen2-1.5B</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2ForCausalLM</span>(</span><br><span class="line">  (model): <span class="built_in">Qwen2Model</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2DecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2SdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>Qwen2ForCausalLM</code> 模型主要由两大核心组件构成：</p>
<ol>
<li><strong>模型（<code>model</code>）</strong>：基于 Transformer 的核心架构，负责处理输入 Token 并生成上下文嵌入。</li>
<li><strong>语言建模头（<code>lm_head</code>）</strong>：将模型输出的嵌入转换为对应词汇的 Logits，支持 Token 预测。</li>
</ol>
<h3 id="值得注意的地方"><a href="#值得注意的地方" class="headerlink" title="值得注意的地方"></a>值得注意的地方</h3><ol>
<li><code>Qwen2RotaryEmbedding</code>: 在注意力机制中使用<strong>旋转位置编码</strong></li>
<li><code>Qwen2MLP</code>: 在MLP中使用了 <strong>门控 MLP（Gated MLP）</strong> 架构 <ul>
<li><code>up_proj</code> 主要负责扩展特征空间，使模型能够学习更复杂的表示</li>
<li><code>gate_proj</code> 主要生成控制信息流的门控信号</li>
<li>升维过程中的</li>
<li>生成门控信号，用于调节 MLP 内部的信息流</li>
</ul>
</li>
<li><code>Qwen2RMSNorm</code>: 归一化使用 <strong>RMSNorm（均方根归一化）</strong> <ul>
<li>RMSNorm 仅基于均方根（Root Mean Square, RMS）来进行归一化，而不计算均值。</li>
<li>对于输入向量 $\mathbf{x}$，RMSNorm 的计算公式为：<script type="math/tex; mode=display">
RMSNorm(x)=γ(xRMS(x)+ϵ)+β\text{RMSNorm}(\mathbf{x}) = \gamma \left( \frac{\mathbf{x}}{\text{RMS}(\mathbf{x}) + \epsilon} \right) + \beta</script>  其中，$\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2}$，$\gamma$ 和 $\beta$ 同样是可学习参数，$\epsilon$ 防止分母为零。</li>
<li><strong>特点</strong>：<ul>
<li><strong>计算效率</strong>：RMSNorm 省去了均值的计算，降低了计算复杂度，特别是在大规模模型中，这种优化可以显著减少训练和推理时间。</li>
<li><strong>性能表现</strong>：尽管 RMSNorm 忽略了均值，但在许多实践中，它能够提供与 LayerNorm 相近甚至更好的性能，尤其是在某些特定任务或架构中。</li>
<li><strong>稳定性</strong>：RMSNorm 通过仅依赖 RMS 进行规范化，可能在某些情况下提供更稳定的梯度流动，有助于训练过程的稳定性。</li>
</ul>
</li>
</ul>
</li>
<li><code>预归一化(Pre-Norm)</code> 和 <code>后归一化(Post-Norm)</code>: <code>Qwen2DecoderLayer</code> 中有两个 RMSNorm 层: <ul>
<li>input_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制和 MLP 之前</li>
<li>post_attention_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制之后，进入 MLP 之前</li>
<li>Transformer 架构中的归一化层可以放置在不同的位置，主要有两种常见的设计：<ul>
<li><strong>后归一化（Post-Norm）</strong>：<ul>
<li>归一化层位于子层（如自注意力层或 MLP 层）之后。</li>
<li>典型的 Transformer 论文如 “Attention is All You Need” 中采用此设计。</li>
<li>缺点：在非常深的模型中，可能导致梯度消失或梯度爆炸，影响训练稳定性。</li>
</ul>
</li>
<li><strong>预归一化（Pre-Norm）</strong>：<ul>
<li>归一化层位于子层之前。</li>
<li>这种设计有助于缓解深层模型中的梯度问题，提高训练的稳定性和效率。</li>
<li>近年来，越来越多的研究和实践表明，预归一化在深层模型中表现更佳。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>强化位置信息</code>: 在进入 <code>lm_head</code> 之前再次应用 <code>rotary_emb</code></li>
<li><code>SiLU (Sigmoid Linear Unit)</code> 激活函数</li>
</ol>
<h3 id="示例流程"><a href="#示例流程" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>文本输入</strong>：提示或部分文本通过 <code>embed_tokens</code> 层进行标记化并转化为嵌入。</li>
</ul>
</li>
<li><strong>模型推理</strong>：<ul>
<li>嵌入传递到堆叠的解码层，逐层应用自注意力、前馈网络和归一化，生成上下文嵌入。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li>最终嵌入通过 <code>lm_head</code> 转换为词汇表的 Logits。</li>
<li>对 Logits 应用 Softmax 获取下一个 Token 的概率分布。</li>
<li>选择概率最高的 Token（或使用采样策略如 top-k 或 nucleus 采样）生成下一个词。</li>
</ul>
</li>
<li><strong>迭代生成</strong>：<ul>
<li>将新生成的 Token 添加到输入序列，重复该过程，直到达到终止条件（例如序列结束 Token 或最大长度）。</li>
</ul>
</li>
</ol>
<h2 id="Qwen2-VL-2B-Instruct-模型结构"><a href="#Qwen2-VL-2B-Instruct-模型结构" class="headerlink" title="Qwen2-VL-2B-Instruct 模型结构"></a><code>Qwen2-VL-2B-Instruct</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2VLForConditionalGeneration</span>(</span><br><span class="line">  (visual): <span class="built_in">Qwen2VisionTransformerPretrainedModel</span>(</span><br><span class="line">    (patch_embed): <span class="built_in">PatchEmbed</span>(</span><br><span class="line">      (proj): <span class="built_in">Conv3d</span>(<span class="number">3</span>, <span class="number">1280</span>, kernel_size=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), stride=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): <span class="built_in">VisionRotaryEmbedding</span>()</span><br><span class="line">    (blocks): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">31</span>): <span class="number">32</span> x <span class="built_in">Qwen2VLVisionBlock</span>(</span><br><span class="line">        (norm1): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (norm2): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (attn): <span class="built_in">VisionSdpaAttention</span>(</span><br><span class="line">          (qkv): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">3840</span>, bias=True)</span><br><span class="line">          (proj): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">VisionMlp</span>(</span><br><span class="line">          (fc1): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">          (act): <span class="built_in">QuickGELUActivation</span>()</span><br><span class="line">          (fc2): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (merger): <span class="built_in">PatchMerger</span>(</span><br><span class="line">      (ln_q): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">      (mlp): <span class="built_in">Sequential</span>(</span><br><span class="line">        (<span class="number">0</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">        (<span class="number">1</span>): <span class="built_in">GELU</span>(approximate=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        (<span class="number">2</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (model): <span class="built_in">Qwen2VLModel</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2VLDecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2VLSdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由上 <code>Qwen2-VL-2B-Instruct</code> 模型主要由三个核心组件组成：</p>
<ol>
<li><strong>视觉模块（<code>visual</code>）</strong>：通过基于视觉 Transformer 的架构处理并编码视觉输入。</li>
<li><strong>语言模块（<code>model</code>）</strong>：通过堆叠的解码层处理文本输入并生成输出。</li>
<li><strong>条件生成头（<code>lm_head</code>）</strong>：将语言模块的输出转化为文本生成的词概率。</li>
</ol>
<h3 id="1-视觉模块-值得注意的地方"><a href="#1-视觉模块-值得注意的地方" class="headerlink" title="1. 视觉模块-值得注意的地方"></a>1. 视觉模块-值得注意的地方</h3><ol>
<li><code>Conv3d</code>: 使用 3D 卷积覆盖非重叠的区域实现Patchify<ul>
<li>将输入的3个通道映射到1280个特征通道</li>
<li>卷积核大小(kernel_size): <code>(2, 14, 14)</code>; 步幅(stride): <code>(2, 14, 14)</code></li>
<li><code>(N, 3, D, H, W) -&gt; (N, 1280, D_out, H_out, W_out)</code><ul>
<li>1280 是每个 Token 的嵌入维度（<code>embedding dimension</code>）。</li>
<li><strong><code>D_out * H_out * W_out</code></strong> 表示生成的 Token 数量</li>
<li>每个 Token 对应于输入图像中的一个 Patch</li>
</ul>
</li>
</ul>
</li>
<li><code>VisionRotaryEmbedding</code>: 视觉特征添加位置信息</li>
<li><code>LayerNorm</code>: 图像部分使用的是LN而非RMS, 但同样是Attn前后各一个(MLP 之前)</li>
<li><code>QuickGELUActivation</code> 激活函数: 位于两个线性层之间，作为 MLP 的非线性激活函数</li>
<li><code>PatchMerger</code>: 进行视觉token数的压缩与进一步提取特征(两层MLP):<ul>
<li><strong>减少 Patch 数量</strong>：合并相邻的 Patch，减少整体的 Token 数量</li>
<li><strong>增强特征表达和对齐维度</strong>：两层MLP提取特征, 同时对齐语言模型维度</li>
<li><strong>GELU激活函数</strong></li>
</ul>
</li>
</ol>
<h3 id="2-语言模块-值得注意的地方"><a href="#2-语言模块-值得注意的地方" class="headerlink" title="2. 语言模块-值得注意的地方"></a>2. 语言模块-值得注意的地方</h3><ol>
<li><code>词表大小</code>: 151657</li>
<li><code>embed_matrix维度</code>: [151,936, 1536]</li>
<li>其余注意事项 <code>同Qwen语言模型</code></li>
</ol>
<h3 id="示例流程-1"><a href="#示例流程-1" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>视觉输入</strong>：通过视觉模块处理，将其转化为 Patch 嵌入并编码空间信息。</li>
<li><strong>文本输入</strong>：通过语言模块的嵌入层将文本转化为特征向量。</li>
</ul>
</li>
<li><strong>条件生成</strong>：<ul>
<li>将视觉和文本信息整合到模型中，生成连贯且相关的输出。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li><code>lm_head</code> 将语言模块的输出转化为词概率，生成最终文本。</li>
</ul>
</li>
</ol>
<p>qwen2vl 的一大创新就来源于对 <code>Patch</code> 的处理</p>
<h3 id="详解一下-PatchEmbed"><a href="#详解一下-PatchEmbed" class="headerlink" title="详解一下 PatchEmbed"></a>详解一下 <code>PatchEmbed</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		patch_size: <span class="built_in">int</span> = <span class="number">14</span>,</span></span><br><span class="line"><span class="params">		temporal_patch_size: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">		in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">		embed_dim: <span class="built_in">int</span> = <span class="number">1152</span>,</span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">		<span class="variable language_">self</span>.temporal_patch_size = temporal_patch_size</span><br><span class="line">		<span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">		<span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">		  </span><br><span class="line">		kernel_size = [temporal_patch_size, patch_size, patch_size]</span><br><span class="line">		<span class="variable language_">self</span>.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=<span class="literal">False</span>)</span><br><span class="line">	  </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">		target_dtype = <span class="variable language_">self</span>.proj.weight.dtype</span><br><span class="line">		hidden_states = hidden_states.view(</span><br><span class="line">		-<span class="number">1</span>, <span class="variable language_">self</span>.in_channels, <span class="variable language_">self</span>.temporal_patch_size, <span class="variable language_">self</span>.patch_size, <span class="variable language_">self</span>.patch_size</span><br><span class="line">		)</span><br><span class="line">		hidden_states = <span class="variable language_">self</span>.proj(hidden_states.to(dtype=target_dtype)).view(-<span class="number">1</span>, <span class="variable language_">self</span>.embed_dim)</span><br><span class="line">		<span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<ul>
<li><p>输入时 <code>hidden_states</code> 维度为: [tokens=5704, dim=1176]</p>
<ul>
<li>想读懂qwen2vl是怎么处理图像视频数据的, 必须搞明白 <code>processor</code> 源码是如何处理的, 尤其是这个 <code>hidden_states</code> 维度</li>
<li>维度详情为: <code>(grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size)</code></li>
<li>可以视作确定了 tokens个数, 并且确定了后续 3D卷积 处理patch<ul>
<li>相当于后面的 3D卷积 只针对一个 patch 进行, 卷出来之后 <code>时间步</code>, <code>长</code>, <code>宽</code> 维度直接为降为1</li>
</ul>
</li>
</ul>
</li>
<li><p><code>hidden_states = hidden_states.view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)</code></p>
<ul>
<li>又将 <code>hidden_states</code> 后面一个维度拆回去</li>
</ul>
</li>
<li><p><code>hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)</code></p>
<ul>
<li>这是一个 <code>dim=1176 -&gt; self.embed_dim=1280</code> 过程</li>
<li>其中 <code>self.proj</code> 是一个 3D 卷积: <ul>
<li>in_channels=3</li>
<li>embed_dim=1280</li>
<li>kernel_size=[temporal_patch_size, patch_size, patch_size]</li>
<li>stride=[temporal_patch_size, patch_size, patch_size]</li>
<li>bias=False</li>
</ul>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>hidden_states.to(dtype=target_dtype).shape</p>
<ul>
<li>torch.Size([5704, 3, 2, 14, 14])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).shape</p>
<ul>
<li>torch.Size([5704, 1280, 1, 1, 1])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim).shape</p>
<ul>
<li>torch.Size([5704, 1280])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="详解一下-PatchMerger"><a href="#详解一下-PatchMerger" class="headerlink" title="详解一下 PatchMerger"></a>详解一下 <code>PatchMerger</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, context_dim: <span class="built_in">int</span>, spatial_merge_size: <span class="built_in">int</span> = <span class="number">2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	<span class="variable language_">self</span>.hidden_size = context_dim * (spatial_merge_size**<span class="number">2</span>)</span><br><span class="line">	<span class="variable language_">self</span>.ln_q = LayerNorm(context_dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">	<span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, <span class="variable language_">self</span>.hidden_size),</span><br><span class="line">		nn.GELU(),</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, dim),</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">	x = <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_q(x).view(-<span class="number">1</span>, <span class="variable language_">self</span>.hidden_size))</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>问题: 如果仔细阅读Qwen2VL的autoprocessor部分源码的话, 你会发现:</p>
<ul>
<li>tokenizer: 正常对文本部分进行分词, 使用”&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;”来进行初步的视频tokens记录</li>
<li>ImageProcessor: 按照 <code>时间步: 2, 长宽: 14x14</code> patchify</li>
<li>最终输出的inputs_id: 会将 “&lt;|image_pad|&gt;”等视觉pad变长, 但是实际长度却是 patchify 之后的 1/4, 这个原因就是来自于 <code>PatchMerger</code> 模块</li>
</ul>
<p>解答: <code>PatchMerger</code> 类用于将多个 patch 合并成一个更高维度的表示。这种合并操作会显著减少 patch 的数量。具体来说，<code>PatchMerger</code> 通过 <code>spatial_merge_size</code>（默认为 2）将相邻的 patch 合并(<code>十字相邻</code>)。例如，<code>spatial_merge_size=2</code> 表示每 2x2 的 patch 会被合并为一个新的 patch。因此，原本的 patch 数量会减少为原来的 1/4。</p>
<ul>
<li>重点在 <code>self.ln_q(x).view(-1, self.hidden_size)</code> 这行代码</li>
</ul>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><h2 id="视觉-语言-Vision-Language-VL-模型中有多个配置文件"><a href="#视觉-语言-Vision-Language-VL-模型中有多个配置文件" class="headerlink" title="视觉-语言(Vision-Language, VL)模型中有多个配置文件"></a>视觉-语言(Vision-Language, VL)模型中有多个配置文件</h2><ol>
<li><code>config.json</code><br> <code>config.json</code> 在模型初始化时被加载, 模型的主要配置文件，用于定义模型的架构和参数。它包含了模型的结构信息，使得模型能够根据这些配置正确地初始化和运行。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;architectures&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="string">&quot;Qwen2VLForConditionalGeneration&quot;</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;attention_dropout&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_start_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151652</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_end_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151653</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151654</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;image_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151655</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;video_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151656</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;hidden_act&quot;</span><span class="punctuation">:</span> <span class="string">&quot;silu&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;hidden_size&quot;</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;initializer_range&quot;</span><span class="punctuation">:</span> <span class="number">0.02</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;intermediate_size&quot;</span><span class="punctuation">:</span> <span class="number">8960</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;max_position_embeddings&quot;</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;max_window_layers&quot;</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;model_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;qwen2_vl&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_attention_heads&quot;</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_hidden_layers&quot;</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;num_key_value_heads&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rms_norm_eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-06</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rope_theta&quot;</span><span class="punctuation">:</span> <span class="number">1000000.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;sliding_window&quot;</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;tie_word_embeddings&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;torch_dtype&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bfloat16&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.41.2&quot;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;use_cache&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;use_sliding_window&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vision_config&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">		<span class="attr">&quot;depth&quot;</span><span class="punctuation">:</span> <span class="number">32</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;embed_dim&quot;</span><span class="punctuation">:</span> <span class="number">1280</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;mlp_ratio&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;num_heads&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;in_chans&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;hidden_size&quot;</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;patch_size&quot;</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;spatial_merge_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;spatial_patch_size&quot;</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;temporal_patch_size&quot;</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">	<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;rope_scaling&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">		<span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mrope&quot;</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">&quot;mrope_section&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span></span><br><span class="line">		<span class="punctuation">]</span></span><br><span class="line">	<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;vocab_size&quot;</span><span class="punctuation">:</span> <span class="number">151936</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>Qwen2VLConfig</code> 与 <code>LlavaConfig</code> 的初始化配置类略有不同</p>
<ul>
<li><code>LlavaConfig</code> 类可以单独接收 <code>vision_config</code>, <code>text_config</code> </li>
<li><code>Qwen2VLConfig</code> 类主要接受语言模型的配置参数，并通过 <code>vision_config</code> 参数嵌套包含视觉模型的配置, 可以直接传入json</li>
</ul>
<p><code>Qwen2VLConfig</code> 接收参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Qwen2VLConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">	model_type = <span class="string">&quot;qwen2_vl&quot;</span></span><br><span class="line">	keys_to_ignore_at_inference = [<span class="string">&quot;past_key_values&quot;</span>]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		vocab_size=<span class="number">152064</span>,</span></span><br><span class="line"><span class="params">		hidden_size=<span class="number">8192</span>,</span></span><br><span class="line"><span class="params">		intermediate_size=<span class="number">29568</span>,</span></span><br><span class="line"><span class="params">		num_hidden_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		num_attention_heads=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">		num_key_value_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">		hidden_act=<span class="string">&quot;silu&quot;</span>,</span></span><br><span class="line"><span class="params">		max_position_embeddings=<span class="number">32768</span>,</span></span><br><span class="line"><span class="params">		initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">		rms_norm_eps=<span class="number">1e-05</span>,</span></span><br><span class="line"><span class="params">		use_cache=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">		tie_word_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		rope_theta=<span class="number">1000000.0</span>,</span></span><br><span class="line"><span class="params">		use_sliding_window=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		sliding_window=<span class="number">4096</span>,</span></span><br><span class="line"><span class="params">		max_window_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		attention_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">		vision_config=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		rope_scaling=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		**kwargs,</span></span><br><span class="line"><span class="params">	</span>):</span><br></pre></td></tr></table></figure></p>
<p><code>Qwen2VLConfig</code> 官方示例:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLForConditionalGeneration, Qwen2VLConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a Qwen2VL style configuration</span></span><br><span class="line">configuration = Qwen2VLConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a model from the Qwen2-VL-7B style configuration</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(configuration)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accessing the model configuration</span></span><br><span class="line">configuration = model.config</span><br></pre></td></tr></table></figure></p>
<p>自定义配置:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLConfig, Qwen2VLForConditionalGeneration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 配置文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;path/to/your/config.json&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    config_dict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Qwen2VLConfig 实例</span></span><br><span class="line">qwen2vl_config = Qwen2VLConfig(**config_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化并加载 Qwen2VL 模型</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(qwen2vl_config)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<ol>
<li><code>generation_config.json</code><br> <code>generation_config.json</code> 在调用生成方法（如 <code>generate()</code>）时被加载, 专门用于定义文本生成过程中的超参数和策略。这些配置项控制生成文本的行为，如生成长度、采样策略、温度、束搜索等, 例如:<ul>
<li><strong>生成长度</strong>：如最大生成长度（<code>max_length</code>）、最小生成长度（<code>min_length</code>）等。</li>
<li><strong>生成策略</strong>：<ul>
<li><strong>采样相关</strong>：如温度（<code>temperature</code>）、顶部K采样（<code>top_k</code>）、顶部P采样（<code>top_p</code>）等。</li>
<li><strong>束搜索</strong>：束宽度（<code>num_beams</code>）、束惩罚因子（<code>repetition_penalty</code>）等。</li>
</ul>
</li>
<li><strong>其他生成参数</strong>：如是否使用核采样（<code>do_sample</code>）、停止标记（<code>eos_token_id</code>）等。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;pad_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;do_sample&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">151643</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">0.001</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;top_k&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.37.0&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>值得注意的一个地方是, 在仅使用 <code>qwen2vl_config = Qwen2VLConfig(**config_dict)</code> 也就是 config.json 初始化模型的时候, 模型也会有推理参数, 这是 huggingface 源码中 PretrainedConfig 类初始化的时候会给一个默认的参数字典:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_generation_defaults</span>() -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">	<span class="keyword">return</span> &#123;</span><br><span class="line">		<span class="string">&quot;max_length&quot;</span>: <span class="number">20</span>,</span><br><span class="line">		<span class="string">&quot;min_length&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;do_sample&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;early_stopping&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;num_beams&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;num_beam_groups&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;diversity_penalty&quot;</span>: <span class="number">0.0</span>,</span><br><span class="line">		<span class="string">&quot;temperature&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;top_k&quot;</span>: <span class="number">50</span>,</span><br><span class="line">		<span class="string">&quot;top_p&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;typical_p&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;repetition_penalty&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;length_penalty&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">&quot;no_repeat_ngram_size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;encoder_no_repeat_ngram_size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">&quot;bad_words_ids&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;num_return_sequences&quot;</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">&quot;output_scores&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;return_dict_in_generate&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;forced_bos_token_id&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;forced_eos_token_id&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;remove_invalid_values&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">&quot;exponential_decay_length_penalty&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;suppress_tokens&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">&quot;begin_suppress_tokens&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p><code>vocab.json</code><br> <code>vocab.json</code> 文件主要用于定义分词器的词汇表。它包含了模型可以识别和处理的所有词汇（tokens）及其对应的唯一标识符（IDs）。一些特殊tokens标记一般不会出现在这里    </p>
</li>
<li><p><code>tokenizer_config.json</code><br> <code>tokenizer_config.json</code> 文件用于存储分词器的高层配置参数。这些参数影响分词器的行为和处理方式, 如填充方式(<code>padding_side</code>)、添加特殊标记(<code>add_special_tokens</code>)、最大序列长度(<code>model_max_length</code>)等. 但不涉及具体的词汇映射或分词逻辑:</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&quot;add_prefix_space&quot;: false,</span><br><span class="line">	<span class="string">&quot;added_tokens_decoder&quot;</span>: &#123;</span><br><span class="line">		&quot;<span class="number">151643</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151644</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151645</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151646</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|object_ref_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151647</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|object_ref_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151648</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|box_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151649</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|box_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151650</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|quad_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151651</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|quad_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151652</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_start|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151653</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_end|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151654</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|vision_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151655</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|image_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;<span class="number">151656</span>&quot;: &#123;</span><br><span class="line">			&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;&lt;|video_pad|&gt;&quot;</span>,</span><br><span class="line">			<span class="string">&quot;lstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;normalized&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;rstrip&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;single_word&quot;</span>: false,</span><br><span class="line">			<span class="string">&quot;special&quot;</span>: true</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;additional_special_tokens&quot;: [<span class="string">&quot;&lt;|im_start|&gt;&quot;</span>, <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>, <span class="string">&quot;&lt;|object_ref_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|object_ref_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|box_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|box_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|quad_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|quad_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_start|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_end|&gt;&quot;</span>,<span class="string">&quot;&lt;|vision_pad|&gt;&quot;</span>,<span class="string">&quot;&lt;|image_pad|&gt;&quot;</span>,<span class="string">&quot;&lt;|video_pad|&gt;&quot;</span>],</span><br><span class="line">	<span class="string">&quot;bos_token&quot;</span>: null,</span><br><span class="line">	<span class="string">&quot;chat_template&quot;</span>: <span class="string">&quot;&#123;% set image_count = namespace(value=0) %&#125;&#123;% set video_count = namespace(value=0) %&#125;&#123;% for message in messages %&#125;&#123;% if loop.first and message[&#x27;role&#x27;] != &#x27;system&#x27; %&#125;&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&#123;% endif %&#125;&lt;|im_start|&gt;&#123;&#123; message[&#x27;role&#x27;] &#125;&#125;\n&#123;% if message[&#x27;content&#x27;] is string %&#125;&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&#123;% else %&#125;&#123;% for content in message[&#x27;content&#x27;] %&#125;&#123;% if content[&#x27;type&#x27;] == &#x27;image&#x27; or &#x27;image&#x27; in content or &#x27;image_url&#x27; in content %&#125;&#123;% set image_count.value = image_count.value + 1 %&#125;&#123;% if add_vision_id %&#125;Picture &#123;&#123; image_count.value &#125;&#125;: &#123;% endif %&#125;&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;&#123;% elif content[&#x27;type&#x27;] == &#x27;video&#x27; or &#x27;video&#x27; in content %&#125;&#123;% set video_count.value = video_count.value + 1 %&#125;&#123;% if add_vision_id %&#125;Video &#123;&#123; video_count.value &#125;&#125;: &#123;% endif %&#125;&lt;|vision_start|&gt;&lt;|video_pad|&gt;&lt;|vision_end|&gt;&#123;% elif &#x27;text&#x27; in content %&#125;&#123;&#123; content[&#x27;text&#x27;] &#125;&#125;&#123;% endif %&#125;&#123;% endfor %&#125;&lt;|im_end|&gt;\n&#123;% endif %&#125;&#123;% endfor %&#125;&#123;% if add_generation_prompt %&#125;&lt;|im_start|&gt;assistant\n&#123;% endif %&#125;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;clean_up_tokenization_spaces&quot;</span>: false,</span><br><span class="line">	<span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;padding_side&quot;</span>: <span class="string">&quot;left&quot;</span>,</span><br><span class="line">	<span class="string">&quot;errors&quot;</span>: <span class="string">&quot;replace&quot;</span>,</span><br><span class="line">	<span class="string">&quot;model_max_length&quot;</span>: <span class="number">32768</span>,</span><br><span class="line">	<span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">	<span class="string">&quot;split_special_tokens&quot;</span>: false,</span><br><span class="line">	<span class="string">&quot;tokenizer_class&quot;</span>: <span class="string">&quot;Qwen2Tokenizer&quot;</span>,</span><br><span class="line">	<span class="string">&quot;unk_token&quot;</span>: null</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tokenizer.json</code><br><code>tokenizer.json</code> 是一个综合性文件，通常包含了分词器的完整配置和分词逻辑。它不仅包含 <code>vocab.json</code> 和 <code>tokenizer_config.json</code> 的内容(但<code>tokenizer.json</code>中的add_tokens可能没有<code>tokenizer_config.json</code>中全)，还包括分词器的具体实现细节，如分词合并规则、正则表达式等, 结合 <code>tokenizer_config.json</code> 的内容，提供完整的分词器配置</p>
</li>
</ol>
<h3 id="vocab-json-tokenizer-json-和-tokenizer-config-json"><a href="#vocab-json-tokenizer-json-和-tokenizer-config-json" class="headerlink" title="vocab.json, tokenizer.json 和 tokenizer_config.json"></a><code>vocab.json</code>, <code>tokenizer.json</code> 和 <code>tokenizer_config.json</code></h3><ul>
<li><strong><code>vocab.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>vocab.json</code> 提供了词汇到ID的基础映射，是分词器不可或缺的一部分。</li>
<li><code>tokenizer.json</code> 将 <code>vocab.json</code> 嵌入其中，并结合分词规则（如BPE的合并规则）和行为参数，形成一个完整的分词器定义。</li>
</ul>
</li>
<li><strong><code>tokenizer_config.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>tokenizer_config.json</code> 专注于高层次的分词器配置参数，控制分词器的整体行为。</li>
<li><code>tokenizer.json</code> 不仅包含 <code>tokenizer_config.json</code> 的内容，还包括具体的分词逻辑和词汇表，是一个更全面的配置文件。</li>
</ul>
</li>
</ul>
<h2 id="Qwen2-1-5B-config"><a href="#Qwen2-1-5B-config" class="headerlink" title="Qwen2-1.5B config"></a><code>Qwen2-1.5B</code> config</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Qwen2Config &#123;</span><br><span class="line">  &quot;_attn_implementation_autoset&quot;: true,</span><br><span class="line">  <span class="string">&quot;_name_or_path&quot;</span>: <span class="string">&quot;/mnt/nas/ianli/models/Qwen2-1.5B&quot;</span>,</span><br><span class="line">  <span class="string">&quot;architectures&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;Qwen2ForCausalLM&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;attention_dropout&quot;</span>: <span class="number">0.0</span>,</span><br><span class="line">  <span class="string">&quot;bos_token_id&quot;</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">&quot;eos_token_id&quot;</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;silu&quot;</span>,</span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: <span class="number">1536</span>,</span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: <span class="number">8960</span>,</span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: <span class="number">131072</span>,</span><br><span class="line">  <span class="string">&quot;max_window_layers&quot;</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">&quot;model_type&quot;</span>: <span class="string">&quot;qwen2&quot;</span>,</span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">&quot;num_key_value_heads&quot;</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">&quot;rms_norm_eps&quot;</span>: <span class="number">1</span>e-<span class="number">06</span>,</span><br><span class="line">  <span class="string">&quot;rope_scaling&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;rope_theta&quot;</span>: <span class="number">1000000.0</span>,</span><br><span class="line">  <span class="string">&quot;sliding_window&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;tie_word_embeddings&quot;</span>: true,</span><br><span class="line">  <span class="string">&quot;torch_dtype&quot;</span>: <span class="string">&quot;bfloat16&quot;</span>,</span><br><span class="line">  <span class="string">&quot;transformers_version&quot;</span>: <span class="string">&quot;4.46.3&quot;</span>,</span><br><span class="line">  <span class="string">&quot;use_cache&quot;</span>: true,</span><br><span class="line">  <span class="string">&quot;use_sliding_window&quot;</span>: false,</span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: <span class="number">151936</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="关于Qwen2VL的训练构想与需求"><a href="#关于Qwen2VL的训练构想与需求" class="headerlink" title="关于Qwen2VL的训练构想与需求"></a>关于Qwen2VL的训练构想与需求</h1><ol>
<li><strong>网络结构</strong>: 继承 Qwen2VL 的 <code>visual部分</code> 网络结构, 修改Qwen2VL的 <code>语言部分</code> 结构与 <code>陈老师的天文语言模型</code> 结构相一致</li>
<li><strong>Tokenizer</strong>: 注意修改语言模型 <code>tokenizer</code> 中的 <code>special_tokens</code>, 尤其是添加相关视觉的 <code>special_tokens</code> 与 Qwen2VL 保持一致</li>
<li><strong>模型权重</strong>:  <code>visual部分</code> 加载所继承的 Qwen2VL 的视觉权重, <code>语言部分</code> 加载 <code>陈老师的天文语言模型</code> </li>
<li><strong>训练框架</strong>: <ul>
<li>方案一: 将修改完 网络结构 和 参数权重 的模型保存好, 基于现有微调框架进行训练<ul>
<li>问题: 可能存在兼容性的问题, 现在能想到的主要兼容问题: <ul>
<li>现有微调框架对于网络结构的继承</li>
</ul>
</li>
</ul>
</li>
<li>方案二: 训练对于直接使用现有框架而言灵活度要求更高, 同时为了后续可拓展性:<ul>
<li>计划基于 Huggingface 直接搭建训练框架(正在进行)</li>
</ul>
</li>
</ul>
</li>
<li><strong>数据要求</strong>: <ol>
<li>通用图文对<ul>
<li>因为重构了原VL模型的结构和权重, 故而该模型训练还有 视觉 和 语言 对齐的任务存在, 故而需要有大规模高质量的图文数据对支撑</li>
<li>需求(正在调研)</li>
</ul>
</li>
<li>天文图文对<ul>
<li>Apod网站爬虫数据集提供了较高质量了天文图文对</li>
<li>天文微调图文对, 使用目前已有的填空、选择、简答</li>
</ul>
</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" data-id="cm4boxx5h0006ykfygy3168pe" data-title="千问相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LLaMa系列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T12:10:20.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/">LLaMa系列</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-LLaMa系列"><a href="#1-LLaMa系列" class="headerlink" title="1. LLaMa系列"></a>1. LLaMa系列</h1><h2 id="1-1-原始llama"><a href="#1-1-原始llama" class="headerlink" title="1.1 原始llama"></a>1.1 原始llama</h2><h3 id="1-1-1-简介"><a href="#1-1-1-简介" class="headerlink" title="1.1.1 简介"></a>1.1.1 简介</h3><ul>
<li><p><strong>LLama 模型</strong>集合由 <strong>Meta AI</strong> 于 2023 年 2 月推出， 包括四种尺寸(7B 、13B 、30B 和 65B)。上下文长度<strong>2048</strong>.</p>
</li>
<li><p>Meta在2023年7月发布了免费可商用版本 <strong>Llama-2</strong>，有7B、13B、34B和70B四个参数量版本，除了34B模型均已开源。模型的上下文长度从<strong>2048</strong>翻倍到了<strong>4096</strong>.</p>
</li>
<li><p>进入2024年4月18日，Meta发布 <strong>LLama 3</strong>。LLama 3在技术层面实现了重大突破，其最大版本的参数量飙升至超过<strong>405B</strong></p>
</li>
<li><p>7月23日, <strong>LLama3.1</strong>发布, 上下文长度可达128k, 几乎就是一本书的长度了</p>
</li>
</ul>
<h3 id="1-1-2-沿革"><a href="#1-1-2-沿革" class="headerlink" title="1.1.2 沿革"></a>1.1.2 沿革</h3><h4 id="1-1-2-1-模型结构沿革"><a href="#1-1-2-1-模型结构沿革" class="headerlink" title="1.1.2.1 模型结构沿革"></a>1.1.2.1 模型结构沿革</h4><ul>
<li><p><strong>llama1</strong>, 先上图:<br><img src="img/llama1-model.png" alt="llama1 模型结构"><br>值得一讲的模块有<code>RMSNorm</code>, <code>Causal Mask</code>, <code>RoPE</code>, 以及上面的<code>Swi-GLU</code>. 那咱就结合代码一点点搞吧</p>
<ul>
<li><p><strong><code>RMSNorm</code></strong> 其主要基于<code>LayerNorm</code>移除了re-center操作, 并且通过实验证明之做到了“降本不降效”(计算时间减少7%-64%不等). 具体公式如下, RMS就是均方误差根:<br>$y = \frac{x - \mathbb{E}(x)}{\sqrt{\text{Var}(x) + \epsilon}} \cdot \gamma + \beta$</p>
<p>$\bar{a}_i = \frac{a_i}{\text{RMS}(a)} g_i, \quad \text{RMS}(a) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}$</p>
<p>现在来看看代码, <code>LN</code>就不看了, 只看<code>RMSNorm</code>的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d, p=-<span class="number">1.</span>, eps=<span class="number">1e-8</span>, bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Root Mean Square Layer Normalization</span></span><br><span class="line"><span class="string">        :param d: model size</span></span><br><span class="line"><span class="string">        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)</span></span><br><span class="line"><span class="string">        :param eps:  epsilon value, default 1e-8</span></span><br><span class="line"><span class="string">        :param bias: whether use bias term for RMSNorm, disabled by</span></span><br><span class="line"><span class="string">            default because RMSNorm doesn&#x27;t enforce re-centering invariance.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(RMSNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.d = d</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(d))</span><br><span class="line">        <span class="variable language_">self</span>.register_parameter(<span class="string">&quot;scale&quot;</span>, <span class="variable language_">self</span>.scale)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="variable language_">self</span>.offset = nn.Parameter(torch.zeros(d))</span><br><span class="line">            <span class="variable language_">self</span>.register_parameter(<span class="string">&quot;offset&quot;</span>, <span class="variable language_">self</span>.offset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.p &lt; <span class="number">0.</span> <span class="keyword">or</span> <span class="variable language_">self</span>.p &gt; <span class="number">1.</span>:</span><br><span class="line">            norm_x = x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = <span class="variable language_">self</span>.d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            partial_size = <span class="built_in">int</span>(<span class="variable language_">self</span>.d * <span class="variable language_">self</span>.p)</span><br><span class="line">            partial_x, _ = torch.split(x, [partial_size, <span class="variable language_">self</span>.d - partial_size], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            norm_x = partial_x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = partial_size</span><br><span class="line"></span><br><span class="line">        rms_x = norm_x * d_x ** (-<span class="number">1.</span> / <span class="number">2</span>)</span><br><span class="line">        x_normed = x / (rms_x + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.scale * x_normed + <span class="variable language_">self</span>.offset</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.scale * x_normed</span><br></pre></td></tr></table></figure>
<p>值得注意在实际应用中还多了一个<code>partial</code>选项, 另一半被partial掉的就不参与计算了, 怀疑一个深意是实验结果表明RMS没必要在token的d维度全部(换言之所有头)做计算. <strong>下去琢磨琢磨</strong> </p>
</li>
<li><p><code>causal_mask</code>(顺道提一嘴BERT里的<code>label=-100</code>)</p>
<ul>
<li>上图中<code>causal_mask</code>实际上是一个shape为<code>num_tokens_per_sample</code>*<code>num_tokens_per_sample</code>的下三角矩阵, 下三角的值你既可以填<code>-10000</code>也可以填<code>-inf</code>.</li>
<li>这种 <strong>causal mask</strong>真的十分巧妙, 每个token只能看到它后面的token了, 让decoder-only结构的模型训练如此高效!</li>
<li>具体而言,</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">q = <span class="variable language_">self</span>.query(x)</span><br><span class="line">k = <span class="variable language_">self</span>.key(x)</span><br><span class="line">v = <span class="variable language_">self</span>.value(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头</span></span><br><span class="line">q = rearrange(q, <span class="string">&#x27;b p (n_he d_h) -&gt; b n_he p d_h&#x27;</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line">k = rearrange(k, <span class="string">&#x27;b p (n_he d_h) -&gt; b n_he p d_h&#x27;</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line">v = rearrange(v, <span class="string">&#x27;b p (n_he d_h) -&gt; b n_he p d_h&#x27;</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">qk = torch.matmul(q, k.transpose(-<span class="number">1</span>,-<span class="number">2</span>))*mask</span><br><span class="line"></span><br><span class="line">qk = F.softmax(qk, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">qkv = torch.matmul(qk, v.transpose(-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rearrange(qkv, <span class="string">&#x27;b n_he p d_h -&gt; b p (n_he d_h)&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>RoPE</code>, 一图胜千言:<br><img src="./img/rope-公式.png" alt="RoPE公式"><br><img src="./img/rope-形象理解.png" alt="RoPE示意"></p>
</li>
<li><p><code>Swi-GLU</code>, 顺便复习下glu<br><img src="./img/swiglu.png" alt="SwiGLU"></p>
<blockquote>
<p>GLU(Gated Linear Units,门控线性单元)引入了两个不同的线性层，其中一个首先经过sigmoid函数，其结果将和另一个线性层的输出进行逐元素相乘作为最终的输出<br>$\text{GLU}(x, W, V, b, c) = \sigma(xW + b) \otimes (xV + c)$</p>
</blockquote>
<p>SiLU就不用说了,$x*\sigma(\beta x)$, 经典激活函数</p>
<p><code>up</code>和<code>down</code>一般是四倍的上下采样线形层. </p>
</li>
</ul>
</li>
<li><p><strong>llama2</strong><br>  核心就是引入了<strong>GQA</strong>,见下图:<br>  <img src="./img/llama2-model.png" alt="llama2-model"></p>
<p>  虽然现在的算力, 一个sample 4096个token根本算不上什么, 但还是一讲, 先上图:<br>  <img src="./img/GQA.png" alt="GQA"></p>
<ul>
<li>GQA-1：一个单独的组，等同于 Multi-Query Attention (MQA)。</li>
<li>GQA-H：组数等于头数，基本上与 Multi-Head Attention (MHA) 相同。</li>
<li><p>GQA-G：一个中间配置，具有G个组，平衡了效率和表达能力。</p>
<p>代码也没必要放了, 已加入torch全家桶, 就是把<code>v</code>,<code>k</code>用<code>torch.chunk</code> 了一下, 不过我们有必要顺带讲一嘴滑动窗口注意力:</p>
<blockquote>
<p>每一个token只和包含其本身在内的前W个token做Attention。最简单的实现其实就是给不需要计算attention的其它token都加上一个mask就可以了</p>
</blockquote>
<p>一图胜千言:<br><img src="./img/slide-window-attention.png" alt="SWA"></p>
</li>
</ul>
</li>
<li><p><strong>llama3</strong><br>似乎在结构上没有大的创新(这就是scaling的威力吗…), 不过引入了“<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fairscale/blob/main/fairscale/nn/model_parallel/layers.py">parallel layers from Fairscale</a>”去并行加速矩阵运算, <strong>有时间看看,并加到这或者单开一章</strong></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" data-id="cm4bei79k0000xufyhx4kd9b6" data-title="LLaMa系列" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Magvit系列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2024-11-23T04:38:26.000Z" itemprop="datePublished">2024-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/11/23/Magvit%E7%B3%BB%E5%88%97/">Open-MAGVIT-2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="MAGVIT1-amp-2-Google-amp-腾讯的实现"><a href="#MAGVIT1-amp-2-Google-amp-腾讯的实现" class="headerlink" title="MAGVIT1&2(Google&腾讯的实现)"></a>MAGVIT1&amp;2(Google&amp;腾讯的实现)</h1><h2 id="MAGVIT-v1-Google"><a href="#MAGVIT-v1-Google" class="headerlink" title="MAGVIT-v1 (Google)"></a>MAGVIT-v1 (Google)</h2><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p><img src="./img/magvit-v1-framework.png" alt="Magvit-v1 framework"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a target="_blank" rel="noopener" href="https://github.com/google-research/magvit">源码地址</a></p>
<p>这是个20年的框架,作者主要用了jax. 所以还是得调研下jax有什么好处, 以及现在生态怎么样了. (<strong>先码住, 以后填坑</strong>)</p>
<h4 id="VQ-tokenizer部分"><a href="#VQ-tokenizer部分" class="headerlink" title="VQ-tokenizer部分"></a>VQ-tokenizer部分</h4><p><img src="./img/magvit-v1-vqtokenizer.png" alt="Magvit-v1 tokenizer"></p>
<p>其结构图如上, 下面上代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQVAE</span>(nn.Module):</span><br><span class="line">  <span class="string">"""VQ-VAE model."""</span></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  precision: <span class="type">Any</span> = jax.lax.Precision.DEFAULT</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">"""VQ-VAE setup."""</span></span><br><span class="line">    quantizer_str = <span class="variable language_">self</span>.config.vqvae.get(</span><br><span class="line">        <span class="string">'vector_quantizer_class'</span>, <span class="string">'VectorQuantizer'</span>)</span><br><span class="line">    <span class="keyword">if</span> quantizer_str == <span class="string">'VectorQuantizer'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.quantizer = VectorQuantizer(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, precision=<span class="variable language_">self</span>.precision, dtype=<span class="variable language_">self</span>.dtype</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(quantizer_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Encoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Decoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'3dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_3dcnn.Encoder(config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_3dcnn.Decoder(config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2plus1dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_2plus1dcnn.Encoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_2plus1dcnn.Decoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">          <span class="string">f'Architecture <span class="subst">{self.config.vqvae.architecture}</span>'</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>先分析<code>enc_dec_3dcnn.Encoder</code>和<code>enc_dec_3dcnn.Decoder</code>这俩:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Encoder Blocks."""</span></span><br><span class="line"></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="variable language_">self</span>.filters = <span class="variable language_">self</span>.config.vqvae.filters</span><br><span class="line">    <span class="variable language_">self</span>.num_res_blocks = <span class="variable language_">self</span>.config.vqvae.num_enc_res_blocks</span><br><span class="line">    <span class="variable language_">self</span>.channel_multipliers = <span class="variable language_">self</span>.config.vqvae.channel_multipliers</span><br><span class="line">    <span class="variable language_">self</span>.temporal_downsample = <span class="variable language_">self</span>.config.vqvae.temporal_downsample</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.temporal_downsample, <span class="built_in">int</span>):</span><br><span class="line">      <span class="variable language_">self</span>.temporal_downsample = _get_selected_flags(</span><br><span class="line">          <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers) - <span class="number">1</span>, <span class="variable language_">self</span>.temporal_downsample, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>.embedding_dim = <span class="variable language_">self</span>.config.vqvae.embedding_dim</span><br><span class="line">    <span class="variable language_">self</span>.conv_downsample = <span class="variable language_">self</span>.config.vqvae.conv_downsample</span><br><span class="line">    <span class="variable language_">self</span>.custom_conv_padding = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'custom_conv_padding'</span>)</span><br><span class="line">    <span class="variable language_">self</span>.norm_type = <span class="variable language_">self</span>.config.vqvae.norm_type</span><br><span class="line">    <span class="variable language_">self</span>.num_remat_block = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'num_enc_remat_blocks'</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'relu'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.relu</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'swish'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.swish</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>从其setup来看, 也是resnet作encoder-decoder那一套. 从下述代码, 我们可以大概归纳出一个层级结构:</p>
<pre><code>Encoder
|---conv_fn
|    |
|---Block (Before)
|    |----ResBlock * N
|    |----conv_downsample
|---Block (last)
|    |----ResBlock * N
|---norm-act-conv_fn
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@nn.compact</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, *, is_train=<span class="literal">False</span></span>):</span><br><span class="line">  conv_fn = functools.partial(</span><br><span class="line">      model_utils.Conv,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">      custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line">  norm_fn = model_utils.get_norm_layer(</span><br><span class="line">      norm_type=<span class="variable language_">self</span>.norm_type, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">  block_args = <span class="built_in">dict</span>(</span><br><span class="line">      norm_fn=norm_fn,</span><br><span class="line">      conv_fn=conv_fn,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      activation_fn=<span class="variable language_">self</span>.activation_fn,</span><br><span class="line">      use_conv_shortcut=<span class="literal">False</span>,</span><br><span class="line">  )</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">  filters = <span class="variable language_">self</span>.filters</span><br><span class="line">  num_blocks = <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">    filters = <span class="variable language_">self</span>.filters * <span class="variable language_">self</span>.channel_multipliers[i]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">      <span class="keyword">if</span> i &lt; <span class="variable language_">self</span>.num_remat_block <span class="keyword">and</span> is_train:</span><br><span class="line">        x = ResBlock(filters, **block_args).remat_call(x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = ResBlock(filters, **block_args)(x)</span><br><span class="line">    <span class="keyword">if</span> i &lt; num_blocks - <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.conv_downsample:</span><br><span class="line">        t_stride = <span class="number">2</span> <span class="keyword">if</span> <span class="variable language_">self</span>.temporal_downsample[i] <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        x = conv_fn(</span><br><span class="line">            filters, kernel_size=(<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>), strides=(t_stride, <span class="number">2</span>, <span class="number">2</span>))(</span><br><span class="line">                x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = model_utils.downsample(x, <span class="variable language_">self</span>.temporal_downsample[i])</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">    x = ResBlock(filters, **block_args)(x)</span><br><span class="line">  x = norm_fn()(x)</span><br><span class="line">  x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.embedding_dim, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>核心就在于它的ResBlock怎么写的,不过应该大差不差.</p>
<pre><code>ResBlock
|
|---norm-act-conv_fn(3x3x3, stride=1)
|    |
|---norm-act-conv_fn(3x3x3, stride=1)
</code></pre><p>在细看<code>ResBlock</code>前,紧急插播一下<code>conv_fn</code>的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">conv_fn = functools.partial(</span><br><span class="line">    model_utils.Conv,</span><br><span class="line">    dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">    padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">    custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(nn.Conv):</span><br><span class="line">  <span class="string">"""Convolution with custom padding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Attributes:</span></span><br><span class="line"><span class="string">    custom_padding: padding mode accepted by jnp.pad. When using this, must set</span></span><br><span class="line"><span class="string">      padding=VALID to disable padding in nn.Conv.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  custom_padding: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.padding == <span class="string">'VALID'</span>, <span class="string">'Must use VALID padding for raw Conv.'</span></span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.kernel_dilation <span class="keyword">in</span> (<span class="number">1</span>, <span class="literal">None</span>), <span class="string">'Kernel dilation not supported.'</span></span><br><span class="line">      pads = [((k - <span class="number">1</span>) // <span class="number">2</span>, k // <span class="number">2</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="variable language_">self</span>.kernel_size]</span><br><span class="line">      pads = [(<span class="number">0</span>, <span class="number">0</span>)] + pads + [(<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding.startswith(</span><br><span class="line">          <span class="string">'reflect_'</span>) <span class="keyword">or</span> <span class="variable language_">self</span>.custom_padding.startswith(<span class="string">'symmetric_'</span>):</span><br><span class="line">        custom_padding, reflect_type = <span class="variable language_">self</span>.custom_padding.split(<span class="string">'_'</span>)</span><br><span class="line">        pad_kwargs = {<span class="string">'reflect_type'</span>: reflect_type}</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        custom_padding = <span class="variable language_">self</span>.custom_padding</span><br><span class="line">        pad_kwargs = {}</span><br><span class="line">      x = jnp.pad(x, pads, mode=custom_padding, **pad_kwargs)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">super</span>(Conv, <span class="variable language_">self</span>).__call__(x)</span><br></pre></td></tr></table></figure>
<p>下面我们来看ResBlock的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Basic Residual Block."""</span></span><br><span class="line">  filters: <span class="built_in">int</span></span><br><span class="line">  norm_fn: <span class="type">Any</span></span><br><span class="line">  conv_fn: <span class="type">Any</span></span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  use_conv_shortcut: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    input_dim = x.shape[-<span class="number">1</span>]</span><br><span class="line">    residual = x</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> input_dim != <span class="variable language_">self</span>.filters:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.use_conv_shortcut:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">    <span class="keyword">return</span> x + residual</span><br></pre></td></tr></table></figure>
<p>由上可见,似乎没有用瓶颈层, 而且有个问题: <strong>input_dim != self.filters:</strong>时不应该直接报错的吗? 答案是不会的, <code>self.filters</code>是out_channel, 至于in_channel, 会自动适应的.</p>
<h4 id="MLM-Masked-Language-Model-部分"><a href="#MLM-Masked-Language-Model-部分" class="headerlink" title="MLM(Masked Language Model)部分"></a>MLM(Masked Language Model)部分</h4><p>很惭愧,我找了半天没找到他们的template command line, 因此只能猜 <code>videogvt/trainers/maskgvt_trainer.py</code> 和 <code>videogvt/trainers/lmgvt_trainer.py</code>是训练脚本.</p>
<p>先看<code>maskgvt_trainer.py</code> </p>
<p>太复杂了,咱就记住<code>flax_model</code>是bert, 最重要的是<code>train_step</code>就行了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_loss_fn</span>(<span class="params">params</span>):</span><br><span class="line">  variables = {<span class="string">'params'</span>: params, **train_state.model_state}</span><br><span class="line">  logits, new_model_state = flax_model.apply(</span><br><span class="line">      variables,</span><br><span class="line">      batch_tokens[<span class="string">'masked_inputs'</span>],</span><br><span class="line">      batch_tokens[<span class="string">'segment_ids'</span>],</span><br><span class="line">      deterministic=<span class="literal">False</span>,</span><br><span class="line">      mutable=mutable,</span><br><span class="line">      rngs={<span class="string">'dropout'</span>: dropout_rng})</span><br><span class="line">  <span class="comment"># logits shape [bs, 1 + (l_cond) + l_t * l_h * l_w,</span></span><br><span class="line">  <span class="comment">#               vq_codebook_size + num_classes + num_special_tokens]</span></span><br><span class="line">  logits = logits[:, :, :vq_codebook_size]</span><br><span class="line">  <span class="comment"># TODO(roadjiang): add classification loss. Use batch_tokens['weights'][0]</span></span><br><span class="line">  one_hot_targets = jax.nn.one_hot(batch_tokens[<span class="string">'targets'</span>], logits.shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  sof_ce_loss = (</span><br><span class="line">      scenic_model_utils.weighted_unnormalized_softmax_cross_entropy(</span><br><span class="line">          logits,</span><br><span class="line">          one_hot_targets,</span><br><span class="line">          weights=batch.get(<span class="string">'batch_mask'</span>),</span><br><span class="line">          label_smoothing=config.get(<span class="string">'label_smoothing'</span>),</span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  weights = batch_tokens[<span class="string">'weights'</span>]</span><br><span class="line">  masked_sof_ce_loss = jnp.<span class="built_in">sum</span>(</span><br><span class="line">      sof_ce_loss * weights, axis=-<span class="number">1</span>) / (</span><br><span class="line">          jnp.<span class="built_in">sum</span>(weights, axis=-<span class="number">1</span>) + <span class="number">1e-8</span>)</span><br><span class="line">  token_loss = jnp.mean(masked_sof_ce_loss)</span><br><span class="line"></span><br><span class="line">  l2_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> config.get(<span class="string">'l2_decay_factor'</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    total_loss = token_loss</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    l2_loss = scenic_model_utils.l2_regularization(params)</span><br><span class="line">    total_loss = token_loss + <span class="number">0.5</span> * config.l2_decay_factor * l2_loss</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> total_loss, (new_model_state, token_loss, l2_loss)</span><br></pre></td></tr></table></figure>
<p>LM那就是BERT, 看一看<code>videogvt/models/simplified_bert.py</code>就行了. 注意,人家的logits不是分类头分出来的,而是和词表乘出来的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertMlmLayer</span>(nn.Module):</span><br><span class="line">  <span class="string">"""BERT layer for masked token prediction."""</span></span><br><span class="line"></span><br><span class="line">  hidden_size: <span class="built_in">int</span></span><br><span class="line">  initializer_fn: InitializerType</span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params"></span></span><br><span class="line"><span class="params">      self, last_layer: jnp.ndarray, embeddings: jnp.ndarray</span></span><br><span class="line"><span class="params">  </span>) -&gt; jnp.ndarray:</span><br><span class="line">    mlm_hidden = nn.Dense(</span><br><span class="line">        features=<span class="variable language_">self</span>.hidden_size,</span><br><span class="line">        kernel_init=<span class="variable language_">self</span>.initializer_fn,</span><br><span class="line">        name=<span class="string">'mlm_dense'</span>,</span><br><span class="line">    )(last_layer)</span><br><span class="line">    mlm_hidden = jax.nn.gelu(mlm_hidden)</span><br><span class="line">    mlm_hidden = nn.LayerNorm(epsilon=TF_LAYERNORM_EPSILON, name=<span class="string">'mlm_ln'</span>)(</span><br><span class="line">        mlm_hidden</span><br><span class="line">    )</span><br><span class="line">    output_weights = jnp.transpose(embeddings)</span><br><span class="line">    logits = jnp.matmul(mlm_hidden, output_weights)</span><br><span class="line">    logits = Bias(name=<span class="string">'mlm_bias'</span>)(logits)</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h2 id="MAGVIT-v2-Google-proposed-Tencent-implementation"><a href="#MAGVIT-v2-Google-proposed-Tencent-implementation" class="headerlink" title="MAGVIT-v2 (Google proposed Tencent implementation)"></a>MAGVIT-v2 (Google proposed Tencent implementation)</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><h4 id="VQ部分"><a href="#VQ部分" class="headerlink" title="VQ部分"></a>VQ部分</h4><p>咱就是说,这代码都大差不差. 不过<code>use_gan</code>设为<code>True</code>后, 很容易<code>disc_loss</code>loss就是<code>nan</code>, 目前正在排查</p>
<h4 id="MLM部分"><a href="#MLM部分" class="headerlink" title="MLM部分"></a>MLM部分</h4><p>他们尚未发布</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" data-id="cm4boxx5e0000ykfy91sv50i2" data-title="Open-MAGVIT-2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LFQ探究" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/" class="article-date">
  <time class="dt-published" datetime="2024-11-23T01:21:21.000Z" itemprop="datePublished">2024-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/">LFQ探究</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="LFQ-Look-up-free-quantization-探究"><a href="#LFQ-Look-up-free-quantization-探究" class="headerlink" title="LFQ(Look up free quantization)探究"></a>LFQ(Look up free quantization)探究</h1><ul>
<li><p><strong>怎么quantize的, token是啥</strong></p>
<p>原文中: </p>
<blockquote>
<p>Specifically, the latent space of LFQ is decomposed as the Cartesian product of single-dimensional variables, as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.804ex;" xmlns="http://www.w3.org/2000/svg" width="12.962ex" height="3.1ex" role="img" focusable="false" viewBox="0 -1015 5729.3 1370.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2102" d="M684 131Q684 125 672 109T633 71T573 29T489 -5T386 -19Q330 -19 276 -3T174 46T91 134T44 261Q39 283 39 341T44 421Q66 538 143 611T341 699Q344 699 364 700T395 701Q449 698 503 677T585 655Q603 655 611 662T620 678T625 694T639 702Q650 702 657 690V481L653 474Q640 467 628 472Q624 476 618 496T595 541Q562 587 507 625T390 663H381Q337 663 299 625Q212 547 212 336Q212 249 233 179Q274 30 405 30Q533 30 641 130Q658 147 666 147Q671 147 677 143T684 131ZM250 625Q264 643 261 643Q238 635 214 620T161 579T110 510T79 414Q74 384 74 341T79 268Q89 213 113 169T164 101T217 61T260 39L277 34Q270 41 264 48Q199 111 181 254Q178 281 178 344T181 434Q200 559 250 625ZM621 565V625Q617 623 613 623Q603 619 590 619H575L588 605Q608 583 610 579L621 565Z"></path></g></g><g data-mml-node="mo" transform="translate(999.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msubsup" transform="translate(2055.6,0)"><g data-mml-node="mo"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="TeXAtom" transform="translate(811,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(811,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4875.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"></path></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p>
</blockquote>
<p>  笛卡尔积啥意思呢, 就是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.014ex;" xmlns="http://www.w3.org/2000/svg" width="24.847ex" height="2.711ex" role="img" focusable="false" viewBox="0 -750 10982.5 1198.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(849,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1314,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(1980.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3036.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3425.6,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(748,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(4577.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5021.8,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(748,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6173.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(6618,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(7956.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(8401.3,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(748,-237.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msubsup" transform="translate(783,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,353.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mn" transform="translate(510,-297.3) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(10593.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>,其中 K 是词表大小.</p>
<p>  进一步做简化,我们只让C从{-1,1}里取值,即原文中:</p>
<blockquote>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="41.282ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 18246.5 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(849,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1641,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2307.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(3363.5,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(394,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(672,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1172,0)"></path></g><g data-mml-node="mo" transform="translate(5091.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(5480.5,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6272.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6939.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(7995,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(8773,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(9273,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(9773,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(10842.7,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mn" transform="translate(11898.5,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(12398.5,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g><g data-mml-node="mo" transform="translate(13120.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(14121,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(14621,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(15121,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(16190.7,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(17246.5,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(17746.5,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.804ex;" xmlns="http://www.w3.org/2000/svg" width="70.208ex" height="3.1ex" role="img" focusable="false" viewBox="0 -1015 31032 1370.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(361,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(917,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1473,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1917,0)"></path></g><g data-mml-node="mo" transform="translate(2445,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2834,0)"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"></path></g></g><g data-mml-node="mo" transform="translate(3345,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4011.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(5067.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(8332.1,0)"><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(500,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(892,0)"></path></g><g data-mml-node="mo" transform="translate(9724.1,0)"><path data-c="2061" d=""></path></g><g data-mml-node="munder" transform="translate(9890.7,0)"><g data-mml-node="mo"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)"></path></g><g data-mml-node="mi" transform="translate(1700,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(12175.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(12453.8,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(13468,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(14468.2,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(748,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(16075.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="munderover" transform="translate(16519.8,0)"><g data-mml-node="mo"><path data-c="220F" d="M158 656Q147 684 131 694Q110 707 69 710H55V750H888V710H874Q840 708 820 698T795 678T786 656V-155Q798 -206 874 -210H888V-250H570V-210H584Q618 -208 638 -197T663 -178T673 -155V710H270V277L271 -155Q283 -206 359 -210H373V-250H55V-210H69Q103 -208 123 -197T148 -178T158 -155V656Z"></path></g><g data-mml-node="TeXAtom" transform="translate(977,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(977,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(429,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1207,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g><g data-mml-node="mo" transform="translate(18920.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(19198.5,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g><g data-mml-node="mo" transform="translate(20299.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(20855.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(21911.4,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msup" transform="translate(25175.9,0)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(533,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mn" transform="translate(26906.5,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(27406.5,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(27906.5,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(28976.3,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(30032,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(30532,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></p>
</blockquote>
<p>  这就是look up free, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.853ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1703 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(849,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1314,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>并非我的“embedding”, 我从codebook查值的时候也无需像以前一样“一一”比对了.</p>
<p>  <strong>以上我们解决了怎么quantize的</strong></p>
<p>  至于<strong>token是啥</strong>, 我们就得结合源代码了, 毕竟文章中说的不清不楚, 总不可能纬度真的就只是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.651ex;" xmlns="http://www.w3.org/2000/svg" width="4.461ex" height="2.565ex" role="img" focusable="false" viewBox="0 -846 1971.6 1133.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msubsup" transform="translate(783,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mn" transform="translate(510,-287.9) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>, 不然哪来的词表?</p>
<p>  一个合理的猜测是, codebook是像nlp里的vocabulary一样是单独初始化的, 但是这样其实面临个实践中我们遇到的问题: 一旦将learnable_codebook设置为True, 且code_dim设置为百维以上时, 模型很难收敛……</p>
<p>  所以,<strong>上源码</strong>(主要看forward部分):</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 源码, from https://github.com/lucidrains/vector-quantize-pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    x,</span></span><br><span class="line"><span class="params">    inv_temperature = <span class="number">100.</span>,</span></span><br><span class="line"><span class="params">    return_loss_breakdown = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    mask = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    einstein notation</span></span><br><span class="line"><span class="string">    b - batch</span></span><br><span class="line"><span class="string">    n - sequence (or flattened spatial dimensions)</span></span><br><span class="line"><span class="string">    d - feature dimension, which is also log2(codebook size)</span></span><br><span class="line"><span class="string">    c - number of codebook dim</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    is_img_or_video = x.ndim &gt;= <span class="number">4</span></span><br><span class="line">    should_transpose = default(<span class="variable language_">self</span>.channel_first, is_img_or_video)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># standardize image or video into (batch, seq, dimension)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> should_transpose:</span><br><span class="line">        x = rearrange(x, <span class="string">'b d ... -&gt; b ... d'</span>)</span><br><span class="line">        x, ps = pack_one(x, <span class="string">'b * d'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> x.shape[-<span class="number">1</span>] == <span class="variable language_">self</span>.dim, <span class="string">f'expected dimension of <span class="subst">{self.dim}</span> but received <span class="subst">{x.shape[-<span class="number">1</span>]}</span>'</span></span><br><span class="line"></span><br><span class="line">    x = <span class="variable language_">self</span>.project_in(x)</span><br></pre></td></tr></table></figure>
<p>  这里我们插播下project_in 的定义</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cosine_sim_project_in:</span><br><span class="line">    cosine_sim_project_in = default(cosine_sim_project_in_scale, codebook_scale)</span><br><span class="line">    project_in_klass = partial(CosineSimLinear, scale = cosine_sim_project_in)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    project_in_klass = partial(nn.Linear, bias = projection_has_bias)</span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.project_in = project_in_klass(dim, codebook_dims) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br></pre></td></tr></table></figure>
<p>  以及dim和codebook_dims的定义</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">codebook_dims = codebook_dim * num_codebooks</span><br><span class="line">dim = default(dim, codebook_dims)</span><br></pre></td></tr></table></figure>
<p>  从上述代码块可见,仅仅只是一个线性变换,从<code>dim</code>到<code>codebook_dim</code>. 且默认是<code>has_projections = default(has_projections, dim != codebook_dims)</code>, 那在默认传入<code>has_projections = None</code>下, 会返回<code>dim != codebook_dims</code>, 那同样的,在默认传入<code>dim = None</code>下, 会返回<code>false</code>.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># maybe soft clamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> exists(<span class="variable language_">self</span>.soft_clamp_input_value):</span><br><span class="line">    clamp_value = <span class="variable language_">self</span>.soft_clamp_input_value</span><br><span class="line">    x = (x / clamp_value).tanh() * clamp_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># split out number of codebooks</span></span><br><span class="line"></span><br><span class="line">x = rearrange(x, <span class="string">'b n (c d) -&gt; b n c d'</span>, c = <span class="variable language_">self</span>.num_codebooks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># maybe l2norm</span></span><br><span class="line"></span><br><span class="line">x = <span class="variable language_">self</span>.maybe_l2norm(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to force quantization step to be full precision or not</span></span><br><span class="line"></span><br><span class="line">force_f32 = <span class="variable language_">self</span>.force_quantization_f32</span><br><span class="line"></span><br><span class="line">quantization_context = partial(autocast, <span class="string">'cuda'</span>, enabled = <span class="literal">False</span>) <span class="keyword">if</span> force_f32 <span class="keyword">else</span> nullcontext</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> quantization_context():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> force_f32:</span><br><span class="line">        orig_dtype = x.dtype</span><br><span class="line">        x = x.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># quantize by eq 3.</span></span><br><span class="line"></span><br><span class="line">    original_input = x</span><br><span class="line"></span><br><span class="line">    codebook_value = torch.ones_like(x) * <span class="variable language_">self</span>.codebook_scale</span><br><span class="line">    quantized = torch.where(x &gt; <span class="number">0</span>, codebook_value, -codebook_value)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate indices</span></span><br><span class="line"></span><br><span class="line">    indices = reduce((quantized &gt; <span class="number">0</span>).<span class="built_in">int</span>() * <span class="variable language_">self</span>.mask.<span class="built_in">int</span>(), <span class="string">'b n c d -&gt; b n c'</span>, <span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># maybe l2norm</span></span><br><span class="line"></span><br><span class="line">    quantized = <span class="variable language_">self</span>.maybe_l2norm(quantized)</span><br></pre></td></tr></table></figure>
<p>  以上是quantize部分, 那么自然来了个疑问: 原始输入的<code>x</code>是什么shape的?<br>  进入到本代码块时,<code>x</code>无疑最后一维是<code>codebook_dims</code>, 就是说<code>codebook_dims=(c d)</code>, 根据之前的定义,确实就没问题了.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use straight-through gradients (optionally with custom activation fn) if training</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">    x = <span class="variable language_">self</span>.activation(x)</span><br><span class="line">    x = x + (quantized - x).detach()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x = quantized</span><br><span class="line"></span><br><span class="line"><span class="comment"># entropy aux loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> force_f32:</span><br><span class="line">        codebook = <span class="variable language_">self</span>.codebook.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    codebook = <span class="variable language_">self</span>.maybe_l2norm(codebook)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the same as euclidean distance up to a constant</span></span><br><span class="line">    distance = -<span class="number">2</span> * einsum(<span class="string">'... i d, j d -&gt; ... i j'</span>, original_input, codebook)</span><br><span class="line"></span><br><span class="line">    prob = (-distance * inv_temperature).softmax(dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># account for mask</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exists(mask):</span><br><span class="line">        prob = prob[mask]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob = rearrange(prob, <span class="string">'b n ... -&gt; (b n) ...'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># whether to only use a fraction of probs, for reducing memory</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.frac_per_sample_entropy &lt; <span class="number">1.</span>:</span><br><span class="line">        num_tokens = prob.shape[<span class="number">0</span>]</span><br><span class="line">        num_sampled_tokens = <span class="built_in">int</span>(num_tokens * <span class="variable language_">self</span>.frac_per_sample_entropy)</span><br><span class="line">        rand_mask = torch.randn(num_tokens).argsort(dim = -<span class="number">1</span>) &lt; num_sampled_tokens</span><br><span class="line">        per_sample_probs = prob[rand_mask]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        per_sample_probs = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate per sample entropy</span></span><br><span class="line"></span><br><span class="line">    per_sample_entropy = entropy(per_sample_probs).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distribution over all available tokens in the batch</span></span><br><span class="line"></span><br><span class="line">    avg_prob = reduce(per_sample_probs, <span class="string">'... c d -&gt; c d'</span>, <span class="string">'mean'</span>)</span><br><span class="line"></span><br><span class="line">    avg_prob = maybe_distributed_mean(avg_prob)</span><br><span class="line"></span><br><span class="line">    codebook_entropy = entropy(avg_prob).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. entropy will be nudged to be low for each code, to encourage the network to output confident predictions</span></span><br><span class="line">    <span class="comment"># 2. codebook entropy will be nudged to be high, to encourage all codes to be uniformly used within the batch</span></span><br><span class="line"></span><br><span class="line">    entropy_aux_loss = per_sample_entropy - <span class="variable language_">self</span>.diversity_gamma * codebook_entropy</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># if not training, just return dummy 0</span></span><br><span class="line">    entropy_aux_loss = per_sample_entropy = codebook_entropy = <span class="variable language_">self</span>.zero</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to make the entropy loss positive or not through a (shifted) softplus</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.experimental_softplus_entropy_loss:</span><br><span class="line">    entropy_aux_loss = F.softplus(entropy_aux_loss + <span class="variable language_">self</span>.entropy_loss_offset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.commitment_loss_weight &gt; <span class="number">0.</span>:</span><br><span class="line"></span><br><span class="line">    commit_loss = F.mse_loss(original_input, quantized.detach(), reduction = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exists(mask):</span><br><span class="line">        commit_loss = commit_loss[mask]</span><br><span class="line"></span><br><span class="line">    commit_loss = commit_loss.mean()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    commit_loss = <span class="variable language_">self</span>.zero</span><br><span class="line"></span><br><span class="line"><span class="comment"># input back to original dtype if needed</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> force_f32:</span><br><span class="line">    x = x.<span class="built_in">type</span>(orig_dtype)</span><br></pre></td></tr></table></figure>
<p>  仔细一看上述计算loss确实完完全照着文章里来的</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># merge back codebook dim</span></span><br><span class="line"></span><br><span class="line">x = rearrange(x, <span class="string">'b n c d -&gt; b n (c d)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># project out to feature dimension if needed</span></span><br><span class="line"></span><br><span class="line">x = <span class="variable language_">self</span>.project_out(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># reconstitute image or video dimensions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> should_transpose:</span><br><span class="line">    x = unpack_one(x, ps, <span class="string">'b * d'</span>)</span><br><span class="line">    x = rearrange(x, <span class="string">'b ... d -&gt; b d ...'</span>)</span><br><span class="line"></span><br><span class="line">    indices = unpack_one(indices, ps, <span class="string">'b * c'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to remove single codebook dim</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.keep_num_codebooks_dim:</span><br><span class="line">    indices = rearrange(indices, <span class="string">'... 1 -&gt; ...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># complete aux loss</span></span><br><span class="line"></span><br><span class="line">aux_loss = entropy_aux_loss * <span class="variable language_">self</span>.entropy_loss_weight + commit_loss * <span class="variable language_">self</span>.commitment_loss_weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns</span></span><br><span class="line"></span><br><span class="line">ret = Return(x, indices, aux_loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> return_loss_breakdown:</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> ret, LossBreakdown(per_sample_entropy, codebook_entropy, commit_loss)</span><br></pre></td></tr></table></figure>
<p>  最后一块也没什么可说的,和其他的VQ也一样</p>
<p>  至此, 我们可以回答了: token维度就是encoder出来的维度, 不过我们用了 c(codebook nums)*d(codebook dim) 个去quantify它. 而且出乎意料, token里的元素确实是由-1,1组成的,和之前的<code>learnable_codebook = false</code>有异曲同工之处.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/" data-id="cm4boxx5h0003ykfyeofc9ezz" data-title="LFQ探究" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/">MoE_Rag_Peft_RLHF_DPO</a>
          </li>
        
          <li>
            <a href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/">Olmo及dolma实践</a>
          </li>
        
          <li>
            <a href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/">DeepSpeed相关</a>
          </li>
        
          <li>
            <a href="/2024/12/05/DP%E5%92%8CDDP/">DP和DDP</a>
          </li>
        
          <li>
            <a href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/">千问相关</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>