<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-torch中的forward-hook" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/" class="article-date">
  <time class="dt-published" datetime="2025-02-04T06:22:36.000Z" itemprop="datePublished">2025-02-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/">torch中的forward hook</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>While reading the source code of <a href="(https://github.com/ShirAmir/dino-vit-features/tree/main"><code>repo:dino-vit-features</code></a>, I find that they use hook function to make the pre-trained model act as they desire. However, the first time I saw the function <code>register_forward_hook</code>, I mistook it for a designed function of <code>dino</code>. In fact, such function is the attribute of <code>nn.Module</code>, thus providing a flexible way to control the pre-trained weights. This short blog will show how to use <code>register_forward_hook</code>.</p>
<p>The sign of <code>register_forward_hook</code> is as follows.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_forward_hook(hook: <span class="type">Callable</span>[[nn.Module, Input, Output], <span class="type">Optional</span>[Output]]) -&gt; torch.utils.hooks.RemovableHandle</span><br></pre></td></tr></table></figure><br><code>DeepSeek</code> gives the following explanation.</p>
<ul>
<li><strong>参数</strong>：<ul>
<li><code>hook</code>：一个回调函数，接受三个参数：<ul>
<li><code>module</code>：当前模块（<code>nn.Module</code> 的实例）。</li>
<li><code>input</code>：模块的输入（一个元组，包含所有输入参数）。</li>
<li><code>output</code>：模块的输出。</li>
</ul>
</li>
<li>回调函数可以返回 <code>None</code>，或者返回一个新的输出（用于修改输出）。</li>
</ul>
</li>
<li><strong>返回值</strong>：<ul>
<li>返回一个 <code>RemovableHandle</code> 对象，可以调用其 <code>remove()</code> 方法来移除这个 Hook。</li>
</ul>
</li>
</ul>
<p>However, we still don’t know how to use and realize such hook function. <code>DeepSeek</code> also gives a toy example to help further understand.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = SimpleModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Hook 回调函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hook_fn</span>(<span class="params">module, <span class="built_in">input</span>, output</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Module: <span class="subst">&#123;module&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Input: <span class="subst">&#123;<span class="built_in">input</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Output: <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 可以在这里修改输出</span></span><br><span class="line">    <span class="keyword">return</span> output + <span class="number">1</span>  <span class="comment"># 修改输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册 Hook</span></span><br><span class="line">handle = model.fc.register_forward_hook(hook_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Output: <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 Hook</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure><br>From above we find that the <code>output</code> is just the normal output of the <code>forward</code> of the <code>nn.Module</code>. And after <code>register_forward_hook</code>, <code>model()</code> will no more call <code>forward</code> but indeed call the hook function we defined.</p>
<p>Now we focus on the example in <a href="(https://github.com/ShirAmir/dino-vit-features/tree/main"><code>repo:dino-vit-features</code></a>, explicitly the <code>self._get_hook()</code> function of class <code>VitExtractor</code><br>First we focus on the source code:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_hook</span>(<span class="params">self, facet: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    generate a hook method for a specific block and facet.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> facet <span class="keyword">in</span> [<span class="string">&#x27;attn&#x27;</span>, <span class="string">&#x27;token&#x27;</span>]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_hook</span>(<span class="params">model, <span class="built_in">input</span>, output</span>):</span><br><span class="line">            <span class="variable language_">self</span>._feats.append(output)</span><br><span class="line">        <span class="keyword">return</span> _hook</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> facet == <span class="string">&#x27;query&#x27;</span>:</span><br><span class="line">        facet_idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> facet == <span class="string">&#x27;key&#x27;</span>:</span><br><span class="line">        facet_idx = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> facet == <span class="string">&#x27;value&#x27;</span>:</span><br><span class="line">        facet_idx = <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f&quot;<span class="subst">&#123;facet&#125;</span> is not a supported facet.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_inner_hook</span>(<span class="params">module, <span class="built_in">input</span>, output</span>):</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>[<span class="number">0</span>]</span><br><span class="line">        B, N, C = <span class="built_in">input</span>.shape</span><br><span class="line">        qkv = module.qkv(<span class="built_in">input</span>).reshape(B, N, <span class="number">3</span>, module.num_heads, C // module.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>._feats.append(qkv[facet_idx]) <span class="comment">#Bxhxtxd</span></span><br><span class="line">    <span class="keyword">return</span> _inner_hook</span><br></pre></td></tr></table></figure><br>When <code>facet</code> is in list <code>[&#39;attn&#39;, &#39;token&#39;]</code>, it will return a function that return nothing but change the attribute <code>self._feats</code>. The tensors added into <code>self._feats</code> is just the normal output of the <code>block</code> in <code>dino</code>; When <code>facet</code> is <code>query</code>, <code>key</code>or <code>value</code>, similiar function is returned. Note that the <code>output</code> parameter is not necessary to be included in the definition, thus we can flexibly use the pre-trained parameters.</p>
<p><a href="(https://github.com/ShirAmir/dino-vit-features/tree/main"><code>repo:dino-vit-features</code></a> use a more complicated way to call the hook function, we list the functions <strong>from top to down</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_descriptors</span>(<span class="params">self, batch: torch.Tensor, layer: <span class="built_in">int</span> = <span class="number">11</span>, facet: <span class="built_in">str</span> = <span class="string">&#x27;key&#x27;</span>,</span></span><br><span class="line"><span class="params">						<span class="built_in">bin</span>: <span class="built_in">bool</span> = <span class="literal">False</span>, include_cls: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; torch.Tensor:</span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	extract descriptors from the model</span></span><br><span class="line"><span class="string">	:param batch: batch to extract descriptors for. Has shape BxCxHxW.</span></span><br><span class="line"><span class="string">	:param layers: layer to extract. A number between 0 to 11.</span></span><br><span class="line"><span class="string">	:param facet: facet to extract. One of the following options: [&#x27;key&#x27; | &#x27;query&#x27; | &#x27;value&#x27; | &#x27;token&#x27;]</span></span><br><span class="line"><span class="string">	:param bin: apply log binning to the descriptor. default is False.</span></span><br><span class="line"><span class="string">	:return: tensor of descriptors. Bx1xtxd&#x27; where d&#x27; is the dimension of the descriptors.</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	<span class="keyword">assert</span> facet <span class="keyword">in</span> [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;value&#x27;</span>, <span class="string">&#x27;token&#x27;</span>], <span class="string">f&quot;&quot;&quot;<span class="subst">&#123;facet&#125;</span> is not a supported facet for descriptors. </span></span><br><span class="line"><span class="string">														 choose from [&#x27;key&#x27; | &#x27;query&#x27; | &#x27;value&#x27; | &#x27;token&#x27;] &quot;&quot;&quot;</span></span><br><span class="line">	<span class="variable language_">self</span>._extract_features(batch, [layer], facet)</span><br><span class="line">	x = <span class="variable language_">self</span>._feats[<span class="number">0</span>]</span><br><span class="line">	<span class="keyword">if</span> facet == <span class="string">&#x27;token&#x27;</span>:</span><br><span class="line">		x.unsqueeze_(dim=<span class="number">1</span>) <span class="comment">#Bx1xtxd</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> include_cls:</span><br><span class="line">		x = x[:, :, <span class="number">1</span>:, :]  <span class="comment"># remove cls token</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">assert</span> <span class="keyword">not</span> <span class="built_in">bin</span>, <span class="string">&quot;bin = True and include_cls = True are not supported together, set one of them False.&quot;</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">bin</span>:</span><br><span class="line">		desc = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).flatten(start_dim=-<span class="number">2</span>, end_dim=-<span class="number">1</span>).unsqueeze(dim=<span class="number">1</span>)  <span class="comment"># Bx1xtx(dxh)</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		desc = <span class="variable language_">self</span>._log_bin(x)</span><br><span class="line">	<span class="keyword">return</span> desc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_extract_features</span>(<span class="params">self, batch: torch.Tensor, layers: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="number">11</span>, facet: <span class="built_in">str</span> = <span class="string">&#x27;key&#x27;</span></span>) -&gt; <span class="type">List</span>[torch.Tensor]:</span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	extract features from the model</span></span><br><span class="line"><span class="string">	:param batch: batch to extract features for. Has shape BxCxHxW.</span></span><br><span class="line"><span class="string">	:param layers: layer to extract. A number between 0 to 11.</span></span><br><span class="line"><span class="string">	:param facet: facet to extract. One of the following options: [&#x27;key&#x27; | &#x27;query&#x27; | &#x27;value&#x27; | &#x27;token&#x27; | &#x27;attn&#x27;]</span></span><br><span class="line"><span class="string">	:return : tensor of features.</span></span><br><span class="line"><span class="string">			  if facet is &#x27;key&#x27; | &#x27;query&#x27; | &#x27;value&#x27; has shape Bxhxtxd</span></span><br><span class="line"><span class="string">			  if facet is &#x27;attn&#x27; has shape Bxhxtxt</span></span><br><span class="line"><span class="string">			  if facet is &#x27;token&#x27; has shape Bxtxd</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	B, C, H, W = batch.shape</span><br><span class="line">	<span class="variable language_">self</span>._feats = []</span><br><span class="line">	<span class="variable language_">self</span>._register_hooks(layers, facet)</span><br><span class="line">	_ = <span class="variable language_">self</span>.model(batch)</span><br><span class="line">	<span class="variable language_">self</span>._unregister_hooks()</span><br><span class="line">	<span class="variable language_">self</span>.load_size = (H, W)</span><br><span class="line">	<span class="variable language_">self</span>.num_patches = (<span class="number">1</span> + (H - <span class="variable language_">self</span>.p) // <span class="variable language_">self</span>.stride[<span class="number">0</span>], <span class="number">1</span> + (W - <span class="variable language_">self</span>.p) // <span class="variable language_">self</span>.stride[<span class="number">1</span>])</span><br><span class="line">	<span class="keyword">return</span> <span class="variable language_">self</span>._feats</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_register_hooks</span>(<span class="params">self, layers: <span class="type">List</span>[<span class="built_in">int</span>], facet: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	register hook to extract features.</span></span><br><span class="line"><span class="string">	:param layers: layers from which to extract features.</span></span><br><span class="line"><span class="string">	:param facet: facet to extract. One of the following options: [&#x27;key&#x27; | &#x27;query&#x27; | &#x27;value&#x27; | &#x27;token&#x27; | &#x27;attn&#x27;]</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	<span class="keyword">for</span> block_idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.model.blocks):</span><br><span class="line">		<span class="keyword">if</span> block_idx <span class="keyword">in</span> layers:</span><br><span class="line">			<span class="keyword">if</span> facet == <span class="string">&#x27;token&#x27;</span>:</span><br><span class="line">				<span class="variable language_">self</span>.hook_handlers.append(block.register_forward_hook(<span class="variable language_">self</span>._get_hook(facet)))</span><br><span class="line">			<span class="keyword">elif</span> facet == <span class="string">&#x27;attn&#x27;</span>:</span><br><span class="line">				<span class="variable language_">self</span>.hook_handlers.append(block.attn.attn_drop.register_forward_hook(<span class="variable language_">self</span>._get_hook(facet)))</span><br><span class="line">			<span class="keyword">elif</span> facet <span class="keyword">in</span> [<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;value&#x27;</span>]:</span><br><span class="line">				<span class="variable language_">self</span>.hook_handlers.append(block.attn.register_forward_hook(<span class="variable language_">self</span>._get_hook(facet)))</span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				<span class="keyword">raise</span> TypeError(<span class="string">f&quot;<span class="subst">&#123;facet&#125;</span> is not a supported facet.&quot;</span>)</span><br></pre></td></tr></table></figure><br>A new attribute now bothers us, what is <code>self.hook_handlers</code> and how it is used?<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.hook_handlers = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_unregister_hooks</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	unregisters the hooks. should be called after feature extraction.</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	<span class="keyword">for</span> handle <span class="keyword">in</span> <span class="variable language_">self</span>.hook_handlers:</span><br><span class="line">		handle.remove()</span><br><span class="line">	<span class="variable language_">self</span>.hook_handlers = []</span><br></pre></td></tr></table></figure></p>
<p>We see that it is just a list to contains the aforementioned <code>RemovableHandle</code> objects to further remove them after calling the hook functions.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/" data-id="cm6q4nk2m0000rqwih10y8ilu" data-title="torch中的forward hook" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ObjectDetectionRecent20Years" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/01/01/ObjectDetectionRecent20Years/" class="article-date">
  <time class="dt-published" datetime="2025-01-01T00:58:07.000Z" itemprop="datePublished">2025-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/01/01/ObjectDetectionRecent20Years/">ObjectDetectionRecent20Years</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文是邹老师所著的经典的目标检测综述，记录了至22年来的演化。本文旨在记录本人的观后感。在看完Introduction后，本文的写作需要达到下述几个要求：</p>
<ol>
<li>英文写作</li>
<li>扩充时间轴思维导图<ol>
<li>技术演变视野<ul>
<li>有那些milestone papers？</li>
<li>技术如何随着时间而演变？</li>
</ul>
</li>
<li>重要问题视野<ul>
<li>领域终极目标？</li>
<li>现有水平？</li>
<li>problem remained？</li>
<li>热点话题？</li>
</ul>
</li>
</ol>
</li>
<li>文字部分主要聚焦重大节点的Problem setting和Motivation</li>
</ol>
<h1 id="Object-Detection-in-20-Years-A-Survey"><a href="#Object-Detection-in-20-Years-A-Survey" class="headerlink" title="Object Detection in 20 Years: A Survey"></a>Object Detection in 20 Years: A Survey</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/01/01/ObjectDetectionRecent20Years/" data-id="cm5i5ohqd0000iewidjnw06xk" data-title="ObjectDetectionRecent20Years" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-playingDINOv2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/30/playingDINOv2/" class="article-date">
  <time class="dt-published" datetime="2024-12-30T10:08:58.000Z" itemprop="datePublished">2024-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/30/playingDINOv2/">playingDINOv2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>In this article, I will record a temptation to use pre-trained DinoV2 to generate the <strong>Relationship Masks</strong>. First, I will attempt to utilize the pre-trained model <code>dinov2_vits14</code> (“s” means small) on a semantic segmentation task <strong>WITHOUT MMSEG</strong>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dinov2_vits14 = torch.hub.load(<span class="string">'facebookresearch/dinov2'</span>, <span class="string">'dinov2_vits14'</span>)</span><br></pre></td></tr></table></figure><br>Out:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DinoVisionTransformer(</span><br><span class="line">  (patch_embed): PatchEmbed(</span><br><span class="line">    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))</span><br><span class="line">    (norm): Identity()</span><br><span class="line">  )</span><br><span class="line">  (blocks): ModuleList(</span><br><span class="line">    (0-11): 12 x NestedTensorBlock(</span><br><span class="line">      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">      (attn): MemEffAttention(</span><br><span class="line">        (qkv): Linear(in_features=384, out_features=1152, bias=True)</span><br><span class="line">        (attn_drop): Dropout(p=0.0, inplace=False)</span><br><span class="line">        (proj): Linear(in_features=384, out_features=384, bias=True)</span><br><span class="line">        (proj_drop): Dropout(p=0.0, inplace=False)</span><br><span class="line">      )</span><br><span class="line">      (ls1): LayerScale()</span><br><span class="line">      (drop_path1): Identity()</span><br><span class="line">      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">      (mlp): Mlp(</span><br><span class="line">        (fc1): Linear(in_features=384, out_features=1536, bias=True)</span><br><span class="line">        (act): GELU(approximate='none')</span><br><span class="line">        (fc2): Linear(in_features=1536, out_features=384, bias=True)</span><br><span class="line">        (drop): Dropout(p=0.0, inplace=False)</span><br><span class="line">      )</span><br><span class="line">      (ls2): LayerScale()</span><br><span class="line">      (drop_path2): Identity()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">  (head): Identity()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>Here, we find that the class name of the pre-trained DinoV2 is <code>DinoVisionTransformer</code>, where we can find the definition in file <code>dinov2/models/vision_transformer.py</code> of repo <code>facebook/dinov2</code>.</p>
<p>Now, we figure out how the official demo <code>notebooks/semantic_segmentation.ipynb</code> initializes a segmentation model of which the <code>backbone</code> is the aforementioned <code>dinov2_vits14</code>. Here, two puzzles bother us:</p>
<ul>
<li>Where is the definition of the segmentation model?</li>
<li>How does it initialize the “segmentation head”?</li>
</ul>
<p>For these two questions, I find that the procedure of  initialization is:</p>
<ul>
<li><p>first, fetch the official configure file <code>cfg</code> from urls.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HEAD_SCALE_COUNT = <span class="number">3</span> <span class="comment"># more scales: slower but better results, in (1,2,3,4,5)</span></span><br><span class="line">HEAD_DATASET = <span class="string">"voc2012"</span> <span class="comment"># in ("ade20k", "voc2012")</span></span><br><span class="line">HEAD_TYPE = <span class="string">"ms"</span> <span class="comment"># in ("ms, "linear")</span></span><br><span class="line"></span><br><span class="line">DINOV2_BASE_URL = <span class="string">"https://dl.fbaipublicfiles.com/dinov2"</span></span><br><span class="line">head_config_url = <span class="string">f"<span class="subst">{DINOV2_BASE_URL}</span>/<span class="subst">{backbone_name}</span>/<span class="subst">{backbone_name}</span>_<span class="subst">{HEAD_DATASET}</span>_<span class="subst">{HEAD_TYPE}</span>_config.py"</span></span><br><span class="line"></span><br><span class="line">cfg_str = load_config_from_url(head_config_url)</span><br></pre></td></tr></table></figure>
<p>  Then, I print the  <code>cfg_str</code> and get:</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">dataset_type = 'PascalVOCDataset'</span><br><span class="line">data_root = '/checkpoint/dino/datasets/VOC2012'</span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)</span><br><span class="line">crop_size = (640, 640)</span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type='LoadImageFromFile'),</span><br><span class="line">    dict(type='LoadAnnotations'),</span><br><span class="line">    dict(type='Resize', img_scale=(99999999, 640), ratio_range=(1.0, 3.0)),</span><br><span class="line">    dict(type='RandomCrop', crop_size=(640, 640), cat_max_ratio=0.75),</span><br><span class="line">    dict(type='RandomFlip', prob=0.5),</span><br><span class="line">    dict(type='PhotoMetricDistortion'),</span><br><span class="line">    dict(</span><br><span class="line">        type='Normalize',</span><br><span class="line">        mean=[123.675, 116.28, 103.53],</span><br><span class="line">        std=[58.395, 57.12, 57.375],</span><br><span class="line">        to_rgb=True),</span><br><span class="line">    dict(type='Pad', size=(640, 640), pad_val=0, seg_pad_val=255),</span><br><span class="line">    dict(type='DefaultFormatBundle'),</span><br><span class="line">    dict(type='Collect', keys=['img', 'gt_semantic_seg'])</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type='LoadImageFromFile'),</span><br><span class="line">    dict(</span><br><span class="line">        type='MultiScaleFlipAug',</span><br><span class="line">        img_scale=(99999999, 640),</span><br><span class="line">        img_ratios=[1.0, 1.32, 1.73, 2.28, 3.0],</span><br><span class="line">        flip=True,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type='Resize', keep_ratio=True),</span><br><span class="line">            dict(type='RandomFlip'),</span><br><span class="line">            dict(</span><br><span class="line">                type='Normalize',</span><br><span class="line">                mean=[123.675, 116.28, 103.53],</span><br><span class="line">                std=[58.395, 57.12, 57.375],</span><br><span class="line">                to_rgb=True),</span><br><span class="line">            dict(type='ImageToTensor', keys=['img']),</span><br><span class="line">            dict(type='Collect', keys=['img'])</span><br><span class="line">        ])</span><br><span class="line">]</span><br><span class="line">data = dict(</span><br><span class="line">    samples_per_gpu=2,</span><br><span class="line">    workers_per_gpu=6,</span><br><span class="line">    train=dict(</span><br><span class="line">        type='PascalVOCDataset',</span><br><span class="line">        data_root='/checkpoint/dino/datasets/VOC2012',</span><br><span class="line">        img_dir='JPEGImages',</span><br><span class="line">        ann_dir=['SegmentationClass', 'SegmentationClassAug'],</span><br><span class="line">        split=[</span><br><span class="line">            'ImageSets/Segmentation/train.txt',</span><br><span class="line">            'ImageSets/Segmentation/aug.txt'</span><br><span class="line">        ],</span><br><span class="line">        pipeline=[</span><br><span class="line">            dict(type='LoadImageFromFile'),</span><br><span class="line">            dict(type='LoadAnnotations'),</span><br><span class="line">            dict(</span><br><span class="line">                type='Resize',</span><br><span class="line">                img_scale=(99999999, 640),</span><br><span class="line">                ratio_range=(1.0, 3.0)),</span><br><span class="line">            dict(type='RandomCrop', crop_size=(640, 640), cat_max_ratio=0.75),</span><br><span class="line">            dict(type='RandomFlip', prob=0.5),</span><br><span class="line">            dict(type='PhotoMetricDistortion'),</span><br><span class="line">            dict(</span><br><span class="line">                type='Normalize',</span><br><span class="line">                mean=[123.675, 116.28, 103.53],</span><br><span class="line">                std=[58.395, 57.12, 57.375],</span><br><span class="line">                to_rgb=True),</span><br><span class="line">            dict(type='Pad', size=(640, 640), pad_val=0, seg_pad_val=255),</span><br><span class="line">            dict(type='DefaultFormatBundle'),</span><br><span class="line">            dict(type='Collect', keys=['img', 'gt_semantic_seg'])</span><br><span class="line">        ]),</span><br><span class="line">    val=dict(</span><br><span class="line">        type='PascalVOCDataset',</span><br><span class="line">        data_root='/checkpoint/dino/datasets/VOC2012',</span><br><span class="line">        img_dir='JPEGImages',</span><br><span class="line">        ann_dir='SegmentationClass',</span><br><span class="line">        split='ImageSets/Segmentation/val.txt',</span><br><span class="line">        pipeline=[</span><br><span class="line">            dict(type='LoadImageFromFile'),</span><br><span class="line">            dict(</span><br><span class="line">                type='MultiScaleFlipAug',</span><br><span class="line">                img_scale=(99999999, 640),</span><br><span class="line">                img_ratios=[1.0, 1.32, 1.73, 2.28, 3.0],</span><br><span class="line">                flip=True,</span><br><span class="line">                transforms=[</span><br><span class="line">                    dict(type='Resize', keep_ratio=True),</span><br><span class="line">                    dict(type='RandomFlip'),</span><br><span class="line">                    dict(</span><br><span class="line">                        type='Normalize',</span><br><span class="line">                        mean=[123.675, 116.28, 103.53],</span><br><span class="line">                        std=[58.395, 57.12, 57.375],</span><br><span class="line">                        to_rgb=True),</span><br><span class="line">                    dict(type='ImageToTensor', keys=['img']),</span><br><span class="line">                    dict(type='Collect', keys=['img'])</span><br><span class="line">                ])</span><br><span class="line">        ]),</span><br><span class="line">    test=dict(</span><br><span class="line">        type='PascalVOCDataset',</span><br><span class="line">        data_root='/checkpoint/dino/datasets/VOC2012',</span><br><span class="line">        img_dir='JPEGImages',</span><br><span class="line">        ann_dir='SegmentationClass',</span><br><span class="line">        split='ImageSets/Segmentation/val.txt',</span><br><span class="line">        pipeline=[</span><br><span class="line">            dict(type='LoadImageFromFile'),</span><br><span class="line">            dict(</span><br><span class="line">                type='MultiScaleFlipAug',</span><br><span class="line">                img_scale=(99999999, 640),</span><br><span class="line">                img_ratios=[1.0, 1.32, 1.73, 2.28, 3.0],</span><br><span class="line">                flip=True,</span><br><span class="line">                transforms=[</span><br><span class="line">                    dict(type='Resize', keep_ratio=True),</span><br><span class="line">                    dict(type='RandomFlip'),</span><br><span class="line">                    dict(</span><br><span class="line">                        type='Normalize',</span><br><span class="line">                        mean=[123.675, 116.28, 103.53],</span><br><span class="line">                        std=[58.395, 57.12, 57.375],</span><br><span class="line">                        to_rgb=True),</span><br><span class="line">                    dict(type='ImageToTensor', keys=['img']),</span><br><span class="line">                    dict(type='Collect', keys=['img'])</span><br><span class="line">                ])</span><br><span class="line">        ]))</span><br><span class="line">log_config = dict(</span><br><span class="line">    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])</span><br><span class="line">dist_params = dict(backend='nccl')</span><br><span class="line">log_level = 'INFO'</span><br><span class="line">load_from = None</span><br><span class="line">resume_from = None</span><br><span class="line">workflow = [('train', 1)]</span><br><span class="line">cudnn_benchmark = True</span><br><span class="line">optimizer = dict(</span><br><span class="line">    type='AdamW', lr=0.001, weight_decay=0.0001, betas=(0.9, 0.999))</span><br><span class="line">optimizer_config = dict(</span><br><span class="line">    type='DistOptimizerHook',</span><br><span class="line">    update_interval=1,</span><br><span class="line">    grad_clip=None,</span><br><span class="line">    coalesce=True,</span><br><span class="line">    bucket_size_mb=-1,</span><br><span class="line">    use_fp16=False)</span><br><span class="line">lr_config = dict(</span><br><span class="line">    policy='poly',</span><br><span class="line">    warmup='linear',</span><br><span class="line">    warmup_iters=1500,</span><br><span class="line">    warmup_ratio=1e-06,</span><br><span class="line">    power=1.0,</span><br><span class="line">    min_lr=0.0,</span><br><span class="line">    by_epoch=False)</span><br><span class="line">runner = dict(type='IterBasedRunner', max_iters=40000)</span><br><span class="line">checkpoint_config = dict(by_epoch=False, interval=10000)</span><br><span class="line">evaluation = dict(interval=40000, metric='mIoU', pre_eval=True)</span><br><span class="line">fp16 = None</span><br><span class="line">find_unused_parameters = True</span><br><span class="line">norm_cfg = dict(type='SyncBN', requires_grad=True)</span><br><span class="line">model = dict(</span><br><span class="line">    type='EncoderDecoder',</span><br><span class="line">    pretrained=None,</span><br><span class="line">    backbone=dict(type='DinoVisionTransformer', out_indices=[8, 9, 10, 11]),</span><br><span class="line">    decode_head=dict(</span><br><span class="line">        type='BNHead',</span><br><span class="line">        in_channels=[384, 384, 384, 384],</span><br><span class="line">        in_index=[0, 1, 2, 3],</span><br><span class="line">        input_transform='resize_concat',</span><br><span class="line">        channels=1536,</span><br><span class="line">        dropout_ratio=0,</span><br><span class="line">        num_classes=21,</span><br><span class="line">        norm_cfg=dict(type='SyncBN', requires_grad=True),</span><br><span class="line">        align_corners=False,</span><br><span class="line">        loss_decode=dict(</span><br><span class="line">            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),</span><br><span class="line">    test_cfg=dict(mode='slide', crop_size=(640, 640), stride=(320, 320)))</span><br><span class="line">auto_resume = True</span><br><span class="line">gpu_ids = range(0, 8)</span><br><span class="line">work_dir = '/checkpoint/dino/evaluations/segmentation/dinov2_vits14_voc2012_ms'</span><br></pre></td></tr></table></figure>
<p>  TLDR: The class of segmentation model is <code>EncoderDecoder</code> lying in <code>mmseg/models/segmentors/encoder_decoder.py</code> of Repo <code>open-mmlab/mmsegmentation</code>.<br>  Now, check the details.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@MODELS.register_module()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(<span class="title class_ inherited__">BaseSegmentor</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 backbone: ConfigType,</span></span><br><span class="line"><span class="params">                 decode_head: ConfigType,</span></span><br><span class="line"><span class="params">                 neck: OptConfigType = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 auxiliary_head: OptConfigType = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 train_cfg: OptConfigType = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 test_cfg: OptConfigType = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 data_preprocessor: OptConfigType = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 pretrained: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 init_cfg: OptMultiConfig = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(</span><br><span class="line">            data_preprocessor=data_preprocessor, init_cfg=init_cfg)</span><br><span class="line">        <span class="keyword">if</span> pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> backbone.get(<span class="string">'pretrained'</span>) <span class="keyword">is</span> <span class="literal">None</span>, \</span><br><span class="line">                <span class="string">'both backbone and segmentor set pretrained weight'</span></span><br><span class="line">            backbone.pretrained = pretrained</span><br><span class="line">        <span class="variable language_">self</span>.backbone = MODELS.build(backbone)</span><br><span class="line">        <span class="keyword">if</span> neck <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.neck = MODELS.build(neck)</span><br><span class="line">        <span class="variable language_">self</span>._init_decode_head(decode_head)</span><br><span class="line">        <span class="variable language_">self</span>._init_auxiliary_head(auxiliary_head)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.train_cfg = train_cfg</span><br><span class="line">        <span class="variable language_">self</span>.test_cfg = test_cfg</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.with_decode_head</span><br></pre></td></tr></table></figure>
<p>  Now leaves the <code>decode_head</code> which is initialized in <code>EncoderDecoder</code> by function <code>_init_decode_head</code>.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_decode_head</span>(<span class="params">self, decode_head: ConfigType</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="string">"""Initialize ``decode_head``"""</span></span><br><span class="line">   <span class="variable language_">self</span>.decode_head = MODELS.build(decode_head)</span><br><span class="line">   <span class="variable language_">self</span>.align_corners = <span class="variable language_">self</span>.decode_head.align_corners</span><br><span class="line">   <span class="variable language_">self</span>.num_classes = <span class="variable language_">self</span>.decode_head.num_classes</span><br><span class="line">   <span class="variable language_">self</span>.out_channels = <span class="variable language_">self</span>.decode_head.out_channels</span><br></pre></td></tr></table></figure>
<p>  The initialization of <code>decode_head</code> depends on Class <code>Registry</code> in dependency <code>mmengine</code></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MODELS = Registry(<span class="string">'model'</span>, parent=MMENGINE_MODELS, locations=[<span class="string">'mmseg.models'</span>])</span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Registry</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">               name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">               build_func: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               parent: <span class="type">Optional</span>[<span class="string">'Registry'</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               scope: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               locations: <span class="type">List</span> = []</span>):</span><br><span class="line">	    <span class="keyword">from</span> .build_functions <span class="keyword">import</span> build_from_cfg</span><br><span class="line">        <span class="variable language_">self</span>._name = name</span><br><span class="line">        <span class="variable language_">self</span>._module_dict: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Type</span>] = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="variable language_">self</span>._children: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="string">'Registry'</span>] = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="variable language_">self</span>._locations = locations</span><br><span class="line">        <span class="variable language_">self</span>._imported = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">isinstance</span>(scope, <span class="built_in">str</span>)</span><br><span class="line">            <span class="variable language_">self</span>._scope = scope</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>._scope = <span class="variable language_">self</span>.infer_scope()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># See https://mypy.readthedocs.io/en/stable/common_issues.html#</span></span><br><span class="line">        <span class="comment"># variables-vs-type-aliases for the use</span></span><br><span class="line">        <span class="variable language_">self</span>.parent: <span class="type">Optional</span>[<span class="string">'Registry'</span>]</span><br><span class="line">        <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">isinstance</span>(parent, Registry)</span><br><span class="line">            parent._add_child(<span class="variable language_">self</span>)</span><br><span class="line">            <span class="variable language_">self</span>.parent = parent</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.parent = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.build_func will be set with the following priority:</span></span><br><span class="line">        <span class="comment"># 1. build_func</span></span><br><span class="line">        <span class="comment"># 2. parent.build_func</span></span><br><span class="line">        <span class="comment"># 3. build_from_cfg</span></span><br><span class="line">        <span class="variable language_">self</span>.build_func: <span class="type">Callable</span></span><br><span class="line">        <span class="keyword">if</span> build_func <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="variable language_">self</span>.build_func = <span class="variable language_">self</span>.parent.build_func</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.build_func = build_from_cfg</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.build_func = build_func</span><br><span class="line">...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, cfg: <span class="built_in">dict</span>, *args, **kwargs</span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">      <span class="string">"""Build an instance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Build an instance by calling :attr:`build_func`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Args:</span></span><br><span class="line"><span class="string">          cfg (dict): Config dict needs to be built.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Returns:</span></span><br><span class="line"><span class="string">          Any: The constructed object.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Examples:</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; from mmengine import Registry</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; MODELS = Registry('models')</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; @MODELS.register_module()</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; class ResNet:</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt;     def __init__(self, depth, stages=4):</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt;         self.depth = depth</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt;         self.stages = stages</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; cfg = dict(type='ResNet', depth=50)</span></span><br><span class="line"><span class="string">          &gt;&gt;&gt; model = MODELS.build(cfg)</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">      <span class="keyword">return</span> <span class="variable language_">self</span>.build_func(cfg, *args, **kwargs, registry=<span class="variable language_">self</span>)</span><br></pre></td></tr></table></figure>
<p>  Therefore, We find that the <code>build</code> function of <code>MODELS</code> is inherited from <code>MMENGINE_MODELS</code>, of which the definition is:</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmengine.registry <span class="keyword">import</span> MODELS <span class="keyword">as</span> MMENGINE_MODELS</span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mangage all kinds of modules inheriting `nn.Module`</span></span><br><span class="line">MODELS = Registry(<span class="string">'model'</span>, build_model_from_cfg)</span><br></pre></td></tr></table></figure>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_model_from_cfg</span>(<span class="params"></span></span><br><span class="line"><span class="params">    cfg: <span class="type">Union</span>[<span class="built_in">dict</span>, ConfigDict, Config],</span></span><br><span class="line"><span class="params">    registry: Registry,</span></span><br><span class="line"><span class="params">    default_args: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">dict</span>, <span class="string">'ConfigDict'</span>, <span class="string">'Config'</span>]] = <span class="literal">None</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="string">'nn.Module'</span>:</span><br><span class="line">    <span class="string">"""Build a PyTorch model from config dict(s). Different from</span></span><br><span class="line"><span class="string">    ``build_from_cfg``, if cfg is a list, a ``nn.Sequential`` will be built.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cfg (dict, list[dict]): The config of modules, which is either a config</span></span><br><span class="line"><span class="string">            dict or a list of config dicts. If cfg is a list, the built</span></span><br><span class="line"><span class="string">            modules will be wrapped with ``nn.Sequential``.</span></span><br><span class="line"><span class="string">        registry (:obj:`Registry`): A registry the module belongs to.</span></span><br><span class="line"><span class="string">        default_args (dict, optional): Default arguments to build the module.</span></span><br><span class="line"><span class="string">            Defaults to None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        nn.Module: A built nn.Module.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">from</span> ..model <span class="keyword">import</span> Sequential</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(cfg, <span class="built_in">list</span>):</span><br><span class="line">        modules = [</span><br><span class="line">            build_from_cfg(_cfg, registry, default_args) <span class="keyword">for</span> _cfg <span class="keyword">in</span> cfg</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> Sequential(*modules)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> build_from_cfg(cfg, registry, default_args)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_from_cfg</span>(<span class="params"></span></span><br><span class="line"><span class="params">       cfg: <span class="type">Union</span>[<span class="built_in">dict</span>, ConfigDict, Config],</span></span><br><span class="line"><span class="params">       registry: Registry,</span></span><br><span class="line"><span class="params">       default_args: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">dict</span>, ConfigDict, Config]] = <span class="literal">None</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">    <span class="string">"""Build a module from config dict when it is a class configuration, or</span></span><br><span class="line"><span class="string">    call a function from config dict when it is a function configuration.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If the global variable default scope (:obj:`DefaultScope`) exists,</span></span><br><span class="line"><span class="string">    :meth:`build` will firstly get the responding registry and then call</span></span><br><span class="line"><span class="string">    its own :meth:`build`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At least one of the ``cfg`` and ``default_args`` contains the key "type",</span></span><br><span class="line"><span class="string">    which should be either str or class. If they all contain it, the key</span></span><br><span class="line"><span class="string">    in ``cfg`` will be used because ``cfg`` has a high priority than</span></span><br><span class="line"><span class="string">    ``default_args`` that means if a key exists in both of them, the value of</span></span><br><span class="line"><span class="string">    the key will be ``cfg[key]``. They will be merged first and the key "type"</span></span><br><span class="line"><span class="string">    will be popped up and the remaining keys will be used as initialization</span></span><br><span class="line"><span class="string">    arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples:</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		from mmengine import Registry, build_from_cfg</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		MODELS = Registry('models')</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		@MODELS.register_module()</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		class ResNet:</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		    def __init__(self, depth, stages=4):</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		        self.depth = depth</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		        self.stages = stages</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		cfg = dict(type='ResNet', depth=50)</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		model = build_from_cfg(cfg, MODELS)</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		# Returns an instantiated object</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		@MODELS.register_module()</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		def resnet50():</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		    pass</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		resnet = build_from_cfg(dict(type='resnet50'), MODELS)</span></span><br><span class="line"><span class="string">      &gt;&gt;&gt; 		# Return a result of the calling function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cfg (dict or ConfigDict or Config): Config dict. It should at least</span></span><br><span class="line"><span class="string">            contain the key "type".</span></span><br><span class="line"><span class="string">        registry (:obj:`Registry`): The registry to search the type from.</span></span><br><span class="line"><span class="string">        default_args (dict or ConfigDict or Config, optional): Default</span></span><br><span class="line"><span class="string">            initialization arguments. Defaults to None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        object: The constructed object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Avoid circular import</span></span><br><span class="line">    <span class="keyword">from</span> ..logging <span class="keyword">import</span> print_log</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(cfg, (<span class="built_in">dict</span>, ConfigDict, Config)):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(</span><br><span class="line">            <span class="string">f'cfg should be a dict, ConfigDict or Config, but got <span class="subst">{<span class="built_in">type</span>(cfg)}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'type'</span> <span class="keyword">not</span> <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> default_args <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="string">'type'</span> <span class="keyword">not</span> <span class="keyword">in</span> default_args:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">'`cfg` or `default_args` must contain the key "type", '</span></span><br><span class="line">                <span class="string">f'but got <span class="subst">{cfg}</span>\n<span class="subst">{default_args}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(registry, Registry):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'registry must be a mmengine.Registry object, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">{<span class="built_in">type</span>(registry)}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (<span class="built_in">isinstance</span>(default_args,</span><br><span class="line">                       (<span class="built_in">dict</span>, ConfigDict, Config)) <span class="keyword">or</span> default_args <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(</span><br><span class="line">            <span class="string">'default_args should be a dict, ConfigDict, Config or None, '</span></span><br><span class="line">            <span class="string">f'but got <span class="subst">{<span class="built_in">type</span>(default_args)}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    args = cfg.copy()</span><br><span class="line">    <span class="keyword">if</span> default_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> default_args.items():</span><br><span class="line">            args.setdefault(name, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instance should be built under target scope, if `_scope_` is defined</span></span><br><span class="line">    <span class="comment"># in cfg, current default scope should switch to specified scope</span></span><br><span class="line">    <span class="comment"># temporarily.</span></span><br><span class="line">    scope = args.pop(<span class="string">'_scope_'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">with</span> registry.switch_scope_and_registry(scope) <span class="keyword">as</span> registry:</span><br><span class="line">        obj_type = args.pop(<span class="string">'type'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj_type, <span class="built_in">str</span>):</span><br><span class="line">            obj_cls = registry.get(obj_type)</span><br><span class="line">            <span class="keyword">if</span> obj_cls <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> KeyError(</span><br><span class="line">                    <span class="string">f'<span class="subst">{obj_type}</span> is not in the <span class="subst">{registry.scope}</span>::<span class="subst">{registry.name}</span> registry. '</span>  <span class="comment"># noqa: E501</span></span><br><span class="line">                    <span class="string">f'Please check whether the value of `<span class="subst">{obj_type}</span>` is '</span></span><br><span class="line">                    <span class="string">'correct or it was registered as expected. More details '</span></span><br><span class="line">                    <span class="string">'can be found at '</span></span><br><span class="line">                    <span class="string">'https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'</span>  <span class="comment"># noqa: E501</span></span><br><span class="line">                )</span><br><span class="line">        <span class="comment"># this will include classes, functions, partial functions and more</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">callable</span>(obj_type):</span><br><span class="line">            obj_cls = obj_type</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(</span><br><span class="line">                <span class="string">f'type must be a str or valid type, but got <span class="subst">{<span class="built_in">type</span>(obj_type)}</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If `obj_cls` inherits from `ManagerMixin`, it should be</span></span><br><span class="line">        <span class="comment"># instantiated by `ManagerMixin.get_instance` to ensure that it</span></span><br><span class="line">        <span class="comment"># can be accessed globally.</span></span><br><span class="line">        <span class="keyword">if</span> inspect.isclass(obj_cls) <span class="keyword">and</span> \</span><br><span class="line">                <span class="built_in">issubclass</span>(obj_cls, ManagerMixin):  <span class="comment"># type: ignore</span></span><br><span class="line">            obj = obj_cls.get_instance(**args)  <span class="comment"># type: ignore</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            obj = obj_cls(**args)  <span class="comment"># type: ignore</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (inspect.isclass(obj_cls) <span class="keyword">or</span> inspect.isfunction(obj_cls)</span><br><span class="line">                <span class="keyword">or</span> inspect.ismethod(obj_cls)):</span><br><span class="line">            print_log(</span><br><span class="line">                <span class="string">f'An `<span class="subst">{obj_cls.__name__}</span>` instance is built from '</span>  <span class="comment"># type: ignore # noqa: E501</span></span><br><span class="line">                <span class="string">'registry, and its implementation can be found in '</span></span><br><span class="line">                <span class="string">f'<span class="subst">{obj_cls.__module__}</span>'</span>,  <span class="comment"># type: ignore</span></span><br><span class="line">                logger=<span class="string">'current'</span>,</span><br><span class="line">                level=logging.DEBUG)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print_log(</span><br><span class="line">                <span class="string">'An instance is built from registry, and its constructor '</span></span><br><span class="line">                <span class="string">f'is <span class="subst">{obj_cls}</span>'</span>,</span><br><span class="line">                logger=<span class="string">'current'</span>,</span><br><span class="line">                level=logging.DEBUG)</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  <strong>Now we conclude that</strong> the definitions of all decode_heads including <code>BNHead</code> are in <code>mmseg/models/decode_heads</code></p>
</li>
<li>The answer to the second question is “Yes”, the codes of loading are as follows.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">head_checkpoint_url = <span class="string">f"<span class="subst">{DINOV2_BASE_URL}</span>/<span class="subst">{backbone_name}</span>/<span class="subst">{backbone_name}</span>_<span class="subst">{HEAD_DATASET}</span>_<span class="subst">{HEAD_TYPE}</span>_head.pth"</span></span><br><span class="line">model = create_segmenter(cfg, backbone_model=backbone_model)</span><br><span class="line">load_checkpoint(model, head_checkpoint_url, map_location=<span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/30/playingDINOv2/" data-id="cm5i5ohqg0003iewi3bnf6zxy" data-title="playingDINOv2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-CNN-surveys" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/30/CNN-surveys/" class="article-date">
  <time class="dt-published" datetime="2024-12-30T08:15:28.000Z" itemprop="datePublished">2024-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/30/CNN-surveys/">CNN-surveys</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>This survey is published on TPAMI, indicating its great acceptance in the Computer Vision Community. My main goal of writing this blog is to figure out what obstacles harms the cv community and how researchers overcome them. A detailed request for myself is to figure out </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/30/CNN-surveys/" data-id="cm5i5ohqg0004iewifso55xqr" data-title="CNN-surveys" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/basical-network/" rel="tag">basical-network</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-小样本和llm" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/" class="article-date">
  <time class="dt-published" datetime="2024-12-10T02:03:06.000Z" itemprop="datePublished">2024-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/">小样本和llm</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/" data-id="cm5ar5vhi000lciwi27vsb5ea" data-title="小样本和llm" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-多模态大模型安全及越狱攻击防御" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/09/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E5%8F%8A%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB%E9%98%B2%E5%BE%A1/" class="article-date">
  <time class="dt-published" datetime="2024-12-09T09:55:04.000Z" itemprop="datePublished">2024-12-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/09/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E5%8F%8A%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB%E9%98%B2%E5%BE%A1/">多模态大模型安全及越狱攻击防御</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="一、一个单模态的切入"><a href="#一、一个单模态的切入" class="headerlink" title="一、一个单模态的切入"></a>一、一个单模态的切入</h1><h2 id="论文标题-截屏2024-12-09-17-58-51-png"><a href="#论文标题-截屏2024-12-09-17-58-51-png" class="headerlink" title="论文标题![[截屏2024-12-09 17.58.51.png]]"></a><strong>论文标题</strong>![[截屏2024-12-09 17.58.51.png]]</h2><ul>
<li>网址: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.08983">https://arxiv.org/abs/2402.08983</a></li>
</ul>
<h2 id="1-带着问题看摘要"><a href="#1-带着问题看摘要" class="headerlink" title="1. 带着问题看摘要"></a>1. 带着问题看摘要</h2><p><strong>越狱攻击</strong>: </p>
<blockquote>
<p><strong>Jailbreak attacks</strong>, aiming to <strong>provoke</strong> unintended and unsafe behaviors from LLMs</p>
</blockquote>
<p>provoke用的很形象, 但是unintended值得注意:<br>问题:</p>
<ol>
<li>训练语料有违规样本吗?</li>
<li>训练过程中有违规过程吗?</li>
<li>若没有, 背后是什么逻辑导致大模型unsafe</li>
</ol>
<p><strong>本文如何保护</strong>:</p>
<blockquote>
<p>safety-aware decoding strategy</p>
</blockquote>
<p>初见: 不设置额外违禁词检查模块, 而是在decoding中直接采用特定策略<br>问题: LLM需要重训或者SFT吗?</p>
<p><strong>具体strategy的动机</strong></p>
<blockquote>
<p>based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order.</p>
</blockquote>
<p>初见: 看着像“沙里淘金”?<br>问题: <strong>disclaimers</strong>是?</p>
<ul>
<li>免责声明</li>
</ul>
<p><strong>优势</strong></p>
<blockquote>
<p>Our results show that SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries while outperforming six defense methods</p>
</blockquote>
<p>问题: 怎么个“同时在不影响对<strong>良性用户</strong>查询的响应效果的前提下”, 其他方法就影响了吗?</p>
<h2 id="2-阅读Problem-setting"><a href="#2-阅读Problem-setting" class="headerlink" title="2. 阅读Problem setting"></a>2. 阅读Problem setting</h2><p><strong>推理</strong><br>![[截屏2024-12-10 09.51.04.png]]<br><strong>jail breaking</strong><br>prompt <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="9.764ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 4315.8 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1008.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1453.2,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(2791.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3236.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container> that<br>![[截屏2024-12-10 09.51.43.png]]<br>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.109ex;" xmlns="http://www.w3.org/2000/svg" width="1.912ex" height="1.654ex" role="img" focusable="false" viewBox="0 -683 845 731"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="48" d="M18 487Q18 496 29 517T67 566T127 621T216 665T330 683Q359 683 376 669T397 643T400 622Q400 584 382 488T348 343Q348 342 467 342H587L594 366Q615 440 648 534T690 641Q701 656 723 669T764 683Q783 683 783 672L750 578Q716 485 677 346T625 101Q624 92 623 82T622 65T621 56Q621 20 658 20Q666 20 701 25Q709 52 736 69T785 87Q803 87 803 75T791 44T754 3T685 -33T588 -48Q568 -48 562 -46Q522 -31 522 13V23Q531 129 562 250L569 281L565 280Q561 278 556 277T549 274L438 273H328L321 249Q307 202 275 107T232 0Q219 -16 196 -28T155 -41Q149 -41 145 -39T140 -34T139 -29Q139 -24 148 -3T181 86T233 247Q240 270 240 272Q240 273 194 273H169Q139 273 139 285Q139 295 153 308T187 332Q206 341 236 342L260 343L264 359Q278 414 289 482T300 578Q300 613 260 613H254Q198 613 169 592Q148 578 127 544T104 508Q72 478 37 475Q18 475 18 487Z"></path></g></g></g></g></svg></mjx-container> the “bad behavior” sample space</p>
<h2 id="3-阅读方法部分-SafeDecoding"><a href="#3-阅读方法部分-SafeDecoding" class="headerlink" title="3. 阅读方法部分: SafeDecoding"></a>3. 阅读方法部分: SafeDecoding</h2><h3 id="training-phase"><a href="#training-phase" class="headerlink" title="training phase"></a>training phase</h3><ol>
<li>stages<blockquote>
<p>create a fine-tuning dataset: </p>
<ol>
<li>prompting the language model to autonomously generate responses to these harmful queries.</li>
<li>The outputs are then filtered using GPT-4<br>Train LoRA on fine-tuning dataset</li>
</ol>
</blockquote>
</li>
</ol>
<h3 id="inference-phase"><a href="#inference-phase" class="headerlink" title="inference phase"></a>inference phase</h3><ul>
<li>Step 1: Construct the Sample Space</li>
<li>Step 2: Define the Probability Function</li>
</ul>
<h2 id="终、个人感想"><a href="#终、个人感想" class="headerlink" title="终、个人感想"></a>终、个人感想</h2><p><strong>问题的核心在于</strong> : <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.109ex;" xmlns="http://www.w3.org/2000/svg" width="1.912ex" height="1.654ex" role="img" focusable="false" viewBox="0 -683 845 731"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="48" d="M18 487Q18 496 29 517T67 566T127 621T216 665T330 683Q359 683 376 669T397 643T400 622Q400 584 382 488T348 343Q348 342 467 342H587L594 366Q615 440 648 534T690 641Q701 656 723 669T764 683Q783 683 783 672L750 578Q716 485 677 346T625 101Q624 92 623 82T622 65T621 56Q621 20 658 20Q666 20 701 25Q709 52 736 69T785 87Q803 87 803 75T791 44T754 3T685 -33T588 -48Q568 -48 562 -46Q522 -31 522 13V23Q531 129 562 250L569 281L565 280Q561 278 556 277T549 274L438 273H328L321 249Q307 202 275 107T232 0Q219 -16 196 -28T155 -41Q149 -41 145 -39T140 -34T139 -29Q139 -24 148 -3T181 86T233 247Q240 270 240 272Q240 273 194 273H169Q139 273 139 285Q139 295 153 308T187 332Q206 341 236 342L260 343L264 359Q278 414 289 482T300 578Q300 613 260 613H254Q198 613 169 592Q148 578 127 544T104 508Q72 478 37 475Q18 475 18 487Z"></path></g></g></g></g></svg></mjx-container></p>
<ul>
<li>对于 <strong>jailbreaking</strong>: 设法使推理时sample跳出 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.109ex;" xmlns="http://www.w3.org/2000/svg" width="1.912ex" height="1.654ex" role="img" focusable="false" viewBox="0 -683 845 731"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="48" d="M18 487Q18 496 29 517T67 566T127 621T216 665T330 683Q359 683 376 669T397 643T400 622Q400 584 382 488T348 343Q348 342 467 342H587L594 366Q615 440 648 534T690 641Q701 656 723 669T764 683Q783 683 783 672L750 578Q716 485 677 346T625 101Q624 92 623 82T622 65T621 56Q621 20 658 20Q666 20 701 25Q709 52 736 69T785 87Q803 87 803 75T791 44T754 3T685 -33T588 -48Q568 -48 562 -46Q522 -31 522 13V23Q531 129 562 250L569 281L565 280Q561 278 556 277T549 274L438 273H328L321 249Q307 202 275 107T232 0Q219 -16 196 -28T155 -41Q149 -41 145 -39T140 -34T139 -29Q139 -24 148 -3T181 86T233 247Q240 270 240 272Q240 273 194 273H169Q139 273 139 285Q139 295 153 308T187 332Q206 341 236 342L260 343L264 359Q278 414 289 482T300 578Q300 613 260 613H254Q198 613 169 592Q148 578 127 544T104 508Q72 478 37 475Q18 475 18 487Z"></path></g></g></g></g></svg></mjx-container> 是关键</li>
<li>对于 <strong>security</strong>: 则反之</li>
</ul>
<p><strong>现在来看section 2. Related work</strong></p>
<ul>
<li><p><strong>empirical jail breaking</strong></p>
<ul>
<li><p>Liu et al. (2023b) demonstrates <strong>prompt engineering</strong> can effectively jailbreak ChatGPT.</p>
</li>
<li><p>Wei et al. (2023a) identify the root causes of LLMs’ susceptibility to jailbreak attacks as <strong>competing objectives and generalization mismatch</strong>.</p>
<ul>
<li><strong>1. 目标竞争（Competing Objectives）</strong><br> LLMs 在训练时通常被赋予多个目标，例如：<ul>
<li><strong>遵守安全性规则</strong>：避免生成有害内容。</li>
<li><strong>最大化对话质量</strong>：对用户输入提供相关、有帮助的回答。<br>这些目标之间可能存在冲突。例如，当用户输入巧妙构造的提示时，模型需要在“遵守安全规则”和“回应用户请求”之间权衡，这种冲突容易被攻击者利用。<ul>
<li><ol>
<li><strong>泛化不匹配（Generalization Mismatch）</strong></li>
</ol>
</li>
</ul>
</li>
<li>模型在训练过程中，安全性和任务性能的泛化能力可能存在差异：</li>
<li>安全性目标可能在训练分布内有效，但在攻击者精心设计的提示下，模型无法正确泛化安全规则。<br>泛化能力的缺陷导致模型对越狱提示（jailbreak prompts）缺乏鲁棒性，从而生成不安全或违反设计意图的内容。</li>
</ul>
</li>
</ul>
<ul>
<li><p>Zeng et al. (2024) employs a <strong>persuasion taxonomy</strong> from social science to jailbreak LLMs.</p>
<ul>
<li><strong>说服分类法（Persuasion Taxonomy）</strong> (社科概念)</li>
</ul>
</li>
<li><p>Huang et al. (2023) find alterations in <strong>decoding settings</strong> are sufficient to jailbreak many open-source language models.</p>
</li>
<li>Jiang et al. (2024) develop an <strong>ASCII-art based prompt</strong> to jailbreak LLMs.</li>
<li>Deng et al. (2023c) identify the <strong>multilingual</strong> jailbreak challenges of LLMs.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Optimization-based attacks</strong> (identify adversarial prompts through optimization techniques)</p>
<ul>
<li><strong>Gradient-based</strong>: optimize and generate adversarial inputs using gradients</li>
<li><strong>Genetic algorithms-based</strong>: utilize mutation and crossover to discover effective jailbreak prompts</li>
<li><strong>Edit-based method</strong>: leverage a <strong>pre-trained LLM</strong> to revise and enhance the adversarial prompt to subvert alignment.</li>
</ul>
</li>
<li><p>defence</p>
<ul>
<li>Detection-based Defense. (引入<strong>专门的</strong>检测模块, 硬性检查query)</li>
<li>Mitigation-based Defense. (不硬性detect)<h1 id="二、多模态-MLLM"><a href="#二、多模态-MLLM" class="headerlink" title="二、多模态(MLLM)"></a>二、多模态(MLLM)</h1></li>
</ul>
</li>
</ul>
<p>多模态的攻击的创新设计往往集中在image模态</p>
<h3 id="1-典型的jailbreak"><a href="#1-典型的jailbreak" class="headerlink" title="1. 典型的jailbreak"></a>1. 典型的jailbreak</h3><p>在此只总结<strong>仅对img的攻击</strong>和<strong>联合攻击text和img</strong></p>
<p><strong>仅对img的攻击</strong>(这类方法的insight均是图像token异于文本token的<strong>高维连续性质</strong>可以帮助制造对抗样本)</p>
<ul>
<li><strong>empirical jail breaking</strong><ul>
<li>Jailbreak Large Vision-Language Models Through Multi-Modal Linkage (利用各种高层的加密手段去减少恶意内容暴露, 通过CoT去解码诱导jailbreak, 成功击败了4-o) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00473">http://arxiv.org/abs/2412.00473</a></li>
</ul>
</li>
<li><strong>Optimization-based attacks</strong><ul>
<li>Jailbreaking Attack against Multimodal Large Language Model(用极大似然优化手段去生成一个通用样本 <strong>imgJP</strong>, 拿这个当prompt去攻击所有) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.02309">http://arxiv.org/abs/2402.02309</a></li>
<li>VISUAL ADVERSARIAL EXAMPLES  JAILBREAK ALIGNED LARGE LANGUAGE MODELS(典型的用梯度手段去<strong>最大化对抗样本的概率</strong>) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.13213">http://arxiv.org/abs/2306.13213</a></li>
</ul>
</li>
</ul>
<p><strong>联合攻击text和img</strong></p>
<ul>
<li><strong>empirical jail breaking</strong><ul>
<li>Arondight:(用强化学习的手段联合攻击text和img, 具体通过<strong>prompt engineering</strong>去jailbreak) <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3664647.3681379">https://dl.acm.org/doi/10.1145/3664647.3681379</a></li>
</ul>
</li>
<li><strong>Optimization-based attacks</strong><ul>
<li>JAILBREAK IN PIECES: (把扩散加噪后看似无害的img和无害文本作为trigger去诱发jailbreak) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14539">http://arxiv.org/abs/2307.14539</a></li>
</ul>
</li>
</ul>
<h3 id="2-jailbreak的防御手段"><a href="#2-jailbreak的防御手段" class="headerlink" title="2. jailbreak的防御手段"></a>2. jailbreak的防御手段</h3><p><strong>从模态处理方式来看</strong>, 可分为“Text-Image”<strong>不区分处理</strong>和<strong>区分处理</strong></p>
<p>对于<strong>不区分处理</strong>, 我认为LLM的方法可以直接迁移</p>
<ul>
<li>Detection-based Defense. <ul>
<li>JailGuard(对样本添加扰动来增强detection效果); <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10766">http://arxiv.org/abs/2312.10766</a></li>
</ul>
</li>
<li>Mitigation-based Defense. <ul>
<li>AdaShield(典型的特征工程,构建一个prompt集,推理时从中找最合适的); <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09513">http://arxiv.org/abs/2403.09513</a></li>
</ul>
</li>
</ul>
<p>对于<strong>区分处理</strong></p>
<ul>
<li>Detection-based Defense. <ul>
<li>CIDER(基于“Insight that 对抗样本更不robust”, 作者设计了个即插即用的检测块, 重点通过检查img在加噪前后的变化程度来进行defense“ <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21659">http://arxiv.org/abs/2407.21659</a></li>
</ul>
</li>
<li>Mitigation-based Defense. <ul>
<li>(暂未仔细看)</li>
</ul>
</li>
</ul>
<h3 id="3-感想"><a href="#3-感想" class="headerlink" title="3. 感想"></a>3. 感想</h3><ol>
<li>多模态里<strong>视觉模态</strong>和<strong>音频模态</strong>一定是最薄弱的一环. </li>
<li>LLM的越狱安全依旧有可研究之处, Jailbreak Large Vision-Language Models Through Multi-Modal Linkage 一文中提出的“密码学级别的加密和利用CoT解密”的攻击方法目前没有LLM能抗衡</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/09/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E5%8F%8A%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB%E9%98%B2%E5%BE%A1/" data-id="cm5ar5vhh000bciwi3sn0alzv" data-title="多模态大模型安全及越狱攻击防御" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm-securaty/" rel="tag">llm-securaty</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-MAR" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/07/MAR/" class="article-date">
  <time class="dt-published" datetime="2024-12-06T16:10:39.000Z" itemprop="datePublished">2024-12-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/07/MAR/">MAR</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前注"><a href="#前注" class="headerlink" title="前注"></a>前注</h1><p>diffusion部分用的ddim, 所以等我整理完个扩散模型的专题后再详细补完吧:</p>
<ul>
<li>参照 <a target="_blank" rel="noopener" href="https://github.com/hojonathanho/diffusion">https://github.com/hojonathanho/diffusion</a></li>
</ul>
<h1 id="Autoregressive-Image-Generation-without-Vector-Quantization-凯明组出品"><a href="#Autoregressive-Image-Generation-without-Vector-Quantization-凯明组出品" class="headerlink" title="Autoregressive Image Generation without Vector Quantization(凯明组出品)"></a>Autoregressive Image Generation without Vector Quantization(凯明组出品)</h1><p>今天曲师姐组会分享了这篇文章, 趁热打铁, 过一遍顺便再把code看一下</p>
<ul>
<li>arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.11838">https://arxiv.org/abs/2406.11838</a></li>
<li>github: <a target="_blank" rel="noopener" href="https://github.com/LTH14/mar">https://github.com/LTH14/mar</a><br>其他地方都挺清楚的, 就是在充当小扩散模型的MLP及训练loss有疑问</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2>
<p><strong>疑问:</strong></p>
<ol>
<li>feature_map是离散的, 但扩散后是连续的, 离散时训练的VQ-VAE的decoder会“认账”吗?<ul>
<li>答: 会</li>
</ul>
</li>
<li>Sec. 3.2中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.469ex;" xmlns="http://www.w3.org/2000/svg" width="32.935ex" height="4.07ex" role="img" focusable="false" viewBox="0 -1149.5 14557.4 1799"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="mo" transform="translate(690,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1079,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1544,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1988.7,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2560.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3227.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(4283.2,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D53C" d="M12 666Q12 675 24 683H582Q590 680 593 672V588Q593 514 591 502T575 490Q567 490 563 495T555 517Q552 556 517 590Q486 623 445 634T340 648H282Q266 636 264 620T260 492V370H277Q329 375 358 391T404 439Q420 480 420 506Q420 529 436 529Q445 529 451 521Q455 517 455 361Q455 333 455 298T456 253Q456 217 453 207T437 197Q420 196 420 217Q420 240 406 270Q377 328 284 335H260V201Q261 174 261 134Q262 73 264 61T278 38Q281 36 282 35H331Q400 35 449 50Q571 93 602 179Q605 203 622 203Q629 203 634 197T640 183Q638 181 624 95T604 3L600 -1H24Q12 5 12 16Q12 35 51 35Q92 38 97 52Q102 60 102 341T97 632Q91 645 51 648Q12 648 12 666ZM137 341Q137 131 136 89T130 37Q129 36 129 35H235Q233 41 231 48L226 61V623L231 635L235 648H129Q132 641 133 638T135 603T137 517T137 341ZM557 603V648H504Q504 646 515 639Q527 634 542 619L557 603ZM420 317V397L406 383Q394 370 380 363L366 355Q373 350 382 346Q400 333 409 328L420 317ZM582 61L586 88Q585 88 582 83Q557 61 526 46L511 37L542 35H577Q577 36 578 39T580 49T582 61Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(700,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g><g data-mml-node="mo" transform="translate(466,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(744,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(5981.2,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M224 -649V1150H455V1099H275V-598H455V-649H224Z"></path></g><g data-mml-node="msup" transform="translate(472,0)"><g data-mml-node="mrow"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g><g data-mml-node="mo" transform="translate(966.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(1966.4,0)"><g data-mml-node="mi"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g><g data-mml-node="mi" transform="translate(499,-150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(2847.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3236.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(4424.1,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(4979.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(5340.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5785.6,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(6250.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6639.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g><g data-mml-node="mn" transform="translate(6950.6,477.1) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(7826.1,0) translate(0 -0.5)"><path data-c="5D" d="M16 1099V1150H247V-649H16V-598H196V1099H16Z"></path></g></g><g data-mml-node="mo" transform="translate(14279.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g></g></g></svg></mjx-container>到底是个?(你真得好好看看扩散了, 必须得补一篇关于扩散的blog)</li>
<li>Sec. 3.3中称<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="22.502ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9945.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1069.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2125.5,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2675.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3064.5,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(4073.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4517.7,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5526.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(5970.9,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(7309.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(7754.3,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(9556.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>, 但事实上进transformer绝对都是离散的z啊, 这个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container>又是什么?<h2 id="diffusion-loss"><a href="#diffusion-loss" class="headerlink" title="diffusion loss"></a>diffusion loss</h2></li>
</ol>
<p>由于笔者对diffusion-model并不熟悉, 短期内恶补相关知识又不太现实(<strong>一周内必定补全相关blog</strong>, 说到做到), 因此先就事论事一下. </p>
<p>从整体结构而言, 该项目把<em>parameterized loss</em>(ganloss和diffloss)全与模型本体隔离了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --nnodes=4 --node_rank=<span class="variable">${NODE_RANK}</span> --master_addr=<span class="variable">${MASTER_ADDR}</span> --master_port=<span class="variable">${MASTER_PORT}</span> \</span><br><span class="line">main_mar.py \</span><br><span class="line">--img_size 256 --vae_path pretrained_models/vae/kl16.ckpt --vae_embed_dim 16 --vae_stride 16 --patch_size 1 \</span><br><span class="line">--model mar_large --diffloss_d 3 --diffloss_w 1024 \</span><br><span class="line">--epochs 400 --warmup_epochs 100 --batch_size 64 --blr 1.0e-4 --diffusion_batch_mul 4 \</span><br><span class="line">--output_dir <span class="variable">${OUTPUT_DIR}</span> --resume <span class="variable">${OUTPUT_DIR}</span> \</span><br><span class="line">--data_path <span class="variable">${IMAGENET_PATH}</span></span><br></pre></td></tr></table></figure></p>
<p>由上可见, MLP depth只有“3”</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiffLoss</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Diffusion Loss"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, target_channels, z_channels, depth, width, num_sampling_steps, grad_checkpointing=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DiffLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.in_channels = target_channels</span><br><span class="line">        <span class="variable language_">self</span>.net = SimpleMLPAdaLN(</span><br><span class="line">            in_channels=target_channels,</span><br><span class="line">            model_channels=width,</span><br><span class="line">            out_channels=target_channels * <span class="number">2</span>,  <span class="comment"># for vlb loss</span></span><br><span class="line">            z_channels=z_channels,</span><br><span class="line">            num_res_blocks=depth,</span><br><span class="line">            grad_checkpointing=grad_checkpointing</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.train_diffusion = create_diffusion(timestep_respacing=<span class="string">""</span>, noise_schedule=<span class="string">"cosine"</span>)</span><br><span class="line">        <span class="variable language_">self</span>.gen_diffusion = create_diffusion(timestep_respacing=num_sampling_steps, noise_schedule=<span class="string">"cosine"</span>)</span><br></pre></td></tr></table></figure>
<p>首先来看<code>self.net</code> ,(附有绘制的总体MLP- diffusion总体结构图)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleMLPAdaLN</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The MLP for Diffusion Loss.</span></span><br><span class="line"><span class="string">    :param in_channels: channels in the input Tensor.</span></span><br><span class="line"><span class="string">    :param model_channels: base channel count for the model.</span></span><br><span class="line"><span class="string">    :param out_channels: channels in the output Tensor.</span></span><br><span class="line"><span class="string">    :param z_channels: channels in the condition.</span></span><br><span class="line"><span class="string">    :param num_res_blocks: number of residual blocks per downsample.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channels,</span></span><br><span class="line"><span class="params">        model_channels,</span></span><br><span class="line"><span class="params">        out_channels,</span></span><br><span class="line"><span class="params">        z_channels,</span></span><br><span class="line"><span class="params">        num_res_blocks,</span></span><br><span class="line"><span class="params">        grad_checkpointing=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">        <span class="variable language_">self</span>.model_channels = model_channels</span><br><span class="line">        <span class="variable language_">self</span>.out_channels = out_channels</span><br><span class="line">        <span class="variable language_">self</span>.num_res_blocks = num_res_blocks</span><br><span class="line">        <span class="variable language_">self</span>.grad_checkpointing = grad_checkpointing</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.time_embed = TimestepEmbedder(model_channels)</span><br><span class="line">        <span class="variable language_">self</span>.cond_embed = nn.Linear(z_channels, model_channels)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.input_proj = nn.Linear(in_channels, model_channels)</span><br><span class="line"></span><br><span class="line">        res_blocks = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">            res_blocks.append(ResBlock(</span><br><span class="line">                model_channels,</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.res_blocks = nn.ModuleList(res_blocks)</span><br><span class="line">        <span class="variable language_">self</span>.final_layer = FinalLayer(model_channels, out_channels)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.initialize_weights()</span><br></pre></td></tr></table></figure><br>下面来看<code>forward</code>部分和扩散部分的总体结构图<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----------------------</span></span><br><span class="line"><span class="comment"># DiffLoss forward 部分</span></span><br><span class="line"><span class="comment">#-----------------------</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, target, z, mask=<span class="literal">None</span></span>):</span><br><span class="line">        t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.train_diffusion.num_timesteps, (target.shape[<span class="number">0</span>],), device=target.device)</span><br><span class="line">        model_kwargs = <span class="built_in">dict</span>(c=z)</span><br><span class="line">        loss_dict = <span class="variable language_">self</span>.train_diffusion.training_losses(<span class="variable language_">self</span>.net, target, t, model_kwargs)</span><br><span class="line">        loss = loss_dict[<span class="string">"loss"</span>]</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------------------</span></span><br><span class="line"><span class="comment"># train_diffusion 是什么</span></span><br><span class="line"><span class="comment">#------------------------</span></span><br><span class="line"><span class="variable language_">self</span>.train_diffusion = create_diffusion(timestep_respacing=<span class="string">""</span>, noise_schedule=<span class="string">"cosine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#-------------------------------------------</span></span><br><span class="line"><span class="comment"># self.train_diffusion.training_losses 是什么</span></span><br><span class="line"><span class="comment">#-------------------------------------------</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpacedDiffusion</span>(<span class="title class_ inherited__">GaussianDiffusion</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, model, *args, **kwargs</span></span><br><span class="line"><span class="params">    </span>):  <span class="comment"># pylint: disable=signature-differs</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().training_losses(<span class="variable language_">self</span>._wrap_model(model), *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params">self, model, x_start, t, model_kwargs=<span class="literal">None</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute training losses for a single timestep.</span></span><br><span class="line"><span class="string">        :param model: the model to evaluate loss on.</span></span><br><span class="line"><span class="string">        :param x_start: the [N x C x ...] tensor of inputs.</span></span><br><span class="line"><span class="string">        :param t: a batch of timestep indices.</span></span><br><span class="line"><span class="string">        :param model_kwargs: if not None, a dict of extra keyword arguments to</span></span><br><span class="line"><span class="string">            pass to the model. This can be used for conditioning.</span></span><br><span class="line"><span class="string">        :param noise: if specified, the specific Gaussian noise to try to remove.</span></span><br><span class="line"><span class="string">        :return: a dict with the key "loss" containing a tensor of shape [N].</span></span><br><span class="line"><span class="string">                 Some mean or variance settings may also have other keys.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = {}</span><br><span class="line">        <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            noise = th.randn_like(x_start)</span><br><span class="line">        x_t = <span class="variable language_">self</span>.q_sample(x_start, t, noise=noise)</span><br><span class="line"></span><br><span class="line">        terms = {}</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.KL <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">            terms[<span class="string">"loss"</span>] = <span class="variable language_">self</span>._vb_terms_bpd( <span class="comment">#专门算vb_loss的</span></span><br><span class="line">                model=model, </span><br><span class="line">                x_start=x_start,</span><br><span class="line">                x_t=x_t, </span><br><span class="line">                t=t,</span><br><span class="line">                clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                model_kwargs=model_kwargs,</span><br><span class="line">            )[<span class="string">"output"</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">                terms[<span class="string">"loss"</span>] *= <span class="variable language_">self</span>.num_timesteps</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == LossType.MSE <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">            model_output = model(x_t, t, **model_kwargs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [</span><br><span class="line">                ModelVarType.LEARNED,</span><br><span class="line">                ModelVarType.LEARNED_RANGE,</span><br><span class="line">            ]:</span><br><span class="line">                B, C = x_t.shape[:<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x_t.shape[<span class="number">2</span>:])</span><br><span class="line">                model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Learn the variance using the variational bound, but don't let</span></span><br><span class="line">                <span class="comment"># it affect our mean prediction.</span></span><br><span class="line">                frozen_out = th.cat([model_output.detach(), model_var_values], dim=<span class="number">1</span>)</span><br><span class="line">                terms[<span class="string">"vb"</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                    model=<span class="keyword">lambda</span> *args, r=frozen_out: r,</span><br><span class="line">                    x_start=x_start,</span><br><span class="line">                    x_t=x_t,</span><br><span class="line">                    t=t,</span><br><span class="line">                    clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                )[<span class="string">"output"</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">                    <span class="comment"># Divide by 1000 for equivalence with initial implementation.</span></span><br><span class="line">                    <span class="comment"># Without a factor of 1/1000, the VB term hurts the MSE term.</span></span><br><span class="line">                    terms[<span class="string">"vb"</span>] *= <span class="variable language_">self</span>.num_timesteps / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">            target = {</span><br><span class="line">                ModelMeanType.PREVIOUS_X: <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">                    x_start=x_start, x_t=x_t, t=t</span><br><span class="line">                )[<span class="number">0</span>],</span><br><span class="line">                ModelMeanType.START_X: x_start,</span><br><span class="line">                ModelMeanType.EPSILON: noise,</span><br><span class="line">            }[<span class="variable language_">self</span>.model_mean_type]</span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == target.shape == x_start.shape</span><br><span class="line">            terms[<span class="string">"mse"</span>] = mean_flat((target - model_output) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"vb"</span> <span class="keyword">in</span> terms:</span><br><span class="line">                terms[<span class="string">"loss"</span>] = terms[<span class="string">"mse"</span>] + terms[<span class="string">"vb"</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                terms[<span class="string">"loss"</span>] = terms[<span class="string">"mse"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="variable language_">self</span>.loss_type)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> terms</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_diffusion</span>(<span class="params"></span></span><br><span class="line"><span class="params">    timestep_respacing,</span></span><br><span class="line"><span class="params">    noise_schedule=<span class="string">"linear"</span>, </span></span><br><span class="line"><span class="params">    use_kl=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    sigma_small=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    predict_xstart=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    learn_sigma=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    rescale_learned_sigmas=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    diffusion_steps=<span class="number">1000</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">	<span class="comment">#------------------------------------</span></span><br><span class="line">	<span class="comment"># get_named_beta_schedule: 加噪声策略</span></span><br><span class="line">	<span class="comment">#------------------------------------</span></span><br><span class="line">    betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)</span><br><span class="line">    <span class="keyword">if</span> use_kl:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_KL</span><br><span class="line">    <span class="keyword">elif</span> rescale_learned_sigmas:</span><br><span class="line">        loss_type = gd.LossType.RESCALED_MSE</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loss_type = gd.LossType.MSE</span><br><span class="line">    <span class="keyword">if</span> timestep_respacing <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> timestep_respacing == <span class="string">""</span>:</span><br><span class="line">        timestep_respacing = [diffusion_steps]</span><br><span class="line">    <span class="keyword">return</span> SpacedDiffusion(</span><br><span class="line">        use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),</span><br><span class="line">        betas=betas,</span><br><span class="line">        model_mean_type=(</span><br><span class="line">            gd.ModelMeanType.EPSILON <span class="keyword">if</span> <span class="keyword">not</span> predict_xstart <span class="keyword">else</span> gd.ModelMeanType.START_X</span><br><span class="line">        ),</span><br><span class="line">        model_var_type=(</span><br><span class="line">            (</span><br><span class="line">                gd.ModelVarType.FIXED_LARGE</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> sigma_small</span><br><span class="line">                <span class="keyword">else</span> gd.ModelVarType.FIXED_SMALL</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> learn_sigma</span><br><span class="line">            <span class="keyword">else</span> gd.ModelVarType.LEARNED_RANGE</span><br><span class="line">        ),</span><br><span class="line">        loss_type=loss_type</span><br><span class="line">        <span class="comment"># rescale_timesteps=rescale_timesteps,</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpacedDiffusion</span>(<span class="title class_ inherited__">GaussianDiffusion</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A diffusion process which can skip steps in a base diffusion process.</span></span><br><span class="line"><span class="string">    :param use_timesteps: a collection (sequence or set) of timesteps from the</span></span><br><span class="line"><span class="string">                          original diffusion process to retain.</span></span><br><span class="line"><span class="string">    :param kwargs: the kwargs to create the base diffusion process.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, use_timesteps, **kwargs</span>):</span><br><span class="line">        <span class="variable language_">self</span>.use_timesteps = <span class="built_in">set</span>(use_timesteps)</span><br><span class="line">        <span class="variable language_">self</span>.timestep_map = []</span><br><span class="line">        <span class="variable language_">self</span>.original_num_steps = <span class="built_in">len</span>(kwargs[<span class="string">"betas"</span>])</span><br><span class="line"></span><br><span class="line">        base_diffusion = GaussianDiffusion(**kwargs)  <span class="comment"># pylint: disable=missing-kwoa</span></span><br><span class="line">        last_alpha_cumprod = <span class="number">1.0</span></span><br><span class="line">        new_betas = []</span><br><span class="line">        <span class="keyword">for</span> i, alpha_cumprod <span class="keyword">in</span> <span class="built_in">enumerate</span>(base_diffusion.alphas_cumprod):</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.use_timesteps:</span><br><span class="line">                new_betas.append(<span class="number">1</span> - alpha_cumprod / last_alpha_cumprod)</span><br><span class="line">                last_alpha_cumprod = alpha_cumprod</span><br><span class="line">                <span class="variable language_">self</span>.timestep_map.append(i)</span><br><span class="line">        kwargs[<span class="string">"betas"</span>] = np.array(new_betas)</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Utilities for training and sampling diffusion models.</span></span><br><span class="line"><span class="string">    Original ported from this codebase:</span></span><br><span class="line"><span class="string">    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42</span></span><br><span class="line"><span class="string">    :param betas: a 1-D numpy array of betas for each diffusion timestep,</span></span><br><span class="line"><span class="string">                  starting at T and going to 1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        betas,</span></span><br><span class="line"><span class="params">        model_mean_type,</span></span><br><span class="line"><span class="params">        model_var_type,</span></span><br><span class="line"><span class="params">        loss_type</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.num_timesteps = <span class="built_in">int</span>(betas.shape[<span class="number">0</span>])</span><br><span class="line">		...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对于<code>ResBlock</code>, 整体来看是用dit里adaln那一套作为time_embedder去扩散<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual block that can optionally change the number of channels.</span></span><br><span class="line"><span class="string">    :param channels: the number of input channels.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        channels</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.in_ln = nn.LayerNorm(channels, eps=<span class="number">1e-6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.adaLN_modulation = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, <span class="number">3</span> * channels, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><br>对于其 <code>forward</code> 部分,先贴代码再贴示意图:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">modulate</span>(<span class="params">x, shift, scale</span>):</span><br><span class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> + scale) + shift</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">	shift_mlp, scale_mlp, gate_mlp = <span class="variable language_">self</span>.adaLN_modulation(y).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">	h = modulate(<span class="variable language_">self</span>.in_ln(x), shift_mlp, scale_mlp)</span><br><span class="line">	h = <span class="variable language_">self</span>.mlp(h)</span><br><span class="line">	<span class="keyword">return</span> x + gate_mlp * h</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/07/MAR/" data-id="cm5ar5vhg0005ciwihluxavs6" data-title="MAR" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MoE-Rag-Peft-RLHF-DPO" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T18:16:27.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/">MoE_Rag_Peft_RLHF_DPO</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="零"><a href="#零" class="headerlink" title="零"></a>零</h1><p>本篇就是个杂记,杂烩涉猎一番,不精不深, 故而也不分不分章节序号了</p>
<h1 id="MoE-Mixture-of-Experts"><a href="#MoE-Mixture-of-Experts" class="headerlink" title="MoE(Mixture of Experts)"></a>MoE(Mixture of Experts)</h1><blockquote>
<p>混合专家模型（Mixture of Experts，简称MOE）是一种机器学习的集成技术，由多个专家模型（Experts）和一个门控网络（Gating Network）组成。这种模型的核心思想是将复杂的问题分解成多个子问题，每个子问题由一个专家模型处理，而门控网络则负责决定每个输入样本应该由哪个专家处理。</p>
</blockquote>

<p>MoE的具体实现可以是多个FFN, 也可以更复杂(比如垒出层级的gate+FFN). 同时由此可见, expert层确实是模型权重的大头. </p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><blockquote>
<ul>
<li><p>能够在远少于稠密模型所需的计算资源下进行有效的预训练。</p>
</li>
<li><p>这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。</p>
</li>
<li><p>特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p>
</li>
</ul>
</blockquote>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul>
<li>在微调阶段往往面临<strong>泛化能力不足</strong>的问题，长期以来易于引发<strong>过拟合</strong>现象。</li>
<li><strong>推理时参数量过大</strong>(所有expert都得上内存), 因此对内存的需求非常高</li>
<li><strong>batch大小的不均匀分配</strong>(不会均匀分给每一个expert)和<strong>资源利用效率不高</strong>(稠密模型时刻负载都会很高, 而MoE由于其特性很难做到)</li>
</ul>
<h2 id="典型解决办法"><a href="#典型解决办法" class="headerlink" title="典型解决办法"></a>典型解决办法</h2><ul>
<li><strong>更强的内部正则化</strong>:<ul>
<li><blockquote>
<p>例如，可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p>
</blockquote>
</li>
<li>Router z-loss <strong>(回头仔细看看)</strong><blockquote>
<p>在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 logits 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>微调阶段</strong>:<ul>
<li><strong>不应</strong>冻结所有非专家层的权重。实践中，这会导致性能大幅下降(MoE层占据了网络的主要部分, 和Qwen-VL类似, 直接把vision model冻了可能把模型调“坏”);<br><strong>应</strong>仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</li>
<li>较小的batchsize大小和较高的lr</li>
</ul>
</li>
<li><p><strong>推理时参数量过大</strong>:</p>
<ul>
<li><blockquote>
<p>预先蒸馏: 将 MoE 模型蒸馏回其对应的稠密模型。</p>
</blockquote>
</li>
<li><blockquote>
<p>专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。</p>
</blockquote>
</li>
<li><blockquote>
<p>任务级别路由: 路由器被修改为将整个句子或任务直接路由到一个专家。</p>
</blockquote>
<p>(最后一条有点费解,<strong>回头仔细看</strong>)</p>
</li>
</ul>
</li>
<li><p><strong>资源利用效率不高</strong></p>
<ul>
<li><blockquote>
<p>在experts并行中，experts被放置在不同的node上，每个node处理不同批次的训练样本。对于非 MoE 层，experts并行的行为与数据并行相同。对于 MoE 层，序列中的tokens被发送到拥有所需expert的节点。(有点类似model parallel)</p>
</blockquote>

</li>
<li><blockquote>
<p>FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p>
</blockquote>
</li>
<li><blockquote>
<p>Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>batch大小的不均匀分配</strong><br>采用一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E)，门控网络通常为一个带有softmax函数的简单网络。</p>
<ul>
<li>MOE层<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex;" xmlns="http://www.w3.org/2000/svg" width="20.712ex" height="2.563ex" role="img" focusable="false" viewBox="0 -789.6 9154.8 1132.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(1823.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(4276.9,0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(5062.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5451.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(6023.9,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(6739.8,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(771,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7804.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8193.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(8765.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>门控网络<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="24.898ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 11005 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(819,-150) scale(0.707)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g></g><g data-mml-node="mo" transform="translate(1272.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1661.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2233.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2900.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(3956.3,0)"><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(556,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(1056,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1362,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1751,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2584,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(3084,0)"></path></g><g data-mml-node="mo" transform="translate(7568.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7957.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(8751.5,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msub" transform="translate(9251.8,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g></g><g data-mml-node="mo" transform="translate(10616,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>tok门控<ul>
<li>添加噪声<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="60.901ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 26918.3 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(888,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1277,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(1849,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2842.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3898.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4287.5,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(5081.7,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msub" transform="translate(5582,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g></g><g data-mml-node="msub" transform="translate(6946.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7884.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mtext" transform="translate(8884.6,0)"><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(556,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(945,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1445,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(2001,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(2557,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(3057,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(3449,0)"></path><path data-c="4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z" transform="translate(4005,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(4755,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(5255,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(5647,0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(6480,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(6980,0)"></path></g><g data-mml-node="mo" transform="translate(16142.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(16531.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(17142.9,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mtext" transform="translate(17643.1,0)"><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(556,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(1056,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1362,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1751,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(2307,0)"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(2585,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3141,0)"></path></g><g data-mml-node="mo" transform="translate(21178.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(21567.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(21956.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(22750.3,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msub" transform="translate(23250.5,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mtext" transform="translate(977,-150) scale(0.707)"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(556,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(1056,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(1334,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1728,0)"></path></g></g><g data-mml-node="msub" transform="translate(25813.4,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(26529.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>保留前K值<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="72.927ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 32233.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="4B" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H313Q235 637 233 620Q232 618 232 462L233 307L379 449Q425 494 479 546Q518 584 524 591T531 607V608Q531 630 503 636Q501 636 498 636T493 637H489V683H499Q517 680 630 680Q704 680 716 683H722V637H708Q633 633 589 597Q584 592 495 506T406 419T515 254T631 80Q644 60 662 54T715 46H736V0H728Q719 3 615 3Q493 3 472 0H461V46H469Q515 46 515 72Q515 78 512 84L336 351Q332 348 278 296L232 251V156Q232 62 235 58Q243 47 302 46H335V0H324Q303 3 180 3Q45 3 36 0H25V46H58Q100 47 109 49T128 61V622Z"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(778,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1222,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1666,0)"></path><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" transform="translate(2222,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(2944,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(3444,0)"></path><path data-c="4B" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H313Q235 637 233 620Q232 618 232 462L233 307L379 449Q425 494 479 546Q518 584 524 591T531 607V608Q531 630 503 636Q501 636 498 636T493 637H489V683H499Q517 680 630 680Q704 680 716 683H722V637H708Q633 633 589 597Q584 592 495 506T406 419T515 254T631 80Q644 60 662 54T715 46H736V0H728Q719 3 615 3Q493 3 472 0H461V46H469Q515 46 515 72Q515 78 512 84L336 351Q332 348 278 296L232 251V156Q232 62 235 58Q243 47 302 46H335V0H324Q303 3 180 3Q45 3 36 0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(4000,0)"></path></g><g data-mml-node="mo" transform="translate(4778,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5167,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(5652,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(6096.7,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="msub" transform="translate(6617.7,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7611.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(8667.2,0)"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mtable" transform="translate(500,0)"><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(1812,0)"><g data-mml-node="mtext"><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(278,0)"></path><path data-c="A0" d="" transform="translate(584,0)"></path></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mtext" transform="translate(1646,0)"><path data-c="A0" d=""></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(250,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(528,0)"></path><path data-c="20" d="" transform="translate(922,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(1172,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1450,0)"></path><path data-c="20" d="" transform="translate(2006,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(2256,0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2645,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(3201,0)"></path><path data-c="20" d="" transform="translate(3645,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3895,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(4284,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(4784,0)"></path><path data-c="A0" d="" transform="translate(5340,0)"></path></g><g data-mml-node="mi" transform="translate(7236,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mtext" transform="translate(7757,0)"><path data-c="A0" d=""></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(250,0)"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(694,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(972,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1416,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2249,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2693,0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3249,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3638,0)"></path><path data-c="20" d="" transform="translate(4032,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(4282,0)"></path><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z" transform="translate(4782,0)"></path><path data-c="A0" d="" transform="translate(5088,0)"></path></g><g data-mml-node="mi" transform="translate(13095,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(13580,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mtext" fill="red" stroke="red" transform="translate(14024.6,0)"><path data-c="5C" d="M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z"></path><path data-c="2D" d="M11 179V252H277V179H11Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(14857.6,0)"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g></g><g data-mml-node="mtd" transform="translate(18669.6,0)"><g data-mml-node="mtext"><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(500,0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(889,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1445,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1889,0)"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(2281,0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(3003,0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3281,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(3675,0)"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(4119,0)"></path></g></g></g></g><g data-mml-node="mo" transform="translate(23566.6,0) translate(0 250)"></g></g></g></g></svg></mjx-container></li>
<li>门控网络<br>G(x) &amp;= \text{Softmax}(\text{KeepTopK}(H(x), k))</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" data-id="cm5ar5vhg0006ciwiclx5gavg" data-title="MoE_Rag_Peft_RLHF_DPO" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Olmo及dolma实践" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T15:13:57.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/">Olmo及dolma实践</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="零、导言"><a href="#零、导言" class="headerlink" title="零、导言"></a>零、导言</h3><h4 id="0-0-模型训练的总体流程"><a href="#0-0-模型训练的总体流程" class="headerlink" title="0.0 模型训练的总体流程"></a><strong>0.0 模型训练的总体流程</strong></h4><p>模型训练的完整流程通常包括以下几个步骤：</p>
<ol>
<li><strong>数据准备</strong>：将原始数据通过数据清洗与处理，转化为可用于模型训练的高质量训练数据。Dolma 工具在此阶段发挥重要作用，确保数据经过标记、去重、过滤等处理步骤，形成高质量的训练语料。</li>
<li><strong>词元切分</strong>：对训练数据进行 Tokenization，将文本数据转换为模型能够理解和处理的 tokens，生成可供训练的输入数据。</li>
<li><strong>预训练</strong>：使用训练数据对 OLMo 模型进行预训练，模型在大量数据上学习语言的基本规律和特征。</li>
<li><strong>指令微调</strong>：对预训练后的模型进行指令微调，使其能够更好地理解和执行特定任务指令。</li>
<li><strong>模型评测</strong>：使用 OpenCompass 工具对模型进行全面评测，包括对模型的语言理解、生成能力等多方面的评价，以便了解模型性能并进行进一步优化。</li>
</ol>
<h3 id="一、Docker使用与环境搭建"><a href="#一、Docker使用与环境搭建" class="headerlink" title="一、Docker使用与环境搭建"></a><strong>一、Docker使用与环境搭建</strong></h3><h4 id="1-1-Docker简介"><a href="#1-1-Docker简介" class="headerlink" title="1.1 Docker简介"></a><strong>1.1 Docker简介</strong></h4><p><strong>容器技术的定义</strong>：容器技术通过在物理主机操作系统上创建一个个孤立的分组（即容器），将应用程序及其依赖项打包在一个独立的容器中，使其能够在任何支持容器的环境中运行，而不受底层系统的影响。Docker 是目前主流的容器技术之一，其他类似的容器技术还有 Kubernetes (K8s) 等。</p>
<p><strong>Docker的特点</strong>：</p>
<ul>
<li><strong>轻量级</strong>：容器共享主机的操作系统内核，避免了虚拟机的资源开销，启动更快，占用更少资源。</li>
<li><strong>可移植性</strong>：容器技术允许应用程序在不同的云平台和数据中心中轻松迁移，确保在不同环境中运行时不会出现兼容性问题。</li>
<li><strong>一致性</strong>：容器保证了应用程序在不同环境中的一致性运行，减少了“在我的机器上能够运行”的问题，使得开发、测试、生产等环境的统一性得以保证。</li>
</ul>
<h4 id="1-2-Docker的基本概念"><a href="#1-2-Docker的基本概念" class="headerlink" title="1.2 Docker的基本概念"></a><strong>1.2 Docker的基本概念</strong></h4><p>Docker 包括三个基本概念：</p>
<ol>
<li><strong>镜像（Image）</strong>：Docker 镜像相当于一个 root 文件系统。例如，官方镜像 <code>ubuntu:16.04</code> 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。镜像是静态的，作为容器的模板存在。</li>
<li><strong>容器（Container）</strong>：容器是镜像的运行实例，类似于面向对象编程中的类和对象。镜像是静态的定义，容器是镜像在运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</li>
<li><strong>仓库（Repository）</strong>：仓库是用于保存镜像的地方，类似于代码控制中心。在仓库中，可以存储、获取、分发 Docker 镜像。</li>
</ol>
<h4 id="1-3-Docker安装与使用"><a href="#1-3-Docker安装与使用" class="headerlink" title="1.3 Docker安装与使用"></a><strong>1.3 Docker安装与使用</strong></h4><p><strong>安装Docker</strong>：请参考官方网站的安装指南，或者访问 <a target="_blank" rel="noopener" href="https://www.runoob.com/docker/ubuntu-docker-install.html">菜鸟教程Docker安装</a> 完成 Docker 的安装。</p>
<p><strong>常用操作命令</strong>：</p>
<ol>
<li><p><strong>下载镜像</strong>：通过命令 <code>docker pull &lt;镜像名&gt;</code> 下载所需的 Docker 镜像。例如，<code>docker pull ubuntu:16.04</code>。</p>
</li>
<li><p><strong>查看镜像</strong>：使用 <code>docker images</code> 查看已经下载到本地的镜像列表。</p>
</li>
<li><p><strong>启动容器</strong>：使用 <code>docker run [OPTIONS] &lt;image&gt; [COMMAND] [ARG...]</code> 从镜像启动一个新的容器。</p>
<ul>
<li>常用的 <code>OPTIONS</code> 参数：<ul>
<li><code>--gpus</code>：配置容器内部使用的 GPU，示例如 <code>--gpus all</code>。</li>
<li><code>-v &lt;host目录&gt;:&lt;容器目录&gt;</code>：挂载主机目录到容器中，例如挂载数据集等。在容器内部修改挂载的目录或文件会直接反映到宿主机。</li>
<li><code>--privileged</code>：使用该参数，容器内的 root 用户拥有真正的 root 权限。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="1-4-容器内部环境的安装与配置"><a href="#1-4-容器内部环境的安装与配置" class="headerlink" title="1.4 容器内部环境的安装与配置"></a><strong>1.4 容器内部环境的安装与配置</strong></h4><p>在启动并进入容器后，可以对容器内部进行各种环境的配置，例如安装 Anaconda 和其他必要的工具。</p>
<p><strong>步骤</strong>：</p>
<ol>
<li><strong>安装Anaconda</strong>：下载 Anaconda 安装文件，然后运行 <code>bash Anaconda****.sh</code> 进行安装。</li>
<li><strong>创建 Python 环境</strong>：使用 Anaconda 创建一个指定 Python 版本的环境，例如 <code>conda create –name myenv python=3.8</code>，然后使用 <code>conda activate myenv</code> 激活环境。</li>
<li><strong>安装Dolma</strong>：在已激活的 Anaconda 环境中，运行 <code>pip install dolma</code> 安装 Dolma 工具包。</li>
<li><strong>安装OLMo</strong>：参照 OLMo 的 GitHub 指导文档进行安装，并确保所使用的 PyTorch 版本与 CUDA 版本匹配，以获得最佳性能。</li>
</ol>
<p><strong>环境安装验证</strong>：完成所有安装后，可运行相关命令（例如 <code>python</code>、<code>pip list</code> 等）检查环境配置是否正确。</p>
<h4 id="1-5-容器的管理与镜像保存"><a href="#1-5-容器的管理与镜像保存" class="headerlink" title="1.5 容器的管理与镜像保存"></a><strong>1.5 容器的管理与镜像保存</strong></h4><p><strong>将容器放到后台运行</strong>：</p>
<ul>
<li>使用快捷键 <code>Ctrl + P + Q</code> 将当前容器放到后台运行，而不停止容器的执行。</li>
</ul>
<p><strong>查看正在运行的容器</strong>：</p>
<ul>
<li>通过 <code>docker ps</code> 查看当前运行的容器。如果容器已停止，可使用 <code>docker ps -a</code> 查看所有容器。</li>
</ul>
<p><strong>重新启动和进入容器</strong>：</p>
<ul>
<li>使用 <code>docker start &lt;容器ID&gt;</code> 启动已停止的容器。</li>
<li>使用 <code>docker attach &lt;容器ID&gt;</code> 进入正在运行的容器。</li>
</ul>
<p><strong>保存容器为镜像</strong>：</p>
<ul>
<li>通过 <code>docker commit &lt;容器ID&gt; &lt;镜像名&gt;</code> 将已配置好环境的容器保存为一个新的镜像。</li>
</ul>
<p><strong>保存镜像为离线文件</strong>：</p>
<ul>
<li>使用 <code>docker save -o &lt;文件名&gt;.tar &lt;镜像名&gt;:&lt;tag&gt;</code> 将镜像保存为 <code>.tar</code> 文件，便于在其他设备上使用。</li>
</ul>
<p><strong>加载离线镜像</strong>：</p>
<ul>
<li>使用 <code>docker load -i &lt;文件名&gt;.tar</code> 载入离线镜像，方便在新的环境中快速搭建所需环境。</li>
</ul>
<h3 id="二、数据收集与清洗"><a href="#二、数据收集与清洗" class="headerlink" title="二、数据收集与清洗"></a><strong>二、数据收集与清洗</strong></h3><h4 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a><strong>2.1 数据准备</strong></h4><ul>
<li><strong>原料</strong>：原始文本数据是进行数据清洗与处理的基础，来源包括各种大型语料库和数据集。</li>
<li><strong>工具</strong>：Dolma 是一个高性能的数据清洗工具包，提供了完整的数据处理与清洗流程。</li>
<li><strong>步骤</strong>：<ol>
<li><strong>收集数据</strong>：从不同来源收集原始数据，例如 Common Crawl、The Stack、Wikipedia、Wikibooks 等。</li>
<li><strong>数据处理与清洗</strong>：使用 Dolma 工具对数据进行标记、去重、筛选与混合等处理。</li>
<li><strong>词元切分</strong>：将清洗后的文本进行分词，生成可供模型训练的 tokens。</li>
</ol>
</li>
<li><strong>结果</strong>：完成数据清洗与处理后，最终获得可供模型训练使用的 tokens，规模约在 100B 左右。</li>
</ul>
<h4 id="2-2-Dolma的特点"><a href="#2-2-Dolma的特点" class="headerlink" title="2.2 Dolma的特点"></a><strong>2.2 Dolma的特点</strong></h4><p>Dolma 是一款强大且灵活的数据处理工具包，具备以下主要特点：</p>
<ul>
<li><strong>高性能</strong>：能够并行处理包含数十亿个文档的数据集，支持高效的数据清洗与预处理。</li>
<li><strong>可移植性</strong>：Dolma 可在单机、集群或云计算环境中运行，灵活性强。</li>
<li><strong>内置标记器</strong>：内置多种标记器，包括语言检测、毒性检测、困惑度评分等，提供对文本的多维度分析和过滤。</li>
<li><strong>快速重复数据删除</strong>：采用 Rust 实现的 Bloom 过滤器，能够高效去除数据集中的重复文档，比其他方法快得多。</li>
<li><strong>可扩展</strong>：Dolma 设计为可扩展，可以根据需要使用自定义标记器。</li>
<li><strong>云支持</strong>：Dolma 支持从本地磁盘和 AWS S3 兼容位置读取和写入数据，便于云端与本地数据的交互。</li>
</ul>
<h4 id="2-3-Dolma数据处理流程"><a href="#2-3-Dolma数据处理流程" class="headerlink" title="2.3 Dolma数据处理流程"></a><strong>2.3 Dolma数据处理流程</strong></h4><p>使用 Dolma 进行数据集管理通常分为四个步骤：</p>
<ol>
<li><strong>标记数据</strong>：使用内置标记器对数据集中的文档进行标记，例如标记文档的语言、毒性等属性。</li>
<li><strong>重复数据删除</strong>：根据文档内容或元数据对文档进行去重。</li>
<li><strong>混合数据</strong>：根据文档的属性值，删除或过滤不符合要求的文档，并按比例混合不同来源的数据。</li>
<li><strong>词元切分</strong>：使用与 HuggingFace 兼容的 Tokenizer 对文档进行分词处理，生成可供训练使用的 tokens。</li>
</ol>
<h4 id="2-4-数据处理“菜谱”"><a href="#2-4-数据处理“菜谱”" class="headerlink" title="2.4 数据处理“菜谱”"></a><strong>2.4 数据处理“菜谱”</strong></h4><p>Dolma 工具包针对不同数据类型和数据来源提供了特定的数据处理“菜谱”：</p>
<ul>
<li><p><strong>网页（CC or C4）</strong>：</p>
<ul>
<li><strong>语种</strong>：先检测语言，确保是目标语种。</li>
<li><strong>去重</strong>：两次去重处理，包括段落去重和 URL 去重。</li>
<li><strong>质量过滤</strong>：过滤掉没有终止符的文本、Gopher 过滤规则、重复 token 超过 100 次的文档。</li>
<li><strong>内容过滤</strong>：检测并过滤掉毒性内容和 PII（Personal Identifiable Information）信息。</li>
</ul>
</li>
<li><p><strong>论坛（Reddit）</strong>：</p>
<ul>
<li><strong>语种检测</strong>：确保文本属于目标语言。</li>
<li><strong>质量过滤</strong>：过滤掉评论过短、点赞数少的内容。</li>
<li><strong>内容过滤</strong>：检测并去除包含被禁止、毒性、NSFW 内容的评论。</li>
</ul>
</li>
<li><p><strong>论文（Semantic Scholar）</strong>：</p>
<ul>
<li><strong>语种检测</strong>：确保内容是目标语言。</li>
<li><strong>质量过滤</strong>：过滤掉含有重复 token 超过 100 次的文档。</li>
</ul>
</li>
<li><p><strong>代码（The Stack）</strong>：</p>
<ul>
<li><strong>编程语种</strong>：根据需要筛选特定的编程语言。</li>
<li><strong>质量过滤</strong>：移除序言部分的版权声明，遵循 RedPajama v1 和 Starcoder 过滤规则。</li>
<li><strong>内容过滤</strong>：检测 PII 信息（如邮箱、电话、IP 地址），若文档中包含 6 个以上 PII 信息，移除整个语料。</li>
</ul>
</li>
<li><p><strong>维基（Wikipedia &amp; Wikibooks）</strong>：</p>
<ul>
<li>移除少于 25 个 utf-8 字符的网页。</li>
</ul>
</li>
<li><p><strong>书籍（Gutenberg）</strong>：</p>
<ul>
<li>对每段内容进行语言分类，如果整本书的平均语言得分低于 0.5，则移除。</li>
<li>移除少于 25 个 utf-8 字符的网页，并过滤掉重复 token 超过 100 次的内容。</li>
</ul>
</li>
</ul>
<h4 id="2-5-数据格式"><a href="#2-5-数据格式" class="headerlink" title="2.5 数据格式"></a><strong>2.5 数据格式</strong></h4><p>Dolma 对数据格式有严格要求，以确保数据在处理和训练中具有一致性：</p>
<ol>
<li><p><strong>id字段</strong>：该字段非常重要，用于追溯每个版本的每个文档到原始源文档，确保文档在不同版本中保持唯一性。id 在同一数据源内必须唯一。</p>
</li>
<li><p><strong>metadata字段</strong>：包含任何特定于源的信息，例如代码许可证、论文标识符（DOI、arXiv ID 等）。应尽可能保留源特定标识符，以确保数据的完整性和可追溯性。</p>
</li>
</ol>
<p>Dolma 的标准格式如下，数据清洗前应符合以下结构：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"id"</span><span class="punctuation">:</span> <span class="string">"12345"</span><span class="punctuation">,</span>           <span class="comment">// 必填：来源特定的唯一标识符，用于标记该文档</span></span><br><span class="line">    <span class="attr">"text"</span><span class="punctuation">:</span> <span class="string">"示例文本内容"</span><span class="punctuation">,</span>  <span class="comment">// 必填：文档的文本内容</span></span><br><span class="line">    <span class="attr">"source"</span><span class="punctuation">:</span> <span class="string">"common-crawl"</span><span class="punctuation">,</span> <span class="comment">// 必填：数据来源，例如 peS2o、common-crawl 等</span></span><br><span class="line">    <span class="attr">"added"</span><span class="punctuation">:</span> <span class="string">"2024-09-23T12:34:56Z"</span><span class="punctuation">,</span> <span class="comment">// 可选：数据被 AI2 获取的时间戳</span></span><br><span class="line">    <span class="attr">"created"</span><span class="punctuation">:</span> <span class="string">"2024-01-01T08:00:00Z"</span><span class="punctuation">,</span> <span class="comment">// 可选：原始文档创建的时间戳（如不可用，最好做出合理的推断）</span></span><br><span class="line">    <span class="attr">"metadata"</span><span class="punctuation">:</span> <span class="punctuation">{</span>            <span class="comment">// 可选：与数据源相关的特定元数据信息</span></span><br><span class="line">        <span class="attr">"author"</span><span class="punctuation">:</span> <span class="string">"张三"</span><span class="punctuation">,</span>    <span class="comment">// 示例：作者信息</span></span><br><span class="line">        <span class="attr">"keywords"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">"示例"</span><span class="punctuation">,</span> <span class="string">"文本"</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="comment">// 示例：关键词</span></span><br><span class="line">        <span class="attr">"license"</span><span class="punctuation">:</span> <span class="string">"CC BY-SA"</span> <span class="comment">// 示例：数据的许可信息</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure></p>
<p>Dolma流程格式：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"source"</span><span class="punctuation">:</span> <span class="string">"..."</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"id"</span><span class="punctuation">:</span> <span class="string">"..."</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"attributes"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">      <span class="attr">"toxicity"</span><span class="punctuation">:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure></p>
<h4 id="2-6-数据处理与过滤命令"><a href="#2-6-数据处理与过滤命令" class="headerlink" title="2.6 数据处理与过滤命令"></a><strong>2.6 数据处理与过滤命令</strong></h4><p>在实际数据清洗中，Dolma 提供了一系列的命令用于处理和过滤数据【40】：</p>
<ol>
<li><strong>dolma tag</strong>：使用标记器（taggers）对文档进行标记，例如标记语言、质量等属性。</li>
<li><strong>dolma dedupe</strong>：创建 Bloom 过滤器索引文件，对文本进行去重处理。</li>
<li><strong>dolma mix</strong>：根据标签筛选并按比例混合训练数据。</li>
<li><strong>dolma tokens</strong>：使用 Huggingface Tokenizer 对文档进行分词处理。</li>
</ol>
<h4 id="2-7-词元切分（Tokenization）"><a href="#2-7-词元切分（Tokenization）" class="headerlink" title="2.7 词元切分（Tokenization）"></a><strong>2.7 词元切分（Tokenization）</strong></h4><ul>
<li><strong>原料准备</strong>：训练语料一般为 JSONL 格式文件，使用 gz 等形式进行压缩。</li>
<li><strong>Tokenizer</strong>：可重用已有的 tokenizer，或者使用 BPE、SentencePiece 等重新训练一个新的 tokenizer。</li>
<li><strong>输出</strong>：以 npy 格式保存生成的 tokens。</li>
</ul>
<p><strong>词元切分的流程与问题解决</strong>：</p>
<ul>
<li><strong>中文支持</strong>：Dolma 默认的 Tokenizer 对中文支持较差，建议选择其他支持中文的 Tokenizer，如 Qwen2，或自行训练一个。</li>
<li><strong>数据格式问题</strong>：当使用新词表且词表数量超过 65536 时，可能会造成 id 溢出，需要手动修改 numpy 保存和训练的 dtype 为 uint32。存储空间会因此扩大一倍。</li>
<li><strong>特殊 Tokens</strong>：确保 Tokenizer 设置、Tokenize 过程和训练设置的 End of sentence (EOS) 等特殊 tokens 保持一致。</li>
<li><strong>建议</strong>：处理完成后，查看生成的内容，确保能够正常解码。</li>
</ul>
<h3 id="三、大模型预训练与OLMo框架"><a href="#三、大模型预训练与OLMo框架" class="headerlink" title="三、大模型预训练与OLMo框架"></a><strong>三、大模型预训练与OLMo框架</strong></h3><h4 id="3-1-OLMo简介"><a href="#3-1-OLMo简介" class="headerlink" title="3.1 OLMo简介"></a><strong>3.1 OLMo简介</strong></h4><ul>
<li><strong>OLMo</strong> 是由 AI2（Allen Institute for AI）开源的一个大型语言模型（LLM），全称为 <strong>Open Language Model</strong>。</li>
<li><strong>完全开源</strong>：OLMo 不仅开源了模型的权重，还开源了整个训练过程中的数据、训练代码和评估代码，方便用户了解和复现模型训练的全过程。</li>
<li><strong>集成性好</strong>：只需要具备 GPU 资源，任何用户都可以利用 OLMo 从零开始快速训练自己的 LLM，这极大地降低了大模型预训练的门槛，方便研究人员和开发者进行探索和实践。</li>
</ul>
<h4 id="3-2-OLMo安装"><a href="#3-2-OLMo安装" class="headerlink" title="3.2 OLMo安装"></a><strong>3.2 OLMo安装</strong></h4><p>首先根据您的操作系统的指示安装 <a target="_blank" rel="noopener" href="https://pytorch.org">PyTorch</a>。</p>
<p>如果希望从源代码进行安装，请运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/allenai/OLMo.git</span><br><span class="line"><span class="built_in">cd</span> OLMo</span><br><span class="line">pip install -e .[all]</span><br></pre></td></tr></table></figure>
<p>否则，您可以直接从 PyPI 安装模型代码，使用以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install ai2-olmo</span><br></pre></td></tr></table></figure>
<h4 id="3-3-OLMo预训练"><a href="#3-3-OLMo预训练" class="headerlink" title="3.3 OLMo预训练"></a><strong>3.3 OLMo预训练</strong></h4><p>用于训练官方 OLMo 模型的配置文件可以在 <a target="_blank" rel="noopener" href="https://github.com/allenai/OLMo/blob/main/configs/official"><code>configs/official/</code></a> 目录中找到。</p>
<p>在更新配置文件中的数据路径后，您可以使用 <code>torchrun</code> 启动训练。</p>
<p>例如，要在单个 8x GPU 节点上启动 1B 模型的训练，请运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml</span><br></pre></td></tr></table></figure>
<p>要从检查点恢复训练，可以将路径（本地或 URL）传递给 <code>scripts/train.py</code>，并使用 <code>--load_path</code> 参数。</p>
<p>例如，要从 OLMo 1B 运行的第 1000 步恢复训练：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml --load_path=https</span><br><span class="line"></span><br><span class="line">://olmo-checkpoints.org/ai2-llm/olmo-small/w1r5xfzt/step1000-unsharded</span><br></pre></td></tr></table></figure>
<h4 id="3-3-推理"><a href="#3-3-推理" class="headerlink" title="3.3 推理"></a><strong>3.3 推理</strong></h4><p>您可以使用 Hugging Face 来对 OLMo Transformers 检查点运行推理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">olmo = AutoModelForCausalLM.from_pretrained(<span class="string">"allenai/OLMo-7B-0724-hf"</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"allenai/OLMo-7B-0724-hf"</span>)</span><br><span class="line"></span><br><span class="line">message = [<span class="string">"Language modeling is "</span>]</span><br><span class="line">inputs = tokenizer(message, return_tensors=<span class="string">'pt'</span>, return_token_type_ids=<span class="literal">False</span>)</span><br><span class="line">response = olmo.generate(**inputs, max_new_tokens=<span class="number">100</span>, do_sample=<span class="literal">True</span>, top_k=<span class="number">50</span>, top_p=<span class="number">0.95</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.batch_decode(response, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>或者，使用 Hugging Face 的 pipeline ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line">olmo_pipe = pipeline(<span class="string">"text-generation"</span>, model=<span class="string">"allenai/OLMo-7B-0724-hf"</span>)</span><br><span class="line"><span class="built_in">print</span>(olmo_pipe(<span class="string">"Language modeling is"</span>))</span><br></pre></td></tr></table></figure>
<h3 id="四、指令微调与LLaMA-Factory框架"><a href="#四、指令微调与LLaMA-Factory框架" class="headerlink" title="四、指令微调与LLaMA-Factory框架"></a><strong>四、指令微调与LLaMA-Factory框架</strong></h3><h4 id="4-1-指令微调简介"><a href="#4-1-指令微调简介" class="headerlink" title="4.1 指令微调简介"></a><strong>4.1 指令微调简介</strong></h4><p><strong>指令微调（Instruction Tuning）</strong> 是对预训练后的大语言模型（LLM）进行参数微调的过程，也被称为<strong>有监督微调（Supervised Fine-tuning, SFT）</strong>。在指令微调的过程中，首先需要收集或构建包含指令信息的训练实例，然后通过有监督的方式对模型的参数进行微调。经过指令微调后的大语言模型具备较强的指令遵循能力，能够通过零样本学习（Zero-Shot Learning）的方式解决多种下游任务。</p>
<h4 id="4-2-训练流程"><a href="#4-2-训练流程" class="headerlink" title="4.2 训练流程"></a><strong>4.2 训练流程</strong></h4><ol>
<li><strong>选择基座模型</strong>：选择已预训练的基座模型，或者使用自己预训练的模型。</li>
<li><strong>构建高质量数据集</strong>：根据微调策略构建高质量的指令数据，注意数据质量与多样性，以及不同领域数据的比例。</li>
<li><strong>训练模型</strong>：根据硬件资源和训练目标，配置训练超参数、数据配置等。</li>
<li><strong>监控与评估</strong>：通过 LLaMA Board 等工具实时监控训练过程，观察 loss 曲线，调整训练策略。</li>
<li><strong>评测与推理</strong>：对微调后的模型进行评测，验证其在特定任务上的性能。<h4 id="4-3-指令数据的构建"><a href="#4-3-指令数据的构建" class="headerlink" title="4.3 指令数据的构建"></a><strong>4.3 指令数据的构建</strong></h4></li>
</ol>
<p><strong>指令数据的组成</strong>：</p>
<ul>
<li><strong>任务描述（指令）</strong>：描述模型需要完成的任务，例如回答问题、生成代码等。</li>
<li><strong>任务输入</strong>：提供给模型的任务输入数据，模型需要根据输入生成对应的输出。</li>
<li><strong>任务输出</strong>：预期的任务结果或答案。</li>
<li><strong>历史记录</strong>：在多轮对话中，历史记录用来保持上下文信息，帮助模型理解对话的连续性。</li>
</ul>
<p>在实际应用中，指令微调数据集可以包含<strong>单轮对话</strong>（无历史记录）和<strong>多轮对话</strong>（包含历史记录）。</p>
<p><strong>指令数据集示例</strong>：</p>
<ul>
<li><strong>OpenHermes</strong>、<strong>Alpaca</strong>、<strong>Codeforces-Python-Submissions-SFT</strong> 等数据集，涵盖了通用领域、代码生成、数学推理等不同类型的数据。推荐的数据集比例为通用数据与领域数据保持平衡，例如 5:1。</li>
</ul>
<h4 id="4-4-选择-SFT-训练框架：LLaMA-Factory"><a href="#4-4-选择-SFT-训练框架：LLaMA-Factory" class="headerlink" title="4.4 选择 SFT 训练框架：LLaMA-Factory"></a><strong>4.4 选择 SFT 训练框架：LLaMA-Factory</strong></h4><p><strong>LLaMA-Factory</strong> 是一个统一、高效的微调框架，集成了多种高效训练方法，支持对 100 多种大型语言模型（LLMs）的灵活微调。该框架的特点和优势包括：</p>
<ul>
<li><strong>易于使用</strong>：通过内置的 Web 用户界面 LlamaBoard，无需编码即可灵活地定制模型微调过程。</li>
<li><strong>多模型支持</strong>：支持多种模型，包括 LLaMA、LLaVA、Mistral 等，且提供多种训练方法，如继续预训练、多模态监督微调、奖励建模、PPO、DPO、KTO、ORPO 等。</li>
<li><strong>高效性</strong>：支持全参数微调（Full Fine Tuning, FFT）、参数高效微调（PEFT），以及多种量化技术和参数效率算法，如 LoRA（Low-Rank Adaptation）、Q-LoRA、FlashAttention-2 等，减少 GPU 内存使用并提高训练速度。</li>
<li><strong>实验监控</strong>：集成了 LlamaBoard、TensorBoard、Wandb、MLflow 等工具，方便跟踪训练进度和性能。</li>
</ul>
<h4 id="4-5-微调过程与配置"><a href="#4-5-微调过程与配置" class="headerlink" title="4.5 微调过程与配置"></a><strong>4.5 微调过程与配置</strong></h4><p><strong>数据集注册</strong>：</p>
<ul>
<li>将数据集放在 <code>data</code> 文件夹下，并在 <code>data_info.json</code> 中注册数据集的信息，包括数据集名称、地址和格式。</li>
</ul>
<p><strong>训练配置示例</strong>：</p>
<ul>
<li>以下是对模型进行微调的配置文件示例：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model</span></span><br><span class="line"><span class="attr">model_name_or_path:</span> <span class="string">meta-llama/Meta-Llama-3-8B-Instruct</span></span><br><span class="line"><span class="comment"># method</span></span><br><span class="line"><span class="attr">stage:</span> <span class="string">sft</span></span><br><span class="line"><span class="attr">do_train:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">finetuning_type:</span> <span class="string">full</span></span><br><span class="line"><span class="comment"># ddp</span></span><br><span class="line"><span class="attr">ddp_timeout:</span> <span class="number">180000000</span></span><br><span class="line"><span class="attr">deepspeed:</span> <span class="string">examples/deepspeed/ds_z3_config.json</span></span><br><span class="line"><span class="comment"># dataset</span></span><br><span class="line"><span class="attr">dataset:</span> <span class="string">identity,alpaca_gpt4_en</span></span><br><span class="line"><span class="attr">template:</span> <span class="string">llama3</span></span><br><span class="line"><span class="attr">cutoff_len:</span> <span class="number">1024</span></span><br><span class="line"><span class="attr">max_samples:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">val_size:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">overwrite_cache:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">preprocessing_num_workers:</span> <span class="number">16</span></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="attr">output_dir:</span> <span class="string">saves/llama3-8b/full/sft</span></span><br><span class="line"><span class="attr">logging_steps:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">save_steps:</span> <span class="number">500</span></span><br><span class="line"><span class="attr">plot_loss:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">overwrite_output_dir:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>其中包含了模型名称、训练方法、数据集名称、数据预处理进程数、输出路径等信息。该配置文件可以灵活调整，以适应不同的训练需求。</li>
</ul>
<p><strong>训练与评估设置</strong>：</p>
<ul>
<li>单卡 batch size、梯度累积步数、学习率、训练 epoch、精度（如 FP16、BF16）等参数均可根据硬件资源进行调整。</li>
</ul>
<h4 id="4-6-训练策略"><a href="#4-6-训练策略" class="headerlink" title="4.6 训练策略"></a><strong>4.6 训练策略</strong></h4><p><strong>全量微调（Full Fine Tuning, FFT）</strong>：</p>
<ul>
<li>对全量参数进行训练，具有更高的训练成本，但能获得更全面的模型能力。然而，FFT 可能导致“灾难性遗忘”（Catastrophic Forgetting）现象，即在特定领域数据上微调后，模型在其他领域的表现可能下降。</li>
</ul>
<p><strong>参数高效微调（Parameter-Efficient Fine Tuning, PEFT）</strong>：</p>
<ul>
<li>仅对部分参数进行训练，常用的技术是 LoRA（Low-Rank Adaptation），通过添加旁路线性层 A 和 B，模型只需训练降维和升维的矩阵，实现高效微调。PEFT 能够以较低的训练成本提高模型在特定任务上的性能。</li>
</ul>
<h4 id="4-7-微调实例与训练过程"><a href="#4-7-微调实例与训练过程" class="headerlink" title="4.7 微调实例与训练过程"></a><strong>4.7 微调实例与训练过程</strong></h4><p><strong>训练实例</strong>：</p>
<ul>
<li>通过以下命令对 Llama3-8B-Instruct 模型进行微调、推理和合并操作：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli chat examples/inference/llama3_lora_sft.yaml</span><br><span class="line">llamafactory-cli <span class="built_in">export</span> examples/merge_lora/llama3_lora_sft.yaml</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>可视化训练过程</strong>：</p>
<ul>
<li>使用 LLaMA Board（由 Gradio 驱动）实现训练过程的可视化，方便监控训练的实时状态：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="五、模型评测与OpenCompass"><a href="#五、模型评测与OpenCompass" class="headerlink" title="五、模型评测与OpenCompass"></a><strong>五、模型评测与OpenCompass</strong></h3><h4 id="5-1-OpenCompass简介"><a href="#5-1-OpenCompass简介" class="headerlink" title="5.1 OpenCompass简介"></a><strong>5.1 OpenCompass简介</strong></h4><p><strong>OpenCompass（司南）</strong> 是由上海人工智能实验室发布的一个开源大模型评测体系，旨在为大模型提供一个公平、开放和可复制的评估基准，已经成为目前权威的大型模型评估平台。OpenCompass 支持丰富的模型和数据集，提供分布式高效评测，具备多样化的评估范式、模块化设计和较强的可扩展性，同时具备实验管理和报告生成机制。</p>
<ul>
<li><strong>官方链接</strong>：<ul>
<li>GitHub 项目地址: <a target="_blank" rel="noopener" href="https://github.com/open-compass/opencompass">https://github.com/open-compass/opencompass</a></li>
<li>排行榜网站: <a target="_blank" rel="noopener" href="https://rank.opencompass.org.cn/home">https://rank.opencompass.org.cn/home</a></li>
</ul>
</li>
</ul>
<h4 id="5-2-OpenCompass安装"><a href="#5-2-OpenCompass安装" class="headerlink" title="5.2 OpenCompass安装"></a><strong>5.2 OpenCompass安装</strong></h4><p>安装 OpenCompass 的步骤如下：</p>
<ol>
<li><strong>创建 Conda 虚拟环境</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y</span><br><span class="line">conda activate opencompass</span><br></pre></td></tr></table></figure></li>
<li><strong>克隆 OpenCompass 项目源码</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/open-compass/opencompass opencompass</span><br><span class="line"><span class="built_in">cd</span> opencompass</span><br></pre></td></tr></table></figure></li>
<li><strong>安装依赖</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r ./requirements/runtime.txt</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure></li>
<li><strong>下载测评数据</strong>：使用以下命令下载官方测评数据，并解压到 <code>opencompass</code> 目录下的 <code>data</code> 目录：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip</span><br><span class="line">unzip OpenCompassData-core-20231110.zip</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="5-3-OpenCompass整体流程"><a href="#5-3-OpenCompass整体流程" class="headerlink" title="5.3 OpenCompass整体流程"></a><strong>5.3 OpenCompass整体流程</strong></h4><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：</p>
<ul>
<li><strong>配置</strong>：配置评估过程，包括选择模型、数据集、评估策略、计算后端等，还可以定义结果的显示方式。</li>
<li><strong>推理与评估</strong>：推理阶段让模型从数据集产生输出，评估阶段则衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率。</li>
<li><strong>可视化</strong>：评估完成后，OpenCompass 会将结果整理成易读的表格，并保存为 CSV 和 TXT 文件，还可以在飞书客户端中及时获得评测状态报告。</li>
</ul>
<h4 id="5-4-OpenCompass配置文件"><a href="#5-4-OpenCompass配置文件" class="headerlink" title="5.4 OpenCompass配置文件"></a><strong>5.4 OpenCompass配置文件</strong></h4><ul>
<li><strong>配置文件命名</strong>：一般命名为 <code>eval_xxx.py</code>。</li>
<li><strong>主要配置项</strong>：<ul>
<li><strong>datasets</strong>：指定使用的数据集列表。</li>
<li><strong>models</strong>：指定使用的模型、分词器和推理方法（如 ppl、gen）。</li>
<li><strong>infer</strong>：指定测评任务的分割方式（如 <code>SizePartitioner</code>、<code>NaivePartitioner</code>）。</li>
<li><strong>meta_template</strong>：需要与 SFT 阶段使用的模板保持一致。</li>
</ul>
</li>
</ul>
<p>如使用 vllm 版本，可使用 <code>gpu_memory_utilization</code> 等额外参数，若发生显存不足则可调整相关参数。</p>
<h4 id="5-5-OpenCompass模型准备"><a href="#5-5-OpenCompass模型准备" class="headerlink" title="5.5 OpenCompass模型准备"></a><strong>5.5 OpenCompass模型准备</strong></h4><p>在进行模型评测之前，首先需要将模型转化为适合评测的格式：</p>
<ul>
<li><strong>分布式训练结果转换</strong>：将分布式训练的结果转换为 unsharded 形式：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python OLMo/scripts/unshard.py &lt;sharded_checkpoint_dir&gt; &lt;unsharded_checkpoint_dir&gt;</span><br></pre></td></tr></table></figure></li>
<li><strong>将检查点转换为 HuggingFace 格式</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python /OLMo/scripts/convert_olmo_to_hf_new.py --input_dir=&lt;unsharded_checkpoint_dir&gt; --output_dir=&lt;hf_checkpoint_dir&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>如果使用 <code>llama-factory</code> 或 <code>trl</code> 等包进行模型训练/微调，则无需转换。使用 OLMo 训练的模型在进行 SFT 或评测前均需完成 unsharded 格式转换与 HuggingFace 格式转换。</p>
<h4 id="5-6-OpenCompass运行与输出"><a href="#5-6-OpenCompass运行与输出" class="headerlink" title="5.6 OpenCompass运行与输出"></a><strong>5.6 OpenCompass运行与输出</strong></h4><p>在 <code>opencompass</code> 目录下，运行以下命令启动评测：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python run.py ./configs/eval_olmo.py &gt; nohup.out &amp;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>运行过程中，会在 <code>&lt;workdir&gt;</code> 目录下生成一个以时间戳命名的文件夹，其中包含 <code>configs</code>、<code>logs</code>、<code>predictions</code>、<code>results</code>、<code>summary</code> 等文件夹。</li>
<li>如果在评测过程中遇到出错的项目，OpenCompass 会自动跳过；若错误过多，可查看对应的 <code>log</code> 文件进行排查。</li>
<li>评测完成后，可以在 <code>summary</code> 文件夹中找到每个测评数据集的得分（以 TXT 和 CSV 两种形式记录）。</li>
</ul>
<p><strong>停止进程</strong>：如需终止评测进程，可执行以下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -elf | grep opencompass | awk <span class="string">'{print $4}'</span> | xargs <span class="built_in">kill</span></span><br></pre></td></tr></table></figure></p>
<h4 id="5-7-OpenCompass评测数据集"><a href="#5-7-OpenCompass评测数据集" class="headerlink" title="5.7 OpenCompass评测数据集"></a><strong>5.7 OpenCompass评测数据集</strong></h4><ul>
<li><strong>MMLU（Multilingual Massively Multilingual Understanding）</strong>：多选问答任务的英文评测数据集，涵盖 STEM、人文、社会科学等领域的 57 个学科，共 15908 个问题，评估模型的多任务处理能力。</li>
<li><strong>CMMLU</strong>：综合性的中文评估基准，专门用于评估模型在中文语境下的知识和推理能力，涵盖 67 个主题，共 11528 个问题，包含少样本和测试集部分。</li>
<li><strong>C-Eval</strong>：中文 LLM 评估基准，包含 13948 个多项选择问题，涉及 52 个学科，分为初中、高中、大学和专业四个难度级别，并设有 C-Eval Hard 子集用于高级推理能力的评估。</li>
<li><strong>GSM8K</strong>：中小学数学应用题的英文数据集，包含 8.5K 高质量数学问题，考察模型在多步骤数学推理任务中的性能。</li>
<li><strong>MATH</strong>：数学竞赛问题数据集，包含 12500 道题目，提供了完整的分步骤解答方案，评估模型在数学推理和解题方面的能力。</li>
<li><strong>MBPP（Mostly Basic Python Problems）</strong>：由约 1401 个 Python 编程问题组成，旨在评估模型的编程基础和标准库功能。</li>
<li><strong>HumanEval</strong>：由 OpenAI 发布的编程问题数据集，包含 164 个编程任务，评估模型在代码生成和编写方面的能力。</li>
<li><strong>BBH（Big Bench Hard）</strong>：评估模型在高难度推理任务方面的性能，共 23 个具有挑战性的 BIG-Bench 任务，每类约 250 个测试案例。</li>
<li><strong>GAOKAO-BENCH</strong>：利用中国高考试题作为数据集，评估模型的语言理解和逻辑推理能力。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/" data-id="cm5ar5vhi000kciwi727veact" data-title="Olmo及dolma实践" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-DeepSpeed相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:23:53.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/">DeepSpeed相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Accelerate</strong> 集成了 <strong>DeepSpeed ZeRO</strong> 的所有功能。这包括 <strong>ZeRO</strong> 的第 1、2 和 3 阶段，以及 <strong>ZeRO-Offload</strong>、<strong>ZeRO-Infinity</strong>（可以卸载到 Disk/NVMe）和 <strong>ZeRO++</strong>。</p>
<p>Huggingface中的 <strong>Accelerate</strong> 通过两种选项集成了 DeepSpeed：</p>
<ol>
<li><strong>通过 deepspeed 配置文件</strong></li>
<li><strong>通过 deepspeed_plugin</strong></li>
</ol>
</blockquote>
<hr>
<h2 id="如何通过Accelerate使用DeepSpeed"><a href="#如何通过Accelerate使用DeepSpeed" class="headerlink" title="如何通过Accelerate使用DeepSpeed"></a>如何通过Accelerate使用DeepSpeed</h2><h3 id="1-通过-deepspeed-plugin"><a href="#1-通过-deepspeed-plugin" class="headerlink" title="1. 通过 deepspeed_plugin"></a>1. 通过 deepspeed_plugin</h3><p>在您的机器上运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate config</span><br></pre></td></tr></table></figure>
<p>并回答所提出的问题。它会询问您是否要为 <strong>DeepSpeed</strong> 使用配置文件，您应该回答否。然后回答以下问题以生成一个基本的 <strong>DeepSpeed</strong> 配置。这将生成一个配置文件，在执行以下命令时将自动使用该配置文件来正确设置默认选项：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch my_script.py --args_to_my_script</span><br></pre></td></tr></table></figure>
<p>例如，这里是如何使用 <strong>DeepSpeed Plugin</strong> 运行 NLP 示例 <code>examples/nlp_example.py</code>（从仓库根目录）：</p>
<h4 id="ZeRO-Stage-2-DeepSpeed-Plugin-示例"><a href="#ZeRO-Stage-2-DeepSpeed-Plugin-示例" class="headerlink" title="ZeRO Stage-2 DeepSpeed Plugin 示例"></a>ZeRO Stage-2 DeepSpeed Plugin 示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">none</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">none</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/nlp_example.py --mixed_precision fp16</span><br></pre></td></tr></table></figure>
<h4 id="ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-Plugin-示例"><a href="#ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-Plugin-示例" class="headerlink" title="ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed Plugin 示例"></a>ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed Plugin 示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">cpu</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero3_save_16bit_model:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/nlp_example.py --mixed_precision fp16</span><br></pre></td></tr></table></figure>
<p>目前，<strong>Accelerate</strong> 通过 CLI 支持以下配置：</p>
<ul>
<li><code>zero_stage</code>: [0] 禁用， [1] 优化器状态分区， [2] 优化器+梯度状态分区， [3] 优化器+梯度+参数分区</li>
<li><code>gradient_accumulation_steps</code>: 在平均和应用梯度之前要累积的训练步骤数。</li>
<li><code>gradient_clipping</code>: 启用梯度裁剪并设定值。</li>
<li><code>offload_optimizer_device</code>: [none] 禁用优化器卸载， [cpu] 将优化器卸载到 CPU， [nvme] 将优化器卸载到 NVMe SSD。仅适用于 ZeRO &gt;= Stage-2。</li>
<li><code>offload_optimizer_nvme_path</code>: 决定将优化器状态卸载到的 NVMe 路径。如果未指定，则默认为 ‘none’。</li>
<li><code>offload_param_device</code>: [none] 禁用参数卸载， [cpu] 将参数卸载到 CPU， [nvme] 将参数卸载到 NVMe SSD。仅适用于 ZeRO Stage-3。</li>
<li><code>offload_param_nvme_path</code>: 决定将参数卸载到的 NVMe 路径。如果未指定，则默认为 ‘none’。</li>
<li><code>zero3_init_flag</code>: 决定是否启用 <code>deepspeed.zero.Init</code> 来构建大规模模型。仅适用于 ZeRO Stage-3。</li>
<li><code>zero3_save_16bit_model</code>: 决定在使用 ZeRO Stage-3 时是否保存 16 位模型权重。</li>
<li><code>mixed_precision</code>: <code>no</code> 表示 FP32 训练， <code>fp16</code> 表示 FP16 混合精度训练， <code>bf16</code> 表示 BF16 混合精度训练。</li>
<li><code>deepspeed_moe_layer_cls_names</code>: 逗号分隔的 transformer Mixture-of-Experts (MoE) 层类名列表（区分大小写），例如 <code>MixtralSparseMoeBlock</code>, <code>Qwen2MoeSparseMoeBlock</code>, <code>JetMoEAttention,JetMoEBlock</code> …</li>
<li><code>deepspeed_hostfile</code>: 用于配置多节点计算资源的 <strong>DeepSpeed</strong> hostfile。</li>
<li><code>deepspeed_exclusion_filter</code>: 使用多节点设置时的 <strong>DeepSpeed</strong> 排除过滤字符串。</li>
<li><code>deepspeed_inclusion_filter</code>: 使用多节点设置时的 <strong>DeepSpeed</strong> 包含过滤字符串。</li>
<li><code>deepspeed_multinode_launcher</code>: 要使用的 <strong>DeepSpeed</strong> 多节点启动器。如果未指定，则默认为 <code>pdsh</code>。</li>
<li><code>deepspeed_config_file</code>: <strong>DeepSpeed</strong> 配置文件的路径，格式为 <code>json</code>。有关更多详细信息，请参见下一节。</li>
</ul>
<p>要调整更多选项，您需要使用 <strong>DeepSpeed</strong> 配置文件。</p>
<h3 id="2-DeepSpeed-配置文件"><a href="#2-DeepSpeed-配置文件" class="headerlink" title="2. DeepSpeed 配置文件"></a>2. DeepSpeed 配置文件</h3><p>在您的机器上运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate config</span><br></pre></td></tr></table></figure>
<p>并回答所提出的问题。它会询问您是否要为 <strong>DeepSpeed</strong> 使用配置文件，您应该回答是并提供 <strong>DeepSpeed 配置文件</strong> 的路径。这将生成一个配置文件，在执行以下命令时将自动使用该配置文件来正确设置默认选项：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch my_script.py --args_to_my_script</span><br></pre></td></tr></table></figure>
<p>例如，这里是如何使用 <strong>DeepSpeed 配置文件</strong> 运行 NLP 示例 <code>examples/by_feature/deepspeed_with_config_support.py</code>（从仓库根目录）：</p>
<h4 id="ZeRO-Stage-2-DeepSpeed-配置文件示例"><a href="#ZeRO-Stage-2-DeepSpeed-配置文件示例" class="headerlink" title="ZeRO Stage-2 DeepSpeed 配置文件示例"></a>ZeRO Stage-2 DeepSpeed 配置文件示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">/home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage2_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>zero_stage2_config.json</strong> 的内容如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"fp16"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"enabled"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"loss_scale"</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"loss_scale_window"</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"initial_scale_power"</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"hysteresis"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"min_loss_scale"</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"optimizer"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"AdamW"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"params"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"weight_decay"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"torch_adam"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"adam_w_mode"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"scheduler"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"WarmupDecayLR"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"params"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"warmup_min_lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"warmup_max_lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"warmup_num_steps"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"total_num_steps"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"zero_optimization"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"allgather_partitions"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"allgather_bucket_size"</span><span class="punctuation">:</span> <span class="number">2e8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"overlap_comm"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"reduce_scatter"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"reduce_bucket_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"contiguous_gradients"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_accumulation_steps"</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_clipping"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"steps_per_print"</span><span class="punctuation">:</span> <span class="number">2000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_batch_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_micro_batch_size_per_gpu"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"wall_clock_breakdown"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/by_feature/deepspeed_with_config_support.py \</span><br><span class="line">--config_name <span class="string">"gpt2-large"</span> \</span><br><span class="line">--tokenizer_name <span class="string">"gpt2-large"</span> \</span><br><span class="line">--dataset_name <span class="string">"wikitext"</span> \</span><br><span class="line">--dataset_config_name <span class="string">"wikitext-2-raw-v1"</span> \</span><br><span class="line">--block_size 128 \</span><br><span class="line">--output_dir <span class="string">"./clm/clm_deepspeed_stage2_accelerate"</span> \</span><br><span class="line">--learning_rate 5e-4 \</span><br><span class="line">--per_device_train_batch_size 24 \</span><br><span class="line">--per_device_eval_batch_size 24 \</span><br><span class="line">--num_train_epochs 3 \</span><br><span class="line">--with_tracking \</span><br><span class="line">--report_to <span class="string">"wandb"</span></span><br></pre></td></tr></table></figure>
<h4 id="ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-配置文件示例"><a href="#ZeRO-Stage-3-使用-CPU-Offload-的-DeepSpeed-配置文件示例" class="headerlink" title="ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed 配置文件示例"></a>ZeRO Stage-3 使用 CPU Offload 的 DeepSpeed 配置文件示例</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">/home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage3_offload_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">mixed_precision:</span> <span class="string">fp16</span></span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>zero_stage3_offload_config.json</strong> 的内容如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"fp16"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"enabled"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"loss_scale"</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"loss_scale_window"</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"initial_scale_power"</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"hysteresis"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"min_loss_scale"</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"optimizer"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"AdamW"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"params"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"weight_decay"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"scheduler"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"WarmupDecayLR"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"params"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"warmup_min_lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"warmup_max_lr"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"warmup_num_steps"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"total_num_steps"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"zero_optimization"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_optimizer"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"cpu"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"pin_memory"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_param"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"cpu"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"pin_memory"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"overlap_comm"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"contiguous_gradients"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"reduce_bucket_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_prefetch_bucket_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_param_persistence_threshold"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"sub_group_size"</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_max_live_parameters"</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_max_reuse_distance"</span><span class="punctuation">:</span> <span class="number">1e9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_gather_16bit_weights_on_model_save"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_accumulation_steps"</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_clipping"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"steps_per_print"</span><span class="punctuation">:</span> <span class="number">2000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_batch_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_micro_batch_size_per_gpu"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"wall_clock_breakdown"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch examples/by_feature/deepspeed_with_config_support.py \</span><br><span class="line">--config_name <span class="string">"gpt2-large"</span> \</span><br><span class="line">--tokenizer_name <span class="string">"gpt2-large"</span> \</span><br><span class="line">--dataset_name <span class="string">"wikitext"</span> \</span><br><span class="line">--dataset_config_name <span class="string">"wikitext-2-raw-v1"</span> \</span><br><span class="line">--block_size 128 \</span><br><span class="line">--output_dir <span class="string">"./clm/clm_deepspeed_stage3_offload_accelerate"</span> \</span><br><span class="line">--learning_rate 5e-4 \</span><br><span class="line">--per_device_train_batch_size 32 \</span><br><span class="line">--per_device_eval_batch_size 32 \</span><br><span class="line">--num_train_epochs 3 \</span><br><span class="line">--with_tracking \</span><br><span class="line">--report_to <span class="string">"wandb"</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>说明：</strong></p>
<ul>
<li><p><strong>ZeRO Stage-2 配置文件</strong> 示例展示了如何配置 <strong>DeepSpeed</strong> 以使用 ZeRO 的第 2 阶段优化，包括梯度累积步数、梯度裁剪、优化器类型等参数。</p>
</li>
<li><p><strong>ZeRO Stage-3 使用 CPU Offload 的配置文件</strong> 示例则进一步展示了如何将优化器状态和模型参数卸载到 CPU，以支持更大规模模型的训练。</p>
</li>
<li><p>运行命令中使用的 <code>--config_name</code>、<code>--tokenizer_name</code>、<code>--dataset_name</code> 等参数保持不变，确保 <strong>Accelerate</strong> 能正确加载和使用指定的配置文件。</p>
</li>
<li><p>通过配置文件方式，用户可以更灵活地调整 <strong>DeepSpeed</strong> 的各种参数，以适应不同的训练需求和硬件环境。</p>
</li>
</ul>
<hr>
<h2 id="ZeRO-配置示例"><a href="#ZeRO-配置示例" class="headerlink" title="ZeRO++ 配置示例"></a><strong>ZeRO++ 配置示例</strong></h2><p>您可以通过使用适当的配置参数来使用 <strong>ZeRO++</strong> 的功能。请注意，<strong>ZeRO++</strong> 是 <strong>ZeRO Stage 3</strong> 的扩展。以下是如何修改配置文件的示例，摘自 <strong>DeepSpeed</strong> 的 <strong>ZeRO++</strong> 教程：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"zero_optimization"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"reduce_bucket_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">"zero_quantized_weights"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"zero_hpz_partition_size"</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"zero_quantized_gradients"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">        <span class="attr">"contiguous_gradients"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"overlap_comm"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
<p>对于分层分区，<code>zero_hpz_partition_size</code> 的分区大小理想情况下应设置为每个节点的 GPU 数量。（例如，上述配置文件假设每个节点有 8 个 GPU）</p>
<h3 id="使用-DeepSpeed-配置文件时的重要代码更改"><a href="#使用-DeepSpeed-配置文件时的重要代码更改" class="headerlink" title="使用 DeepSpeed 配置文件时的重要代码更改"></a><strong>使用 DeepSpeed 配置文件时的重要代码更改</strong></h3><p><strong>DeepSpeed Optimizers</strong> 和 <strong>Schedulers</strong>。有关更多信息，请参阅 <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/#optimizer">DeepSpeed Optimizers</a> 和 <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/#scheduler">DeepSpeed Schedulers</a> 文档。我们将查看在使用这些时代码需要进行的更改。</p>
<h4 id="a-DS-Optim-DS-Scheduler"><a href="#a-DS-Optim-DS-Scheduler" class="headerlink" title="a. DS Optim + DS Scheduler"></a>a. <strong>DS Optim + DS Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中同时存在 <code>optimizer</code> 和 <code>scheduler</code> 键时。在这种情况下，将使用配置文件中的优化器和调度器，用户需要使用 <code>accelerate.utils.DummyOptim</code> 和 <code>accelerate.utils.DummyScheduler</code> 来替换代码中的 PyTorch/自定义优化器和调度器。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果配置文件中指定了 `optimizer`，则创建 Dummy Optimizer，否则创建 Adam Optimizer</span></span><br><span class="line">optimizer_cls = (</span><br><span class="line">    torch.optim.AdamW</span><br><span class="line">    <span class="keyword">if</span> accelerator.state.deepspeed_plugin <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">or</span> <span class="string">"optimizer"</span> <span class="keyword">not</span> <span class="keyword">in</span> accelerator.state.deepspeed_plugin.deepspeed_config</span><br><span class="line">    <span class="keyword">else</span> DummyOptim</span><br><span class="line">)</span><br><span class="line">optimizer = optimizer_cls(optimizer_grouped_parameters, lr=args.learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果配置文件中未指定 `scheduler`，则创建 `args.lr_scheduler_type` Scheduler，否则创建 Dummy Scheduler</span></span><br><span class="line"><span class="keyword">if</span> (</span><br><span class="line">    accelerator.state.deepspeed_plugin <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">or</span> <span class="string">"scheduler"</span> <span class="keyword">not</span> <span class="keyword">in</span> accelerator.state.deepspeed_plugin.deepspeed_config</span><br><span class="line">):</span><br><span class="line">    lr_scheduler = get_scheduler(</span><br><span class="line">        name=args.lr_scheduler_type,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        num_warmup_steps=args.num_warmup_steps,</span><br><span class="line">        num_training_steps=args.max_train_steps,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lr_scheduler = DummyScheduler(</span><br><span class="line">        optimizer, total_num_steps=args.max_train_steps, warmup_num_steps=args.num_warmup_steps</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h4 id="b-Custom-Optim-Custom-Scheduler"><a href="#b-Custom-Optim-Custom-Scheduler" class="headerlink" title="b. Custom Optim + Custom Scheduler"></a>b. <strong>Custom Optim + Custom Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中同时缺少 <code>optimizer</code> 和 <code>scheduler</code> 键时。在这种情况下，用户无需更改任何代码，这是使用 <strong>DeepSpeed Plugin</strong> 集成时的情况。在上述示例中，如果配置文件中缺少 <code>optimizer</code> 和 <code>scheduler</code> 键，代码将保持不变。</p>
<h4 id="c-Custom-Optim-DS-Scheduler"><a href="#c-Custom-Optim-DS-Scheduler" class="headerlink" title="c. Custom Optim + DS Scheduler"></a>c. <strong>Custom Optim + DS Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中仅存在 <code>scheduler</code> 键时。在这种情况下，用户必须使用 <code>accelerate.utils.DummyScheduler</code> 来替换代码中的 PyTorch/自定义调度器。</p>
<h4 id="d-DS-Optim-Custom-Scheduler"><a href="#d-DS-Optim-Custom-Scheduler" class="headerlink" title="d. DS Optim + Custom Scheduler"></a>d. <strong>DS Optim + Custom Scheduler</strong></h4><p>当 <strong>DeepSpeed</strong> 配置文件中仅存在 <code>optimizer</code> 键时。这将导致错误，因为只能在使用 <strong>DS Optim</strong> 时使用 <strong>DS Scheduler</strong>。</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><strong>注意事项</strong></h3><p>请注意上述示例 <strong>DeepSpeed</strong> 配置文件中的 <code>auto</code> 值。这些值由 <code>prepare</code> 方法根据模型、数据加载器、Dummy Optimizer 和 Dummy Schedulers 自动处理。仅示例中指定的 <code>auto</code> 字段由 <code>prepare</code> 方法处理，其余字段必须由用户显式指定。</p>
<p><strong>自动值的计算方式如下：</strong></p>
<ul>
<li><code>reduce_bucket_size</code>: <code>hidden_size * hidden_size</code></li>
<li><code>stage3_prefetch_bucket_size</code>: <code>int(0.9 * hidden_size * hidden_size)</code></li>
<li><code>stage3_param_persistence_threshold</code>: <code>10 * hidden_size</code></li>
</ul>
<p>为了使这 3 个配置项的自动功能正常工作，<strong>Accelerate</strong> 将使用 <code>model.config.hidden_size</code> 或 <code>max(model.config.hidden_sizes)</code> 作为 <code>hidden_size</code>。如果这两个都不可用，启动将失败，您需要手动设置这 3 个配置项。请记住，前两个配置项是通信缓冲区——它们越大，通信效率越高，但也会消耗更多的 GPU 内存，因此这是一个可调节的性能权衡。</p>
<hr>
<h3 id="使用-DeepSpeed-配置文件时需要注意的事项"><a href="#使用-DeepSpeed-配置文件时需要注意的事项" class="headerlink" title="使用 DeepSpeed 配置文件时需要注意的事项"></a><strong>使用 DeepSpeed 配置文件时需要注意的事项</strong></h3><p>以下是一个在不同场景下使用 <code>deepspeed_config_file</code> 的示例脚本。</p>
<p><strong>代码 <code>test.py</code>：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> accelerate.state <span class="keyword">import</span> AcceleratorState</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    accelerator = Accelerator()</span><br><span class="line">    accelerator.<span class="built_in">print</span>(<span class="string">f"<span class="subst">{AcceleratorState()}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h4 id="场景-1：手动修改的-accelerate-配置文件，同时包含-deepspeed-config-file-及其他条目"><a href="#场景-1：手动修改的-accelerate-配置文件，同时包含-deepspeed-config-file-及其他条目" class="headerlink" title="场景 1：手动修改的 accelerate 配置文件，同时包含 deepspeed_config_file 及其他条目"></a><strong>场景 1：手动修改的 accelerate 配置文件，同时包含 <code>deepspeed_config_file</code> 及其他条目</strong></h4><p><strong>accelerate 配置内容：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command_file:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">commands:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">gradient_accumulation_steps:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">gradient_clipping:</span> <span class="number">1.0</span></span><br><span class="line">  <span class="attr">offload_optimizer_device:</span> <span class="string">'cpu'</span></span><br><span class="line">  <span class="attr">offload_param_device:</span> <span class="string">'cpu'</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero3_save_16bit_model:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">zero_stage:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">'ds_config.json'</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">'no'</span></span><br><span class="line"><span class="attr">dynamo_backend:</span> <span class="string">'NO'</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">gpu_ids:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_process_ip:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_process_port:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">megatron_lm_config:</span> {}</span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">tpu_name:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">tpu_zone:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong><code>ds_config.json</code> 内容：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"bf16"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"enabled"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"zero_optimization"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_gather_16bit_weights_on_model_save"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_optimizer"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"none"</span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_param"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"none"</span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_clipping"</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_batch_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_micro_batch_size_per_gpu"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_accumulation_steps"</span><span class="punctuation">:</span> <span class="number">10</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"steps_per_print"</span><span class="punctuation">:</span> <span class="number">2000000</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令 <code>accelerate launch test.py</code> 的输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ValueError: When using `deepspeed_config_file`, the following accelerate config variables will be ignored:</span><br><span class="line">['gradient_accumulation_steps', 'gradient_clipping', 'zero_stage', 'offload_optimizer_device', 'offload_param_device',</span><br><span class="line">'zero3_save_16bit_model', 'mixed_precision'].</span><br><span class="line">Please specify them appropriately in the DeepSpeed config file.</span><br><span class="line">If you are using an accelerate config file, remove other config variables mentioned in the above specified list.</span><br><span class="line">The easiest method is to create a new config following the questionnaire via `accelerate config`.</span><br><span class="line">It will only ask for the necessary config variables when using `deepspeed_config_file`.</span><br></pre></td></tr></table></figure>
<h4 id="场景-2：使用错误解决方案创建新的-accelerate-配置，并检查不再抛出模糊错误"><a href="#场景-2：使用错误解决方案创建新的-accelerate-配置，并检查不再抛出模糊错误" class="headerlink" title="场景 2：使用错误解决方案创建新的 accelerate 配置，并检查不再抛出模糊错误"></a><strong>场景 2：使用错误解决方案创建新的 accelerate 配置，并检查不再抛出模糊错误</strong></h4><p><strong>运行 <code>accelerate config</code>：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ accelerate config</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">In which compute environment are you running?</span><br><span class="line">This machine</span><br><span class="line">-------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">Which type of machine are you using?</span><br><span class="line">multi-GPU</span><br><span class="line">How many different machines will you use (use more than 1 for multi-node training)? [1]: </span><br><span class="line">Do you wish to optimize your script with torch dynamo?[yes/NO]: </span><br><span class="line">Do you want to use DeepSpeed? [yes/NO]: yes</span><br><span class="line">Do you want to specify a json file to a DeepSpeed config? [yes/NO]: yes</span><br><span class="line">Please enter the path to the json DeepSpeed config file: ds_config.json</span><br><span class="line">Do you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]: yes</span><br><span class="line">How many GPU(s) should be used for distributed training? [1]:4</span><br><span class="line">accelerate configuration saved at ds_config_sample.yaml</span><br></pre></td></tr></table></figure>
<p><strong>新的 accelerate 配置内容：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">compute_environment:</span> <span class="string">LOCAL_MACHINE</span></span><br><span class="line"><span class="attr">deepspeed_config:</span></span><br><span class="line">  <span class="attr">deepspeed_config_file:</span> <span class="string">ds_config.json</span></span><br><span class="line">  <span class="attr">zero3_init_flag:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">distributed_type:</span> <span class="string">DEEPSPEED</span></span><br><span class="line"><span class="attr">downcast_bf16:</span> <span class="string">'no'</span></span><br><span class="line"><span class="attr">dynamo_backend:</span> <span class="string">'NO'</span></span><br><span class="line"><span class="attr">fsdp_config:</span> {}</span><br><span class="line"><span class="attr">machine_rank:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">main_training_function:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">megatron_lm_config:</span> {}</span><br><span class="line"><span class="attr">num_machines:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">num_processes:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">rdzv_backend:</span> <span class="string">static</span></span><br><span class="line"><span class="attr">same_network:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">use_cpu:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令 <code>accelerate launch test.py</code> 的输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Distributed environment: DEEPSPEED  Backend: nccl</span><br><span class="line">Num processes: 4</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cuda:0</span><br><span class="line">Mixed precision type: bf16</span><br><span class="line">ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': False, 'offload_optimizer': {'device': 'none'}, 'offload_param': {'device': 'none'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 10, 'steps_per_print': inf, 'fp16': {'enabled': False}}</span><br></pre></td></tr></table></figure>
<h4 id="场景-3：在-DeepSpeed-配置文件中将与-DeepSpeed-命令相关的-accelerate-launch-命令参数设置为-“auto”，并检查是否按预期工作"><a href="#场景-3：在-DeepSpeed-配置文件中将与-DeepSpeed-命令相关的-accelerate-launch-命令参数设置为-“auto”，并检查是否按预期工作" class="headerlink" title="场景 3：在 DeepSpeed 配置文件中将与 DeepSpeed 命令相关的 accelerate launch 命令参数设置为 “auto”，并检查是否按预期工作"></a><strong>场景 3：在 DeepSpeed 配置文件中将与 DeepSpeed 命令相关的 accelerate launch 命令参数设置为 “auto”，并检查是否按预期工作</strong></h4><p><strong>新的 <code>ds_config.json</code>，将与 accelerate launch DeepSpeed 命令参数设置为 “auto”：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"bf16"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"enabled"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"zero_optimization"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"stage3_gather_16bit_weights_on_model_save"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_optimizer"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"offload_param"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"device"</span><span class="punctuation">:</span> <span class="string">"auto"</span></span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">    <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_clipping"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_batch_size"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"train_micro_batch_size_per_gpu"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"gradient_accumulation_steps"</span><span class="punctuation">:</span> <span class="string">"auto"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"steps_per_print"</span><span class="punctuation">:</span> <span class="number">2000000</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
<p><strong>运行命令：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch --mixed_precision=<span class="string">"fp16"</span> --zero_stage=3 --gradient_accumulation_steps=5 --gradient_clipping=1.0 --offload_param_device=<span class="string">"cpu"</span> --offload_optimizer_device=<span class="string">"nvme"</span> --zero3_save_16bit_model=<span class="string">"true"</span> test.py</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Distributed environment: DEEPSPEED  Backend: nccl</span><br><span class="line">Num processes: 4</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cuda:0</span><br><span class="line">Mixed precision type: fp16</span><br><span class="line">ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_optimizer': {'device': 'nvme'}, 'offload_param': {'device': 'cpu'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 5, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}}</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ul>
<li>剩余的 “auto” 值在调用 <code>accelerator.prepare()</code> 时处理，如使用 DeepSpeed 配置文件时重要代码更改中的第 2 点所述。</li>
<li>仅当 <code>gradient_accumulation_steps</code> 为 auto 时，创建 <code>Accelerator</code> 对象时传递的值（如 <code>Accelerator(gradient_accumulation_steps=k)</code>）将被使用。</li>
<li>使用 DeepSpeed Plugin 时，将使用 DeepSpeed Plugin 中的值，并覆盖在创建 <code>Accelerator</code> 对象时传递的值。</li>
</ul>
<h4 id="保存和加载"><a href="#保存和加载" class="headerlink" title="保存和加载"></a><strong>保存和加载</strong></h4><ul>
<li><p><strong>ZeRO Stage-1 和 Stage-2</strong> 的模型保存和加载方式保持不变。</p>
</li>
<li><p><strong>ZeRO Stage-3</strong> 下，<code>state_dict</code> 仅包含占位符，因为模型权重被分区到多个 GPU 上。<strong>ZeRO Stage-3</strong> 有两种选项：</p>
<p>a. <strong>保存整个 16 位模型权重</strong>，以便稍后使用 <code>model.load_state_dict(torch.load(pytorch_model.bin))</code> 直接加载。为此，可以在 DeepSpeed 配置文件中设置 <code>zero_optimization.stage3_gather_16bit_weights_on_model_save</code> 为 <code>True</code>，或在 DeepSpeed Plugin 中设置 <code>zero3_save_16bit_model</code> 为 <code>True</code>。请注意，此选项需要在一个 GPU 上合并权重，可能会很慢且占用大量内存，因此仅在需要时使用。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新代码 #</span></span><br><span class="line"><span class="comment"># 如果 DeepSpeed 配置文件中的 `stage3_gather_16bit_weights_on_model_save` 为 True 或</span></span><br><span class="line"><span class="comment"># DeepSpeed Plugin 中的 `zero3_save_16bit_model` 为 True，则在 ZeRO Stage-3 中将整个/未分区的 fp16 模型保存到输出目录。</span></span><br><span class="line"><span class="comment"># 对于 ZeRO Stage 1 和 2，模型按通常方式保存在输出目录中。</span></span><br><span class="line"><span class="comment"># 保存的模型名称为 `pytorch_model.bin`</span></span><br><span class="line">unwrapped_model.save_pretrained(</span><br><span class="line">    args.output_dir,</span><br><span class="line">    is_main_process=accelerator.is_main_process,</span><br><span class="line">    save_function=accelerator.save,</span><br><span class="line">    state_dict=accelerator.get_state_dict(model),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>b. <strong>获取 32 位权重</strong>，首先使用 <code>model.save_checkpoint()</code> 保存模型。以下是来自 <code>examples/by_feature/deepspeed_with_config_support.py</code> 的代码片段：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">success = model.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)</span><br><span class="line">status_msg = <span class="string">f"checkpointing: PATH=<span class="subst">{PATH}</span>, ckpt_id=<span class="subst">{ckpt_id}</span>"</span></span><br><span class="line"><span class="keyword">if</span> success:</span><br><span class="line">    logging.info(<span class="string">f"Success <span class="subst">{status_msg}</span>"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logging.warning(<span class="string">f"Failure <span class="subst">{status_msg}</span>"</span>)</span><br></pre></td></tr></table></figure>
<p>  这将在检查点目录中创建 ZeRO 模型和优化器分区以及 <code>zero_to_fp32.py</code> 脚本。您可以使用此脚本进行离线合并，无需配置文件或 GPU。以下是其使用示例：</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /path/to/checkpoint_dir</span><br><span class="line">$ ./zero_to_fp32.py . pytorch_model.bin</span><br><span class="line">Processing zero checkpoint at global_step1</span><br><span class="line">Detected checkpoint of <span class="built_in">type</span> zero stage 3, world_size: 2</span><br><span class="line">Saving fp32 state dict to pytorch_model.bin (total_numel=60506624)</span><br></pre></td></tr></table></figure>
<p>  <strong>获取用于保存/推理的 32 位模型，您可以执行以下操作：</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> deepspeed.utils.zero_to_fp32 <span class="keyword">import</span> load_state_dict_from_zero_checkpoint</span><br><span class="line"></span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">fp32_model = load_state_dict_from_zero_checkpoint(unwrapped_model, checkpoint_dir)</span><br></pre></td></tr></table></figure>
<p>  <strong>如果您只对 <code>state_dict</code> 感兴趣，可以执行以下操作：</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> deepspeed.utils.zero_to_fp32 <span class="keyword">import</span> get_fp32_state_dict_from_zero_checkpoint</span><br><span class="line"></span><br><span class="line">state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)</span><br></pre></td></tr></table></figure>
<p>  请注意，所有这些函数需要约 2 倍于最终检查点大小的内存（通用 RAM）。</p>
</li>
</ul>
<h4 id="ZeRO-推理"><a href="#ZeRO-推理" class="headerlink" title="ZeRO 推理"></a><strong>ZeRO 推理</strong></h4><p><strong>DeepSpeed ZeRO Inference</strong> 支持 ZeRO Stage 3 与 <strong>ZeRO-Infinity</strong>。它使用与训练相同的 ZeRO 协议，但不使用优化器和学习率调度器，仅 Stage 3 相关。通过 <strong>Accelerate</strong> 集成，您只需按如下所示准备模型和数据加载器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model, eval_dataloader = accelerator.prepare(model, eval_dataloader)</span><br></pre></td></tr></table></figure>
<h4 id="需要注意的几个问题"><a href="#需要注意的几个问题" class="headerlink" title="需要注意的几个问题"></a><strong>需要注意的几个问题</strong></h4><ul>
<li>当前的集成不支持 <strong>DeepSpeed</strong> 的 <strong>Pipeline Parallelism</strong>。</li>
<li>当前的集成不支持 <strong>mpu</strong>，限制了在 <strong>Megatron-LM</strong> 中支持的张量并行。</li>
<li>当前的集成不支持多个模型。</li>
</ul>
<h4 id="DeepSpeed-资源"><a href="#DeepSpeed-资源" class="headerlink" title="DeepSpeed 资源"></a><strong>DeepSpeed 资源</strong></h4><p>与 DeepSpeed 相关的内部文档可以在这里找到：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">项目的 GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/">使用文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/api/">API 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deepspeed.ai/blog/">博客文章</a></li>
<li><strong>论文：</strong><ul>
<li><strong>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</strong></li>
<li><strong>ZeRO-Offload: Democratizing Billion-Scale Model Training</strong></li>
<li><strong>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</strong></li>
<li><strong>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</strong></li>
</ul>
</li>
</ul>
<p>最后，请记住，<strong>Accelerate</strong> 仅集成了 <strong>DeepSpeed</strong>，因此如果您在使用 <strong>DeepSpeed</strong> 时遇到任何问题或有任何疑问，请在 <strong>DeepSpeed GitHub</strong> 上提交问题。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/" data-id="cm5ar5vhe0001ciwifr5w65so" data-title="DeepSpeed相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basical-network/" rel="tag">basical-network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm-securaty/" rel="tag">llm-securaty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/basical-network/" style="font-size: 10px;">basical-network</a> <a href="/tags/llm/" style="font-size: 20px;">llm</a> <a href="/tags/llm-securaty/" style="font-size: 10px;">llm-securaty</a> <a href="/tags/object-detection/" style="font-size: 15px;">object-detection</a> <a href="/tags/self-supervised/" style="font-size: 10px;">self-supervised</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 15px;">多模态</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/">torch中的forward hook</a>
          </li>
        
          <li>
            <a href="/2025/01/01/ObjectDetectionRecent20Years/">ObjectDetectionRecent20Years</a>
          </li>
        
          <li>
            <a href="/2024/12/30/playingDINOv2/">playingDINOv2</a>
          </li>
        
          <li>
            <a href="/2024/12/30/CNN-surveys/">CNN-surveys</a>
          </li>
        
          <li>
            <a href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/">小样本和llm</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>