<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Open-MAGVIT-2 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="MAGVIT1&amp;2(Google&amp;腾讯的实现)MAGVIT-v1 (Google)Framework 代码源码地址 这是个20年的框架,作者主要用了jax. 所以还是得调研下jax有什么好处, 以及现在生态怎么样了. (先码住, 以后填坑) VQ-tokenizer部分 其结构图如上, 下面上代码: 12345678910111213141516171819202122232425">
<meta property="og:type" content="article">
<meta property="og:title" content="Open-MAGVIT-2">
<meta property="og:url" content="http://example.com/2024/11/23/Magvit%E7%B3%BB%E5%88%97/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="MAGVIT1&amp;2(Google&amp;腾讯的实现)MAGVIT-v1 (Google)Framework 代码源码地址 这是个20年的框架,作者主要用了jax. 所以还是得调研下jax有什么好处, 以及现在生态怎么样了. (先码住, 以后填坑) VQ-tokenizer部分 其结构图如上, 下面上代码: 12345678910111213141516171819202122232425">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-11-23T04:38:26.000Z">
<meta property="article:modified_time" content="2024-12-06T03:44:03.572Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Magvit系列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2024-11-23T04:38:26.000Z" itemprop="datePublished">2024-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Open-MAGVIT-2
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="MAGVIT1-amp-2-Google-amp-腾讯的实现"><a href="#MAGVIT1-amp-2-Google-amp-腾讯的实现" class="headerlink" title="MAGVIT1&2(Google&腾讯的实现)"></a>MAGVIT1&amp;2(Google&amp;腾讯的实现)</h1><h2 id="MAGVIT-v1-Google"><a href="#MAGVIT-v1-Google" class="headerlink" title="MAGVIT-v1 (Google)"></a>MAGVIT-v1 (Google)</h2><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a target="_blank" rel="noopener" href="https://github.com/google-research/magvit">源码地址</a></p>
<p>这是个20年的框架,作者主要用了jax. 所以还是得调研下jax有什么好处, 以及现在生态怎么样了. (<strong>先码住, 以后填坑</strong>)</p>
<h4 id="VQ-tokenizer部分"><a href="#VQ-tokenizer部分" class="headerlink" title="VQ-tokenizer部分"></a>VQ-tokenizer部分</h4>
<p>其结构图如上, 下面上代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQVAE</span>(nn.Module):</span><br><span class="line">  <span class="string">"""VQ-VAE model."""</span></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  precision: <span class="type">Any</span> = jax.lax.Precision.DEFAULT</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">"""VQ-VAE setup."""</span></span><br><span class="line">    quantizer_str = <span class="variable language_">self</span>.config.vqvae.get(</span><br><span class="line">        <span class="string">'vector_quantizer_class'</span>, <span class="string">'VectorQuantizer'</span>)</span><br><span class="line">    <span class="keyword">if</span> quantizer_str == <span class="string">'VectorQuantizer'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.quantizer = VectorQuantizer(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, precision=<span class="variable language_">self</span>.precision, dtype=<span class="variable language_">self</span>.dtype</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(quantizer_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Encoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Decoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'3dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_3dcnn.Encoder(config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_3dcnn.Decoder(config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2plus1dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_2plus1dcnn.Encoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_2plus1dcnn.Decoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">          <span class="string">f'Architecture <span class="subst">{self.config.vqvae.architecture}</span>'</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>先分析<code>enc_dec_3dcnn.Encoder</code>和<code>enc_dec_3dcnn.Decoder</code>这俩:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Encoder Blocks."""</span></span><br><span class="line"></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="variable language_">self</span>.filters = <span class="variable language_">self</span>.config.vqvae.filters</span><br><span class="line">    <span class="variable language_">self</span>.num_res_blocks = <span class="variable language_">self</span>.config.vqvae.num_enc_res_blocks</span><br><span class="line">    <span class="variable language_">self</span>.channel_multipliers = <span class="variable language_">self</span>.config.vqvae.channel_multipliers</span><br><span class="line">    <span class="variable language_">self</span>.temporal_downsample = <span class="variable language_">self</span>.config.vqvae.temporal_downsample</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.temporal_downsample, <span class="built_in">int</span>):</span><br><span class="line">      <span class="variable language_">self</span>.temporal_downsample = _get_selected_flags(</span><br><span class="line">          <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers) - <span class="number">1</span>, <span class="variable language_">self</span>.temporal_downsample, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>.embedding_dim = <span class="variable language_">self</span>.config.vqvae.embedding_dim</span><br><span class="line">    <span class="variable language_">self</span>.conv_downsample = <span class="variable language_">self</span>.config.vqvae.conv_downsample</span><br><span class="line">    <span class="variable language_">self</span>.custom_conv_padding = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'custom_conv_padding'</span>)</span><br><span class="line">    <span class="variable language_">self</span>.norm_type = <span class="variable language_">self</span>.config.vqvae.norm_type</span><br><span class="line">    <span class="variable language_">self</span>.num_remat_block = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'num_enc_remat_blocks'</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'relu'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.relu</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'swish'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.swish</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>从其setup来看, 也是resnet作encoder-decoder那一套. 从下述代码, 我们可以大概归纳出一个层级结构:</p>
<pre><code>Encoder
|---conv_fn
|    |
|---Block (Before)
|    |----ResBlock * N
|    |----conv_downsample
|---Block (last)
|    |----ResBlock * N
|---norm-act-conv_fn
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@nn.compact</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, *, is_train=<span class="literal">False</span></span>):</span><br><span class="line">  conv_fn = functools.partial(</span><br><span class="line">      model_utils.Conv,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">      custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line">  norm_fn = model_utils.get_norm_layer(</span><br><span class="line">      norm_type=<span class="variable language_">self</span>.norm_type, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">  block_args = <span class="built_in">dict</span>(</span><br><span class="line">      norm_fn=norm_fn,</span><br><span class="line">      conv_fn=conv_fn,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      activation_fn=<span class="variable language_">self</span>.activation_fn,</span><br><span class="line">      use_conv_shortcut=<span class="literal">False</span>,</span><br><span class="line">  )</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">  filters = <span class="variable language_">self</span>.filters</span><br><span class="line">  num_blocks = <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">    filters = <span class="variable language_">self</span>.filters * <span class="variable language_">self</span>.channel_multipliers[i]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">      <span class="keyword">if</span> i &lt; <span class="variable language_">self</span>.num_remat_block <span class="keyword">and</span> is_train:</span><br><span class="line">        x = ResBlock(filters, **block_args).remat_call(x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = ResBlock(filters, **block_args)(x)</span><br><span class="line">    <span class="keyword">if</span> i &lt; num_blocks - <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.conv_downsample:</span><br><span class="line">        t_stride = <span class="number">2</span> <span class="keyword">if</span> <span class="variable language_">self</span>.temporal_downsample[i] <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        x = conv_fn(</span><br><span class="line">            filters, kernel_size=(<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>), strides=(t_stride, <span class="number">2</span>, <span class="number">2</span>))(</span><br><span class="line">                x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = model_utils.downsample(x, <span class="variable language_">self</span>.temporal_downsample[i])</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">    x = ResBlock(filters, **block_args)(x)</span><br><span class="line">  x = norm_fn()(x)</span><br><span class="line">  x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.embedding_dim, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>核心就在于它的ResBlock怎么写的,不过应该大差不差.</p>
<pre><code>ResBlock
|
|---norm-act-conv_fn(3x3x3, stride=1)
|    |
|---norm-act-conv_fn(3x3x3, stride=1)
</code></pre><p>在细看<code>ResBlock</code>前,紧急插播一下<code>conv_fn</code>的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">conv_fn = functools.partial(</span><br><span class="line">    model_utils.Conv,</span><br><span class="line">    dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">    padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">    custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(nn.Conv):</span><br><span class="line">  <span class="string">"""Convolution with custom padding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Attributes:</span></span><br><span class="line"><span class="string">    custom_padding: padding mode accepted by jnp.pad. When using this, must set</span></span><br><span class="line"><span class="string">      padding=VALID to disable padding in nn.Conv.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  custom_padding: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.padding == <span class="string">'VALID'</span>, <span class="string">'Must use VALID padding for raw Conv.'</span></span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.kernel_dilation <span class="keyword">in</span> (<span class="number">1</span>, <span class="literal">None</span>), <span class="string">'Kernel dilation not supported.'</span></span><br><span class="line">      pads = [((k - <span class="number">1</span>) // <span class="number">2</span>, k // <span class="number">2</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="variable language_">self</span>.kernel_size]</span><br><span class="line">      pads = [(<span class="number">0</span>, <span class="number">0</span>)] + pads + [(<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding.startswith(</span><br><span class="line">          <span class="string">'reflect_'</span>) <span class="keyword">or</span> <span class="variable language_">self</span>.custom_padding.startswith(<span class="string">'symmetric_'</span>):</span><br><span class="line">        custom_padding, reflect_type = <span class="variable language_">self</span>.custom_padding.split(<span class="string">'_'</span>)</span><br><span class="line">        pad_kwargs = {<span class="string">'reflect_type'</span>: reflect_type}</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        custom_padding = <span class="variable language_">self</span>.custom_padding</span><br><span class="line">        pad_kwargs = {}</span><br><span class="line">      x = jnp.pad(x, pads, mode=custom_padding, **pad_kwargs)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">super</span>(Conv, <span class="variable language_">self</span>).__call__(x)</span><br></pre></td></tr></table></figure>
<p>下面我们来看ResBlock的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Basic Residual Block."""</span></span><br><span class="line">  filters: <span class="built_in">int</span></span><br><span class="line">  norm_fn: <span class="type">Any</span></span><br><span class="line">  conv_fn: <span class="type">Any</span></span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  use_conv_shortcut: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    input_dim = x.shape[-<span class="number">1</span>]</span><br><span class="line">    residual = x</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> input_dim != <span class="variable language_">self</span>.filters:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.use_conv_shortcut:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">    <span class="keyword">return</span> x + residual</span><br></pre></td></tr></table></figure>
<p>由上可见,似乎没有用瓶颈层, 而且有个问题: <strong>input_dim != self.filters:</strong>时不应该直接报错的吗? 答案是不会的, <code>self.filters</code>是out_channel, 至于in_channel, 会自动适应的.</p>
<h4 id="MLM-Masked-Language-Model-部分"><a href="#MLM-Masked-Language-Model-部分" class="headerlink" title="MLM(Masked Language Model)部分"></a>MLM(Masked Language Model)部分</h4><p>很惭愧,我找了半天没找到他们的template command line, 因此只能猜 <code>videogvt/trainers/maskgvt_trainer.py</code> 和 <code>videogvt/trainers/lmgvt_trainer.py</code>是训练脚本.</p>
<p>先看<code>maskgvt_trainer.py</code> </p>
<p>太复杂了,咱就记住<code>flax_model</code>是bert, 最重要的是<code>train_step</code>就行了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_loss_fn</span>(<span class="params">params</span>):</span><br><span class="line">  variables = {<span class="string">'params'</span>: params, **train_state.model_state}</span><br><span class="line">  logits, new_model_state = flax_model.apply(</span><br><span class="line">      variables,</span><br><span class="line">      batch_tokens[<span class="string">'masked_inputs'</span>],</span><br><span class="line">      batch_tokens[<span class="string">'segment_ids'</span>],</span><br><span class="line">      deterministic=<span class="literal">False</span>,</span><br><span class="line">      mutable=mutable,</span><br><span class="line">      rngs={<span class="string">'dropout'</span>: dropout_rng})</span><br><span class="line">  <span class="comment"># logits shape [bs, 1 + (l_cond) + l_t * l_h * l_w,</span></span><br><span class="line">  <span class="comment">#               vq_codebook_size + num_classes + num_special_tokens]</span></span><br><span class="line">  logits = logits[:, :, :vq_codebook_size]</span><br><span class="line">  <span class="comment"># TODO(roadjiang): add classification loss. Use batch_tokens['weights'][0]</span></span><br><span class="line">  one_hot_targets = jax.nn.one_hot(batch_tokens[<span class="string">'targets'</span>], logits.shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  sof_ce_loss = (</span><br><span class="line">      scenic_model_utils.weighted_unnormalized_softmax_cross_entropy(</span><br><span class="line">          logits,</span><br><span class="line">          one_hot_targets,</span><br><span class="line">          weights=batch.get(<span class="string">'batch_mask'</span>),</span><br><span class="line">          label_smoothing=config.get(<span class="string">'label_smoothing'</span>),</span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  weights = batch_tokens[<span class="string">'weights'</span>]</span><br><span class="line">  masked_sof_ce_loss = jnp.<span class="built_in">sum</span>(</span><br><span class="line">      sof_ce_loss * weights, axis=-<span class="number">1</span>) / (</span><br><span class="line">          jnp.<span class="built_in">sum</span>(weights, axis=-<span class="number">1</span>) + <span class="number">1e-8</span>)</span><br><span class="line">  token_loss = jnp.mean(masked_sof_ce_loss)</span><br><span class="line"></span><br><span class="line">  l2_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> config.get(<span class="string">'l2_decay_factor'</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    total_loss = token_loss</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    l2_loss = scenic_model_utils.l2_regularization(params)</span><br><span class="line">    total_loss = token_loss + <span class="number">0.5</span> * config.l2_decay_factor * l2_loss</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> total_loss, (new_model_state, token_loss, l2_loss)</span><br></pre></td></tr></table></figure>
<p>LM那就是BERT, 看一看<code>videogvt/models/simplified_bert.py</code>就行了. 注意,人家的logits不是分类头分出来的,而是和词表乘出来的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertMlmLayer</span>(nn.Module):</span><br><span class="line">  <span class="string">"""BERT layer for masked token prediction."""</span></span><br><span class="line"></span><br><span class="line">  hidden_size: <span class="built_in">int</span></span><br><span class="line">  initializer_fn: InitializerType</span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params"></span></span><br><span class="line"><span class="params">      self, last_layer: jnp.ndarray, embeddings: jnp.ndarray</span></span><br><span class="line"><span class="params">  </span>) -&gt; jnp.ndarray:</span><br><span class="line">    mlm_hidden = nn.Dense(</span><br><span class="line">        features=<span class="variable language_">self</span>.hidden_size,</span><br><span class="line">        kernel_init=<span class="variable language_">self</span>.initializer_fn,</span><br><span class="line">        name=<span class="string">'mlm_dense'</span>,</span><br><span class="line">    )(last_layer)</span><br><span class="line">    mlm_hidden = jax.nn.gelu(mlm_hidden)</span><br><span class="line">    mlm_hidden = nn.LayerNorm(epsilon=TF_LAYERNORM_EPSILON, name=<span class="string">'mlm_ln'</span>)(</span><br><span class="line">        mlm_hidden</span><br><span class="line">    )</span><br><span class="line">    output_weights = jnp.transpose(embeddings)</span><br><span class="line">    logits = jnp.matmul(mlm_hidden, output_weights)</span><br><span class="line">    logits = Bias(name=<span class="string">'mlm_bias'</span>)(logits)</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h2 id="MAGVIT-v2-Google-proposed-Tencent-implementation"><a href="#MAGVIT-v2-Google-proposed-Tencent-implementation" class="headerlink" title="MAGVIT-v2 (Google proposed Tencent implementation)"></a>MAGVIT-v2 (Google proposed Tencent implementation)</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><h4 id="VQ部分"><a href="#VQ部分" class="headerlink" title="VQ部分"></a>VQ部分</h4><p>咱就是说,这代码都大差不差. 不过<code>use_gan</code>设为<code>True</code>后, 很容易<code>disc_loss</code>loss就是<code>nan</code>, 目前正在排查</p>
<h4 id="MLM部分"><a href="#MLM部分" class="headerlink" title="MLM部分"></a>MLM部分</h4><p>他们尚未发布</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" data-id="cm5ar5vhg0007ciwi29ruge1x" data-title="Open-MAGVIT-2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          LLaMa系列
        
      </div>
    </a>
  
  
    <a href="/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">LFQ探究</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basical-network/" rel="tag">basical-network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm-securaty/" rel="tag">llm-securaty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/basical-network/" style="font-size: 10px;">basical-network</a> <a href="/tags/llm/" style="font-size: 20px;">llm</a> <a href="/tags/llm-securaty/" style="font-size: 10px;">llm-securaty</a> <a href="/tags/object-detection/" style="font-size: 15px;">object-detection</a> <a href="/tags/self-supervised/" style="font-size: 10px;">self-supervised</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 15px;">多模态</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/">torch中的forward hook</a>
          </li>
        
          <li>
            <a href="/2025/01/01/ObjectDetectionRecent20Years/">ObjectDetectionRecent20Years</a>
          </li>
        
          <li>
            <a href="/2024/12/30/playingDINOv2/">playingDINOv2</a>
          </li>
        
          <li>
            <a href="/2024/12/30/CNN-surveys/">CNN-surveys</a>
          </li>
        
          <li>
            <a href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/">小样本和llm</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>