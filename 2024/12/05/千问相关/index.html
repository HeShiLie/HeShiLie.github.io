<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>千问相关 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="模型结构Qwen2-1.5B 模型结构123456789101112131415161718192021222324252627Qwen2ForCausalLM(  (model): Qwen2Model(    (embed_tokens): Embedding(151936, 1536)    (layers): ModuleList(      (0-27): 28 x Qwen2Decod">
<meta property="og:type" content="article">
<meta property="og:title" content="千问相关">
<meta property="og:url" content="http://example.com/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="模型结构Qwen2-1.5B 模型结构123456789101112131415161718192021222324252627Qwen2ForCausalLM(  (model): Qwen2Model(    (embed_tokens): Embedding(151936, 1536)    (layers): ModuleList(      (0-27): 28 x Qwen2Decod">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-05T14:19:29.000Z">
<meta property="article:modified_time" content="2024-12-09T09:56:04.786Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="多模态">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-千问相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:19:29.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      千问相关
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="Qwen2-1-5B-模型结构"><a href="#Qwen2-1-5B-模型结构" class="headerlink" title="Qwen2-1.5B 模型结构"></a><code>Qwen2-1.5B</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2ForCausalLM</span>(</span><br><span class="line">  (model): <span class="built_in">Qwen2Model</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2DecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2SdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>Qwen2ForCausalLM</code> 模型主要由两大核心组件构成：</p>
<ol>
<li><strong>模型（<code>model</code>）</strong>：基于 Transformer 的核心架构，负责处理输入 Token 并生成上下文嵌入。</li>
<li><strong>语言建模头（<code>lm_head</code>）</strong>：将模型输出的嵌入转换为对应词汇的 Logits，支持 Token 预测。</li>
</ol>
<h3 id="值得注意的地方"><a href="#值得注意的地方" class="headerlink" title="值得注意的地方"></a>值得注意的地方</h3><ol>
<li><code>Qwen2RotaryEmbedding</code>: 在注意力机制中使用<strong>旋转位置编码</strong></li>
<li><code>Qwen2MLP</code>: 在MLP中使用了 <strong>门控 MLP（Gated MLP）</strong> 架构 <ul>
<li><code>up_proj</code> 主要负责扩展特征空间，使模型能够学习更复杂的表示</li>
<li><code>gate_proj</code> 主要生成控制信息流的门控信号</li>
<li>升维过程中的</li>
<li>生成门控信号，用于调节 MLP 内部的信息流</li>
</ul>
</li>
<li><code>Qwen2RMSNorm</code>: 归一化使用 <strong>RMSNorm（均方根归一化）</strong> <ul>
<li>RMSNorm 仅基于均方根（Root Mean Square, RMS）来进行归一化，而不计算均值。</li>
<li>对于输入向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>，RMSNorm 的计算公式为：<script type="math/tex; mode=display">
RMSNorm(x)=γ(xRMS(x)+ϵ)+β\text{RMSNorm}(\mathbf{x}) = \gamma \left( \frac{\mathbf{x}}{\text{RMS}(\mathbf{x}) + \epsilon} \right) + \beta</script>  其中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.304ex;" xmlns="http://www.w3.org/2000/svg" width="23.621ex" height="4.208ex" role="img" focusable="false" viewBox="0 -1283.6 10440.3 1860"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(736,0)"></path><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z" transform="translate(1653,0)"></path></g><g data-mml-node="mo" transform="translate(2209,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2598,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(3205,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3871.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msqrt" transform="translate(4927.6,0)"><g transform="translate(1020,0)"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(255.4,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(1030.9,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msubsup" transform="translate(3484.2,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,353.6) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(605,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,73.6)"><path data-c="221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"></path></g><rect width="4492.8" height="60" x="1020" y="1163.6"></rect></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container> 同样是可学习参数，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> 防止分母为零。</li>
<li><strong>特点</strong>：<ul>
<li><strong>计算效率</strong>：RMSNorm 省去了均值的计算，降低了计算复杂度，特别是在大规模模型中，这种优化可以显著减少训练和推理时间。</li>
<li><strong>性能表现</strong>：尽管 RMSNorm 忽略了均值，但在许多实践中，它能够提供与 LayerNorm 相近甚至更好的性能，尤其是在某些特定任务或架构中。</li>
<li><strong>稳定性</strong>：RMSNorm 通过仅依赖 RMS 进行规范化，可能在某些情况下提供更稳定的梯度流动，有助于训练过程的稳定性。</li>
</ul>
</li>
</ul>
</li>
<li><code>预归一化(Pre-Norm)</code> 和 <code>后归一化(Post-Norm)</code>: <code>Qwen2DecoderLayer</code> 中有两个 RMSNorm 层: <ul>
<li>input_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制和 MLP 之前</li>
<li>post_attention_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制之后，进入 MLP 之前</li>
<li>Transformer 架构中的归一化层可以放置在不同的位置，主要有两种常见的设计：<ul>
<li><strong>后归一化（Post-Norm）</strong>：<ul>
<li>归一化层位于子层（如自注意力层或 MLP 层）之后。</li>
<li>典型的 Transformer 论文如 “Attention is All You Need” 中采用此设计。</li>
<li>缺点：在非常深的模型中，可能导致梯度消失或梯度爆炸，影响训练稳定性。</li>
</ul>
</li>
<li><strong>预归一化（Pre-Norm）</strong>：<ul>
<li>归一化层位于子层之前。</li>
<li>这种设计有助于缓解深层模型中的梯度问题，提高训练的稳定性和效率。</li>
<li>近年来，越来越多的研究和实践表明，预归一化在深层模型中表现更佳。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>强化位置信息</code>: 在进入 <code>lm_head</code> 之前再次应用 <code>rotary_emb</code></li>
<li><code>SiLU (Sigmoid Linear Unit)</code> 激活函数</li>
</ol>
<h3 id="示例流程"><a href="#示例流程" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>文本输入</strong>：提示或部分文本通过 <code>embed_tokens</code> 层进行标记化并转化为嵌入。</li>
</ul>
</li>
<li><strong>模型推理</strong>：<ul>
<li>嵌入传递到堆叠的解码层，逐层应用自注意力、前馈网络和归一化，生成上下文嵌入。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li>最终嵌入通过 <code>lm_head</code> 转换为词汇表的 Logits。</li>
<li>对 Logits 应用 Softmax 获取下一个 Token 的概率分布。</li>
<li>选择概率最高的 Token（或使用采样策略如 top-k 或 nucleus 采样）生成下一个词。</li>
</ul>
</li>
<li><strong>迭代生成</strong>：<ul>
<li>将新生成的 Token 添加到输入序列，重复该过程，直到达到终止条件（例如序列结束 Token 或最大长度）。</li>
</ul>
</li>
</ol>
<h2 id="Qwen2-VL-2B-Instruct-模型结构"><a href="#Qwen2-VL-2B-Instruct-模型结构" class="headerlink" title="Qwen2-VL-2B-Instruct 模型结构"></a><code>Qwen2-VL-2B-Instruct</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2VLForConditionalGeneration</span>(</span><br><span class="line">  (visual): <span class="built_in">Qwen2VisionTransformerPretrainedModel</span>(</span><br><span class="line">    (patch_embed): <span class="built_in">PatchEmbed</span>(</span><br><span class="line">      (proj): <span class="built_in">Conv3d</span>(<span class="number">3</span>, <span class="number">1280</span>, kernel_size=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), stride=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): <span class="built_in">VisionRotaryEmbedding</span>()</span><br><span class="line">    (blocks): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">31</span>): <span class="number">32</span> x <span class="built_in">Qwen2VLVisionBlock</span>(</span><br><span class="line">        (norm1): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (norm2): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (attn): <span class="built_in">VisionSdpaAttention</span>(</span><br><span class="line">          (qkv): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">3840</span>, bias=True)</span><br><span class="line">          (proj): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">VisionMlp</span>(</span><br><span class="line">          (fc1): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">          (act): <span class="built_in">QuickGELUActivation</span>()</span><br><span class="line">          (fc2): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (merger): <span class="built_in">PatchMerger</span>(</span><br><span class="line">      (ln_q): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">      (mlp): <span class="built_in">Sequential</span>(</span><br><span class="line">        (<span class="number">0</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">        (<span class="number">1</span>): <span class="built_in">GELU</span>(approximate=<span class="string">'none'</span>)</span><br><span class="line">        (<span class="number">2</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (model): <span class="built_in">Qwen2VLModel</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2VLDecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2VLSdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由上 <code>Qwen2-VL-2B-Instruct</code> 模型主要由三个核心组件组成：</p>
<ol>
<li><strong>视觉模块（<code>visual</code>）</strong>：通过基于视觉 Transformer 的架构处理并编码视觉输入。</li>
<li><strong>语言模块（<code>model</code>）</strong>：通过堆叠的解码层处理文本输入并生成输出。</li>
<li><strong>条件生成头（<code>lm_head</code>）</strong>：将语言模块的输出转化为文本生成的词概率。</li>
</ol>
<h3 id="1-视觉模块-值得注意的地方"><a href="#1-视觉模块-值得注意的地方" class="headerlink" title="1. 视觉模块-值得注意的地方"></a>1. 视觉模块-值得注意的地方</h3><ol>
<li><code>Conv3d</code>: 使用 3D 卷积覆盖非重叠的区域实现Patchify<ul>
<li>将输入的3个通道映射到1280个特征通道</li>
<li>卷积核大小(kernel_size): <code>(2, 14, 14)</code>; 步幅(stride): <code>(2, 14, 14)</code></li>
<li><code>(N, 3, D, H, W) -&gt; (N, 1280, D_out, H_out, W_out)</code><ul>
<li>1280 是每个 Token 的嵌入维度（<code>embedding dimension</code>）。</li>
<li><strong><code>D_out * H_out * W_out</code></strong> 表示生成的 Token 数量</li>
<li>每个 Token 对应于输入图像中的一个 Patch</li>
</ul>
</li>
</ul>
</li>
<li><code>VisionRotaryEmbedding</code>: 视觉特征添加位置信息</li>
<li><code>LayerNorm</code>: 图像部分使用的是LN而非RMS, 但同样是Attn前后各一个(MLP 之前)</li>
<li><code>QuickGELUActivation</code> 激活函数: 位于两个线性层之间，作为 MLP 的非线性激活函数</li>
<li><code>PatchMerger</code>: 进行视觉token数的压缩与进一步提取特征(两层MLP):<ul>
<li><strong>减少 Patch 数量</strong>：合并相邻的 Patch，减少整体的 Token 数量</li>
<li><strong>增强特征表达和对齐维度</strong>：两层MLP提取特征, 同时对齐语言模型维度</li>
<li><strong>GELU激活函数</strong></li>
</ul>
</li>
</ol>
<h3 id="2-语言模块-值得注意的地方"><a href="#2-语言模块-值得注意的地方" class="headerlink" title="2. 语言模块-值得注意的地方"></a>2. 语言模块-值得注意的地方</h3><ol>
<li><code>词表大小</code>: 151657</li>
<li><code>embed_matrix维度</code>: [151,936, 1536]</li>
<li>其余注意事项 <code>同Qwen语言模型</code></li>
</ol>
<h3 id="示例流程-1"><a href="#示例流程-1" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>视觉输入</strong>：通过视觉模块处理，将其转化为 Patch 嵌入并编码空间信息。</li>
<li><strong>文本输入</strong>：通过语言模块的嵌入层将文本转化为特征向量。</li>
</ul>
</li>
<li><strong>条件生成</strong>：<ul>
<li>将视觉和文本信息整合到模型中，生成连贯且相关的输出。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li><code>lm_head</code> 将语言模块的输出转化为词概率，生成最终文本。</li>
</ul>
</li>
</ol>
<p>qwen2vl 的一大创新就来源于对 <code>Patch</code> 的处理</p>
<h3 id="详解一下-PatchEmbed"><a href="#详解一下-PatchEmbed" class="headerlink" title="详解一下 PatchEmbed"></a>详解一下 <code>PatchEmbed</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		patch_size: <span class="built_in">int</span> = <span class="number">14</span>,</span></span><br><span class="line"><span class="params">		temporal_patch_size: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">		in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">		embed_dim: <span class="built_in">int</span> = <span class="number">1152</span>,</span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">		<span class="variable language_">self</span>.temporal_patch_size = temporal_patch_size</span><br><span class="line">		<span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">		<span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">		  </span><br><span class="line">		kernel_size = [temporal_patch_size, patch_size, patch_size]</span><br><span class="line">		<span class="variable language_">self</span>.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=<span class="literal">False</span>)</span><br><span class="line">	  </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">		target_dtype = <span class="variable language_">self</span>.proj.weight.dtype</span><br><span class="line">		hidden_states = hidden_states.view(</span><br><span class="line">		-<span class="number">1</span>, <span class="variable language_">self</span>.in_channels, <span class="variable language_">self</span>.temporal_patch_size, <span class="variable language_">self</span>.patch_size, <span class="variable language_">self</span>.patch_size</span><br><span class="line">		)</span><br><span class="line">		hidden_states = <span class="variable language_">self</span>.proj(hidden_states.to(dtype=target_dtype)).view(-<span class="number">1</span>, <span class="variable language_">self</span>.embed_dim)</span><br><span class="line">		<span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<ul>
<li><p>输入时 <code>hidden_states</code> 维度为: [tokens=5704, dim=1176]</p>
<ul>
<li>想读懂qwen2vl是怎么处理图像视频数据的, 必须搞明白 <code>processor</code> 源码是如何处理的, 尤其是这个 <code>hidden_states</code> 维度</li>
<li>维度详情为: <code>(grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size)</code></li>
<li>可以视作确定了 tokens个数, 并且确定了后续 3D卷积 处理patch<ul>
<li>相当于后面的 3D卷积 只针对一个 patch 进行, 卷出来之后 <code>时间步</code>, <code>长</code>, <code>宽</code> 维度直接为降为1</li>
</ul>
</li>
</ul>
</li>
<li><p><code>hidden_states = hidden_states.view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)</code></p>
<ul>
<li>又将 <code>hidden_states</code> 后面一个维度拆回去</li>
</ul>
</li>
<li><p><code>hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)</code></p>
<ul>
<li>这是一个 <code>dim=1176 -&gt; self.embed_dim=1280</code> 过程</li>
<li>其中 <code>self.proj</code> 是一个 3D 卷积: <ul>
<li>in_channels=3</li>
<li>embed_dim=1280</li>
<li>kernel_size=[temporal_patch_size, patch_size, patch_size]</li>
<li>stride=[temporal_patch_size, patch_size, patch_size]</li>
<li>bias=False</li>
</ul>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>hidden_states.to(dtype=target_dtype).shape</p>
<ul>
<li>torch.Size([5704, 3, 2, 14, 14])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).shape</p>
<ul>
<li>torch.Size([5704, 1280, 1, 1, 1])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim).shape</p>
<ul>
<li>torch.Size([5704, 1280])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="详解一下-PatchMerger"><a href="#详解一下-PatchMerger" class="headerlink" title="详解一下 PatchMerger"></a>详解一下 <code>PatchMerger</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, context_dim: <span class="built_in">int</span>, spatial_merge_size: <span class="built_in">int</span> = <span class="number">2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	<span class="variable language_">self</span>.hidden_size = context_dim * (spatial_merge_size**<span class="number">2</span>)</span><br><span class="line">	<span class="variable language_">self</span>.ln_q = LayerNorm(context_dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">	<span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, <span class="variable language_">self</span>.hidden_size),</span><br><span class="line">		nn.GELU(),</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, dim),</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">	x = <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_q(x).view(-<span class="number">1</span>, <span class="variable language_">self</span>.hidden_size))</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>问题: 如果仔细阅读Qwen2VL的autoprocessor部分源码的话, 你会发现:</p>
<ul>
<li>tokenizer: 正常对文本部分进行分词, 使用”&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;”来进行初步的视频tokens记录</li>
<li>ImageProcessor: 按照 <code>时间步: 2, 长宽: 14x14</code> patchify</li>
<li>最终输出的inputs_id: 会将 “&lt;|image_pad|&gt;”等视觉pad变长, 但是实际长度却是 patchify 之后的 1/4, 这个原因就是来自于 <code>PatchMerger</code> 模块</li>
</ul>
<p>解答: <code>PatchMerger</code> 类用于将多个 patch 合并成一个更高维度的表示。这种合并操作会显著减少 patch 的数量。具体来说，<code>PatchMerger</code> 通过 <code>spatial_merge_size</code>（默认为 2）将相邻的 patch 合并(<code>十字相邻</code>)。例如，<code>spatial_merge_size=2</code> 表示每 2x2 的 patch 会被合并为一个新的 patch。因此，原本的 patch 数量会减少为原来的 1/4。</p>
<ul>
<li>重点在 <code>self.ln_q(x).view(-1, self.hidden_size)</code> 这行代码</li>
</ul>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><h2 id="视觉-语言-Vision-Language-VL-模型中有多个配置文件"><a href="#视觉-语言-Vision-Language-VL-模型中有多个配置文件" class="headerlink" title="视觉-语言(Vision-Language, VL)模型中有多个配置文件"></a>视觉-语言(Vision-Language, VL)模型中有多个配置文件</h2><ol>
<li><code>config.json</code><br> <code>config.json</code> 在模型初始化时被加载, 模型的主要配置文件，用于定义模型的架构和参数。它包含了模型的结构信息，使得模型能够根据这些配置正确地初始化和运行。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">	<span class="attr">"architectures"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="string">"Qwen2VLForConditionalGeneration"</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"attention_dropout"</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"bos_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"eos_token_id"</span><span class="punctuation">:</span> <span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_start_token_id"</span><span class="punctuation">:</span> <span class="number">151652</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_end_token_id"</span><span class="punctuation">:</span> <span class="number">151653</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_token_id"</span><span class="punctuation">:</span> <span class="number">151654</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"image_token_id"</span><span class="punctuation">:</span> <span class="number">151655</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"video_token_id"</span><span class="punctuation">:</span> <span class="number">151656</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"hidden_act"</span><span class="punctuation">:</span> <span class="string">"silu"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"hidden_size"</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"initializer_range"</span><span class="punctuation">:</span> <span class="number">0.02</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"intermediate_size"</span><span class="punctuation">:</span> <span class="number">8960</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"max_position_embeddings"</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"max_window_layers"</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"model_type"</span><span class="punctuation">:</span> <span class="string">"qwen2_vl"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_attention_heads"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_hidden_layers"</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_key_value_heads"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rms_norm_eps"</span><span class="punctuation">:</span> <span class="number">1e-06</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rope_theta"</span><span class="punctuation">:</span> <span class="number">1000000.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"sliding_window"</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"tie_word_embeddings"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"torch_dtype"</span><span class="punctuation">:</span> <span class="string">"bfloat16"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"transformers_version"</span><span class="punctuation">:</span> <span class="string">"4.41.2"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"use_cache"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"use_sliding_window"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_config"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">		<span class="attr">"depth"</span><span class="punctuation">:</span> <span class="number">32</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"embed_dim"</span><span class="punctuation">:</span> <span class="number">1280</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"mlp_ratio"</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"num_heads"</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"in_chans"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"hidden_size"</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"patch_size"</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"spatial_merge_size"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"spatial_patch_size"</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"temporal_patch_size"</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">	<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rope_scaling"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">		<span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"mrope"</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"mrope_section"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span></span><br><span class="line">		<span class="punctuation">]</span></span><br><span class="line">	<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vocab_size"</span><span class="punctuation">:</span> <span class="number">151936</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>Qwen2VLConfig</code> 与 <code>LlavaConfig</code> 的初始化配置类略有不同</p>
<ul>
<li><code>LlavaConfig</code> 类可以单独接收 <code>vision_config</code>, <code>text_config</code> </li>
<li><code>Qwen2VLConfig</code> 类主要接受语言模型的配置参数，并通过 <code>vision_config</code> 参数嵌套包含视觉模型的配置, 可以直接传入json</li>
</ul>
<p><code>Qwen2VLConfig</code> 接收参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Qwen2VLConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">	model_type = <span class="string">"qwen2_vl"</span></span><br><span class="line">	keys_to_ignore_at_inference = [<span class="string">"past_key_values"</span>]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		vocab_size=<span class="number">152064</span>,</span></span><br><span class="line"><span class="params">		hidden_size=<span class="number">8192</span>,</span></span><br><span class="line"><span class="params">		intermediate_size=<span class="number">29568</span>,</span></span><br><span class="line"><span class="params">		num_hidden_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		num_attention_heads=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">		num_key_value_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">		hidden_act=<span class="string">"silu"</span>,</span></span><br><span class="line"><span class="params">		max_position_embeddings=<span class="number">32768</span>,</span></span><br><span class="line"><span class="params">		initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">		rms_norm_eps=<span class="number">1e-05</span>,</span></span><br><span class="line"><span class="params">		use_cache=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">		tie_word_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		rope_theta=<span class="number">1000000.0</span>,</span></span><br><span class="line"><span class="params">		use_sliding_window=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		sliding_window=<span class="number">4096</span>,</span></span><br><span class="line"><span class="params">		max_window_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		attention_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">		vision_config=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		rope_scaling=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		**kwargs,</span></span><br><span class="line"><span class="params">	</span>):</span><br></pre></td></tr></table></figure></p>
<p><code>Qwen2VLConfig</code> 官方示例:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLForConditionalGeneration, Qwen2VLConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a Qwen2VL style configuration</span></span><br><span class="line">configuration = Qwen2VLConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a model from the Qwen2-VL-7B style configuration</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(configuration)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accessing the model configuration</span></span><br><span class="line">configuration = model.config</span><br></pre></td></tr></table></figure></p>
<p>自定义配置:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLConfig, Qwen2VLForConditionalGeneration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 配置文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"path/to/your/config.json"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    config_dict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Qwen2VLConfig 实例</span></span><br><span class="line">qwen2vl_config = Qwen2VLConfig(**config_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化并加载 Qwen2VL 模型</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(qwen2vl_config)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<ol>
<li><code>generation_config.json</code><br> <code>generation_config.json</code> 在调用生成方法（如 <code>generate()</code>）时被加载, 专门用于定义文本生成过程中的超参数和策略。这些配置项控制生成文本的行为，如生成长度、采样策略、温度、束搜索等, 例如:<ul>
<li><strong>生成长度</strong>：如最大生成长度（<code>max_length</code>）、最小生成长度（<code>min_length</code>）等。</li>
<li><strong>生成策略</strong>：<ul>
<li><strong>采样相关</strong>：如温度（<code>temperature</code>）、顶部K采样（<code>top_k</code>）、顶部P采样（<code>top_p</code>）等。</li>
<li><strong>束搜索</strong>：束宽度（<code>num_beams</code>）、束惩罚因子（<code>repetition_penalty</code>）等。</li>
</ul>
</li>
<li><strong>其他生成参数</strong>：如是否使用核采样（<code>do_sample</code>）、停止标记（<code>eos_token_id</code>）等。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">	<span class="attr">"bos_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"pad_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"do_sample"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"eos_token_id"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">151643</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"repetition_penalty"</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"temperature"</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"top_p"</span><span class="punctuation">:</span> <span class="number">0.001</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"top_k"</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"transformers_version"</span><span class="punctuation">:</span> <span class="string">"4.37.0"</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>值得注意的一个地方是, 在仅使用 <code>qwen2vl_config = Qwen2VLConfig(**config_dict)</code> 也就是 config.json 初始化模型的时候, 模型也会有推理参数, 这是 huggingface 源码中 PretrainedConfig 类初始化的时候会给一个默认的参数字典:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_generation_defaults</span>() -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">	<span class="keyword">return</span> {</span><br><span class="line">		<span class="string">"max_length"</span>: <span class="number">20</span>,</span><br><span class="line">		<span class="string">"min_length"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"do_sample"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"early_stopping"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"num_beams"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"num_beam_groups"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"diversity_penalty"</span>: <span class="number">0.0</span>,</span><br><span class="line">		<span class="string">"temperature"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"top_k"</span>: <span class="number">50</span>,</span><br><span class="line">		<span class="string">"top_p"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"typical_p"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"repetition_penalty"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"length_penalty"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"no_repeat_ngram_size"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"encoder_no_repeat_ngram_size"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"bad_words_ids"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"num_return_sequences"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"output_scores"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"return_dict_in_generate"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"forced_bos_token_id"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"forced_eos_token_id"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"remove_invalid_values"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"exponential_decay_length_penalty"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"suppress_tokens"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"begin_suppress_tokens"</span>: <span class="literal">None</span>,</span><br><span class="line">	}</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p><code>vocab.json</code><br> <code>vocab.json</code> 文件主要用于定义分词器的词汇表。它包含了模型可以识别和处理的所有词汇（tokens）及其对应的唯一标识符（IDs）。一些特殊tokens标记一般不会出现在这里    </p>
</li>
<li><p><code>tokenizer_config.json</code><br> <code>tokenizer_config.json</code> 文件用于存储分词器的高层配置参数。这些参数影响分词器的行为和处理方式, 如填充方式(<code>padding_side</code>)、添加特殊标记(<code>add_special_tokens</code>)、最大序列长度(<code>model_max_length</code>)等. 但不涉及具体的词汇映射或分词逻辑:</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">	"add_prefix_space": false,</span><br><span class="line">	<span class="string">"added_tokens_decoder"</span>: {</span><br><span class="line">		"<span class="number">151643</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|endoftext|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151644</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|im_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151645</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|im_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151646</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|object_ref_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151647</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|object_ref_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151648</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|box_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151649</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|box_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151650</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|quad_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151651</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|quad_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151652</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151653</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151654</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151655</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|image_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151656</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|video_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		}</span><br><span class="line">	},</span><br><span class="line">	"additional_special_tokens": [<span class="string">"&lt;|im_start|&gt;"</span>, <span class="string">"&lt;|im_end|&gt;"</span>, <span class="string">"&lt;|object_ref_start|&gt;"</span>,<span class="string">"&lt;|object_ref_end|&gt;"</span>,<span class="string">"&lt;|box_start|&gt;"</span>,<span class="string">"&lt;|box_end|&gt;"</span>,<span class="string">"&lt;|quad_start|&gt;"</span>,<span class="string">"&lt;|quad_end|&gt;"</span>,<span class="string">"&lt;|vision_start|&gt;"</span>,<span class="string">"&lt;|vision_end|&gt;"</span>,<span class="string">"&lt;|vision_pad|&gt;"</span>,<span class="string">"&lt;|image_pad|&gt;"</span>,<span class="string">"&lt;|video_pad|&gt;"</span>],</span><br><span class="line">	<span class="string">"bos_token"</span>: null,</span><br><span class="line">	<span class="string">"chat_template"</span>: <span class="string">"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n{% endif %}&lt;|im_start|&gt;{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}&lt;|im_end|&gt;\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|video_pad|&gt;&lt;|vision_end|&gt;{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}&lt;|im_end|&gt;\n{% endif %}{% endfor %}{% if add_generation_prompt %}&lt;|im_start|&gt;assistant\n{% endif %}"</span>,</span><br><span class="line">	<span class="string">"clean_up_tokenization_spaces"</span>: false,</span><br><span class="line">	<span class="string">"eos_token"</span>: <span class="string">"&lt;|im_end|&gt;"</span>,</span><br><span class="line">	<span class="string">"padding_side"</span>: <span class="string">"left"</span>,</span><br><span class="line">	<span class="string">"errors"</span>: <span class="string">"replace"</span>,</span><br><span class="line">	<span class="string">"model_max_length"</span>: <span class="number">32768</span>,</span><br><span class="line">	<span class="string">"pad_token"</span>: <span class="string">"&lt;|endoftext|&gt;"</span>,</span><br><span class="line">	<span class="string">"split_special_tokens"</span>: false,</span><br><span class="line">	<span class="string">"tokenizer_class"</span>: <span class="string">"Qwen2Tokenizer"</span>,</span><br><span class="line">	<span class="string">"unk_token"</span>: null</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tokenizer.json</code><br><code>tokenizer.json</code> 是一个综合性文件，通常包含了分词器的完整配置和分词逻辑。它不仅包含 <code>vocab.json</code> 和 <code>tokenizer_config.json</code> 的内容(但<code>tokenizer.json</code>中的add_tokens可能没有<code>tokenizer_config.json</code>中全)，还包括分词器的具体实现细节，如分词合并规则、正则表达式等, 结合 <code>tokenizer_config.json</code> 的内容，提供完整的分词器配置</p>
</li>
</ol>
<h3 id="vocab-json-tokenizer-json-和-tokenizer-config-json"><a href="#vocab-json-tokenizer-json-和-tokenizer-config-json" class="headerlink" title="vocab.json, tokenizer.json 和 tokenizer_config.json"></a><code>vocab.json</code>, <code>tokenizer.json</code> 和 <code>tokenizer_config.json</code></h3><ul>
<li><strong><code>vocab.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>vocab.json</code> 提供了词汇到ID的基础映射，是分词器不可或缺的一部分。</li>
<li><code>tokenizer.json</code> 将 <code>vocab.json</code> 嵌入其中，并结合分词规则（如BPE的合并规则）和行为参数，形成一个完整的分词器定义。</li>
</ul>
</li>
<li><strong><code>tokenizer_config.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>tokenizer_config.json</code> 专注于高层次的分词器配置参数，控制分词器的整体行为。</li>
<li><code>tokenizer.json</code> 不仅包含 <code>tokenizer_config.json</code> 的内容，还包括具体的分词逻辑和词汇表，是一个更全面的配置文件。</li>
</ul>
</li>
</ul>
<h2 id="Qwen2-1-5B-config"><a href="#Qwen2-1-5B-config" class="headerlink" title="Qwen2-1.5B config"></a><code>Qwen2-1.5B</code> config</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Qwen2Config {</span><br><span class="line">  "_attn_implementation_autoset": true,</span><br><span class="line">  <span class="string">"_name_or_path"</span>: <span class="string">"/mnt/nas/ianli/models/Qwen2-1.5B"</span>,</span><br><span class="line">  <span class="string">"architectures"</span>: [</span><br><span class="line">    <span class="string">"Qwen2ForCausalLM"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"attention_dropout"</span>: <span class="number">0.0</span>,</span><br><span class="line">  <span class="string">"bos_token_id"</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">"eos_token_id"</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">"hidden_act"</span>: <span class="string">"silu"</span>,</span><br><span class="line">  <span class="string">"hidden_size"</span>: <span class="number">1536</span>,</span><br><span class="line">  <span class="string">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">"intermediate_size"</span>: <span class="number">8960</span>,</span><br><span class="line">  <span class="string">"max_position_embeddings"</span>: <span class="number">131072</span>,</span><br><span class="line">  <span class="string">"max_window_layers"</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">"model_type"</span>: <span class="string">"qwen2"</span>,</span><br><span class="line">  <span class="string">"num_attention_heads"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"num_hidden_layers"</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">"num_key_value_heads"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">"rms_norm_eps"</span>: <span class="number">1</span>e-<span class="number">06</span>,</span><br><span class="line">  <span class="string">"rope_scaling"</span>: null,</span><br><span class="line">  <span class="string">"rope_theta"</span>: <span class="number">1000000.0</span>,</span><br><span class="line">  <span class="string">"sliding_window"</span>: null,</span><br><span class="line">  <span class="string">"tie_word_embeddings"</span>: true,</span><br><span class="line">  <span class="string">"torch_dtype"</span>: <span class="string">"bfloat16"</span>,</span><br><span class="line">  <span class="string">"transformers_version"</span>: <span class="string">"4.46.3"</span>,</span><br><span class="line">  <span class="string">"use_cache"</span>: true,</span><br><span class="line">  <span class="string">"use_sliding_window"</span>: false,</span><br><span class="line">  <span class="string">"vocab_size"</span>: <span class="number">151936</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>
<h1 id="关于Qwen2VL的训练构想与需求"><a href="#关于Qwen2VL的训练构想与需求" class="headerlink" title="关于Qwen2VL的训练构想与需求"></a>关于Qwen2VL的训练构想与需求</h1><ol>
<li><strong>网络结构</strong>: 继承 Qwen2VL 的 <code>visual部分</code> 网络结构, 修改Qwen2VL的 <code>语言部分</code> 结构与 <code>陈老师的天文语言模型</code> 结构相一致</li>
<li><strong>Tokenizer</strong>: 注意修改语言模型 <code>tokenizer</code> 中的 <code>special_tokens</code>, 尤其是添加相关视觉的 <code>special_tokens</code> 与 Qwen2VL 保持一致</li>
<li><strong>模型权重</strong>:  <code>visual部分</code> 加载所继承的 Qwen2VL 的视觉权重, <code>语言部分</code> 加载 <code>陈老师的天文语言模型</code> </li>
<li><strong>训练框架</strong>: <ul>
<li>方案一: 将修改完 网络结构 和 参数权重 的模型保存好, 基于现有微调框架进行训练<ul>
<li>问题: 可能存在兼容性的问题, 现在能想到的主要兼容问题: <ul>
<li>现有微调框架对于网络结构的继承</li>
</ul>
</li>
</ul>
</li>
<li>方案二: 训练对于直接使用现有框架而言灵活度要求更高, 同时为了后续可拓展性:<ul>
<li>计划基于 Huggingface 直接搭建训练框架(正在进行)</li>
</ul>
</li>
</ul>
</li>
<li><strong>数据要求</strong>: <ol>
<li>通用图文对<ul>
<li>因为重构了原VL模型的结构和权重, 故而该模型训练还有 视觉 和 语言 对齐的任务存在, 故而需要有大规模高质量的图文数据对支撑</li>
<li>需求(正在调研)</li>
</ul>
</li>
<li>天文图文对<ul>
<li>Apod网站爬虫数据集提供了较高质量了天文图文对</li>
<li>天文微调图文对, 使用目前已有的填空、选择、简答</li>
</ul>
</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" data-id="cm5ar5vhg0009ciwicu0xehv5" data-title="千问相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/12/05/DP%E5%92%8CDDP/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          DP和DDP
        
      </div>
    </a>
  
  
    <a href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">LLaMa系列</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basical-network/" rel="tag">basical-network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm-securaty/" rel="tag">llm-securaty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/basical-network/" style="font-size: 10px;">basical-network</a> <a href="/tags/llm/" style="font-size: 20px;">llm</a> <a href="/tags/llm-securaty/" style="font-size: 10px;">llm-securaty</a> <a href="/tags/object-detection/" style="font-size: 15px;">object-detection</a> <a href="/tags/self-supervised/" style="font-size: 10px;">self-supervised</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 15px;">多模态</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/">torch中的forward hook</a>
          </li>
        
          <li>
            <a href="/2025/01/01/ObjectDetectionRecent20Years/">ObjectDetectionRecent20Years</a>
          </li>
        
          <li>
            <a href="/2024/12/30/playingDINOv2/">playingDINOv2</a>
          </li>
        
          <li>
            <a href="/2024/12/30/CNN-surveys/">CNN-surveys</a>
          </li>
        
          <li>
            <a href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/">小样本和llm</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>