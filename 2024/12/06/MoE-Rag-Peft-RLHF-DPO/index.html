<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>MoE_Rag_Peft_RLHF_DPO | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="零本篇就是个杂记,杂烩涉猎一番,不精不深, 故而也不分不分章节序号了 MoE(Mixture of Experts) 混合专家模型（Mixture of Experts，简称MOE）是一种机器学习的集成技术，由多个专家模型（Experts）和一个门控网络（Gating Network）组成。这种模型的核心思想是将复杂的问题分解成多个子问题，每个子问题由一个专家模型处理，而门控网络则负责决定每个输入">
<meta property="og:type" content="article">
<meta property="og:title" content="MoE_Rag_Peft_RLHF_DPO">
<meta property="og:url" content="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="零本篇就是个杂记,杂烩涉猎一番,不精不深, 故而也不分不分章节序号了 MoE(Mixture of Experts) 混合专家模型（Mixture of Experts，简称MOE）是一种机器学习的集成技术，由多个专家模型（Experts）和一个门控网络（Gating Network）组成。这种模型的核心思想是将复杂的问题分解成多个子问题，每个子问题由一个专家模型处理，而门控网络则负责决定每个输入">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/img/MoE.png">
<meta property="og:image" content="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/img/MoE-expert_parallel.png">
<meta property="article:published_time" content="2024-12-05T18:16:27.000Z">
<meta property="article:modified_time" content="2024-12-05T19:09:00.613Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/img/MoE.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-MoE-Rag-Peft-RLHF-DPO" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T18:16:27.000Z" itemprop="datePublished">2024-12-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      MoE_Rag_Peft_RLHF_DPO
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="零"><a href="#零" class="headerlink" title="零"></a>零</h1><p>本篇就是个杂记,杂烩涉猎一番,不精不深, 故而也不分不分章节序号了</p>
<h1 id="MoE-Mixture-of-Experts"><a href="#MoE-Mixture-of-Experts" class="headerlink" title="MoE(Mixture of Experts)"></a>MoE(Mixture of Experts)</h1><blockquote>
<p>混合专家模型（Mixture of Experts，简称MOE）是一种机器学习的集成技术，由多个专家模型（Experts）和一个门控网络（Gating Network）组成。这种模型的核心思想是将复杂的问题分解成多个子问题，每个子问题由一个专家模型处理，而门控网络则负责决定每个输入样本应该由哪个专家处理。</p>
</blockquote>
<p><img src="./img/MoE.png" alt="MoE"></p>
<p>MoE的具体实现可以是多个FFN, 也可以更复杂(比如垒出层级的gate+FFN). 同时由此可见, expert层确实是模型权重的大头. </p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><blockquote>
<ul>
<li><p>能够在远少于稠密模型所需的计算资源下进行有效的预训练。</p>
</li>
<li><p>这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。</p>
</li>
<li><p>特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p>
</li>
</ul>
</blockquote>
<h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul>
<li>在微调阶段往往面临<strong>泛化能力不足</strong>的问题，长期以来易于引发<strong>过拟合</strong>现象。</li>
<li><strong>推理时参数量过大</strong>(所有expert都得上内存), 因此对内存的需求非常高</li>
<li><strong>batch大小的不均匀分配</strong>(不会均匀分给每一个expert)和<strong>资源利用效率不高</strong>(稠密模型时刻负载都会很高, 而MoE由于其特性很难做到)</li>
</ul>
<h2 id="典型解决办法"><a href="#典型解决办法" class="headerlink" title="典型解决办法"></a>典型解决办法</h2><ul>
<li><strong>更强的内部正则化</strong>:<ul>
<li><blockquote>
<p>例如，可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p>
</blockquote>
</li>
<li>Router z-loss <strong>(回头仔细看看)</strong><blockquote>
<p>在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 logits 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>微调阶段</strong>:<ul>
<li><strong>不应</strong>冻结所有非专家层的权重。实践中，这会导致性能大幅下降(MoE层占据了网络的主要部分, 和Qwen-VL类似, 直接把vision model冻了可能把模型调“坏”);<br><strong>应</strong>仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</li>
<li>较小的batchsize大小和较高的lr</li>
</ul>
</li>
<li><p><strong>推理时参数量过大</strong>:</p>
<ul>
<li><blockquote>
<p>预先蒸馏: 将 MoE 模型蒸馏回其对应的稠密模型。</p>
</blockquote>
</li>
<li><blockquote>
<p>专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。</p>
</blockquote>
</li>
<li><blockquote>
<p>任务级别路由: 路由器被修改为将整个句子或任务直接路由到一个专家。</p>
</blockquote>
<p>(最后一条有点费解,<strong>回头仔细看</strong>)</p>
</li>
</ul>
</li>
<li><p><strong>资源利用效率不高</strong></p>
<ul>
<li><blockquote>
<p>在experts并行中，experts被放置在不同的node上，每个node处理不同批次的训练样本。对于非 MoE 层，experts并行的行为与数据并行相同。对于 MoE 层，序列中的tokens被发送到拥有所需expert的节点。(有点类似model parallel)</p>
</blockquote>
<p><img src="./img/MoE-expert_parallel.png" alt="expert parallel"></p>
</li>
<li><blockquote>
<p>FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p>
</blockquote>
</li>
<li><blockquote>
<p>Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>batch大小的不均匀分配</strong><br>采用一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E)，门控网络通常为一个带有softmax函数的简单网络。</p>
<ul>
<li>MOE层<br>$y = \sum_{i=1}^n G(x)_i E_i(x)$</li>
<li>门控网络<br>$G_\sigma(x) = \text{Softmax}(x \cdot W_g)$</li>
<li>tok门控<ul>
<li>添加噪声<br>$H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_\text{noise})_i)$</li>
<li>保留前K值<br>$\text{KeepTopK}(v, k)_i =\begin{cases} v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v, \-\infty &amp; \text{otherwise.}\end{cases}$</li>
<li>门控网络<br>G(x) &amp;= \text{Softmax}(\text{KeepTopK}(H(x), k))</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/06/MoE-Rag-Peft-RLHF-DPO/" data-id="cm4boxx5g0002ykfyd0g740cd" data-title="MoE_Rag_Peft_RLHF_DPO" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Olmo及dolma实践</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/06/MoE-Rag-Peft-RLHF-DPO/">MoE_Rag_Peft_RLHF_DPO</a>
          </li>
        
          <li>
            <a href="/2024/12/05/Olmo%E5%8F%8Adolma%E5%AE%9E%E8%B7%B5/">Olmo及dolma实践</a>
          </li>
        
          <li>
            <a href="/2024/12/05/DeepSpeed%E7%9B%B8%E5%85%B3/">DeepSpeed相关</a>
          </li>
        
          <li>
            <a href="/2024/12/05/DP%E5%92%8CDDP/">DP和DDP</a>
          </li>
        
          <li>
            <a href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/">千问相关</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>