<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-DP和DDP" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/DP%E5%92%8CDDP/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:20:09.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/DP%E5%92%8CDDP/">DP和DDP</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p>首先 <strong>DP</strong> 和 <strong>DDP</strong> 都只是 <code>数据并行</code> 并不涉及到 <code>模型权重</code> 的拆分。</p>
</blockquote>
<h2 id="DataParallel-DP"><a href="#DataParallel-DP" class="headerlink" title="DataParallel (DP)"></a>DataParallel (DP)</h2><p>DP是较简单的一种数据并行方式，直接将模型复制到多个GPU上并行计算，每个GPU计算batch中的一部分数据，各自完成前向和反向后，将梯度汇总到主GPU上。其基本流程：</p>
<ol>
<li>加载模型、数据至内存；</li>
<li>创建DP模型；</li>
<li>DP模型的forward过程：<ol>
<li><strong>一个batch的数据均分到不同device</strong>上；</li>
<li>为每个device复制一份模型；</li>
<li>至此，每个device上有模型和一份数据，并行进行前向传播；</li>
<li>收集各个device上的输出；</li>
</ol>
</li>
<li>每个device上的模型反向传播后，收集梯度到主device上，更新主device上的模型，将模型广播到其他device上；</li>
<li>3-4循环。</li>
</ol>
<p>在DP中，只有一个主进程，主进程下有多个线程，每个线程管理一个device的训练。因此，DP中内存中只存在一份数据，各个线程间是共享这份数据的。DP和Parameter Server的方式很像。</p>
<p><strong>Demo:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, target</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx], <span class="variable language_">self</span>.target[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个简单的神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.fc(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一些数据</span></span><br><span class="line">n_sample = <span class="number">100</span></span><br><span class="line">n_dim = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">X = torch.randn(n_sample, n_dim)</span><br><span class="line">Y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (n_sample, )).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">dataset = SimpleDataset(X, Y)</span><br><span class="line">data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===== 注意：刚创建的模型是在 cpu 上的 ===== #</span></span><br><span class="line">device_ids = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">model = SimpleModel(n_dim).to(device_ids[<span class="number">0</span>])</span><br><span class="line">model = nn.DataParallel(model, device_ids=device_ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (inputs, targets) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        inputs, targets = inputs.to(<span class="string">'cuda'</span>), targets.to(<span class="string">'cuda'</span>)</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        </span><br><span class="line">        loss = nn.BCELoss()(outputs, targets.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f'Epoch <span class="subst">{epoch}</span>, Batch <span class="subst">{batch_idx}</span>, Loss: <span class="subst">{loss.item()}</span>'</span>)</span><br></pre></td></tr></table></figure>
<p>其中最重要的一行便是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model, device_ids=device_ids)</span><br></pre></td></tr></table></figure>
<p>注意，模型的参数和缓冲区都要放在<code>device_ids[0]</code>上。在执行<code>forward</code>函数时，模型会被复制到各个GPU上，对模型的属性进行更新并不会产生效果，因为前向完后各个卡上的模型就被销毁了。<strong>只有在<code>device_ids[0]</code>上对模型的参数或者buffer进行的更新才会生效！</strong></p>
<h2 id="DistributedDataParallel-DDP"><a href="#DistributedDataParallel-DDP" class="headerlink" title="DistributedDataParallel (DDP)"></a>DistributedDataParallel (DDP)</h2><p><strong>DistributedDataParallel（DDP）</strong> 是 PyTorch 提供的分布式数据并行训练接口，旨在高效地在多 GPU、甚至多机多 GPU 环境下进行训练。与 <code>DataParallel</code>（DP）相比，DDP 具有更高的效率和更好的可扩展性。</p>
<p><strong>DDP 的核心思想：</strong></p>
<ul>
<li><strong>多进程并行</strong>：为每个 GPU 启动一个独立的进程，每个进程负责在其 GPU 上执行模型的前向和反向传播。</li>
<li><strong>梯度同步</strong>：在反向传播过程中，各进程之间通过通信（如 NCCL 后端）同步梯度，确保模型参数在所有进程中保持一致。</li>
<li><strong>数据划分</strong>：使用分布式采样器（<code>DistributedSampler</code>），确保每个进程处理的数据不重叠，实现数据并行。</li>
</ul>
<h3 id="DDP-的执行流程"><a href="#DDP-的执行流程" class="headerlink" title="DDP 的执行流程"></a>DDP 的执行流程</h3><h4 id="1-准备阶段"><a href="#1-准备阶段" class="headerlink" title="1. 准备阶段"></a>1. <strong>准备阶段</strong></h4><h5 id="a-环境初始化"><a href="#a-环境初始化" class="headerlink" title="a. 环境初始化"></a>a. <strong>环境初始化</strong></h5><ul>
<li><strong>初始化进程组</strong>：使用 <code>torch.distributed.init_process_group</code>，指定通信后端（如 NCCL）、进程组名称等。</li>
<li><strong>设置设备</strong>：使用 <code>torch.cuda.set_device(local_rank)</code>，将当前进程绑定到指定的 GPU。</li>
</ul>
<h5 id="b-模型广播"><a href="#b-模型广播" class="headerlink" title="b. 模型广播"></a>b. <strong>模型广播</strong></h5><ul>
<li><strong>创建模型实例</strong>：在各个进程中创建模型实例，并将其移动到对应的 GPU 上。</li>
<li><strong>封装 DDP 模型</strong>：使用 <code>torch.nn.parallel.DistributedDataParallel</code> 封装模型。</li>
<li><strong>模型参数广播</strong>：DDP 会在后台自动将模型的参数和缓冲区从主进程广播到其他进程，确保模型初始状态一致。</li>
</ul>
<h5 id="c-注册梯度钩子"><a href="#c-注册梯度钩子" class="headerlink" title="c. 注册梯度钩子"></a>c. <strong>注册梯度钩子</strong></h5><ul>
<li><strong>Reducer 管理器</strong>：DDP 会为模型参数注册梯度钩子，在反向传播过程中自动进行梯度同步。</li>
</ul>
<h4 id="2-准备数据"><a href="#2-准备数据" class="headerlink" title="2. 准备数据"></a>2. <strong>准备数据</strong></h4><ul>
<li><strong>加载数据集</strong>：使用标准的 PyTorch 数据集或自定义数据集。</li>
<li><strong>创建分布式采样器</strong>：使用 <code>torch.utils.data.distributed.DistributedSampler</code>，确保每个进程加载的数据不重叠。</li>
<li><strong>创建数据加载器</strong>：将采样器传递给数据加载器，以便在每个 epoch 开始时正确地划分数据。</li>
</ul>
<h4 id="3-训练阶段"><a href="#3-训练阶段" class="headerlink" title="3. 训练阶段"></a>3. <strong>训练阶段</strong></h4><h5 id="a-前向传播"><a href="#a-前向传播" class="headerlink" title="a. 前向传播"></a>a. <strong>前向传播</strong></h5><ul>
<li><strong>模型前向计算</strong>：每个进程使用其本地数据执行模型的前向传播。</li>
<li><strong>同步参数和缓冲区</strong>：在初始阶段，DDP 已经同步了参数和缓冲区。在训练过程中，缓冲区（如 BatchNorm 的 <code>running_mean</code> 和 <code>running_var</code>）的更新也会被自动同步。</li>
</ul>
<h5 id="b-计算梯度"><a href="#b-计算梯度" class="headerlink" title="b. 计算梯度"></a>b. <strong>计算梯度</strong></h5><ul>
<li><strong>反向传播</strong>：每个进程独立计算梯度。</li>
<li><strong>梯度同步</strong>：DDP 在后台通过梯度钩子，使用异步的 All-Reduce 操作（如 NCCL）来平均梯度。</li>
<li><strong>更新梯度状态</strong>：当所有参数的梯度都被同步后，DDP 会将平均梯度写回参数的 <code>.grad</code> 属性。</li>
</ul>
<h5 id="c-参数更新"><a href="#c-参数更新" class="headerlink" title="c. 参数更新"></a>c. <strong>参数更新</strong></h5><ul>
<li><strong>优化器更新参数</strong>：使用优化器（如 SGD、Adam）更新模型参数。</li>
<li><strong>参数一致性</strong>：由于梯度已被同步，所有进程中的模型参数在更新后仍然保持一致。</li>
</ul>
<h4 id="4-循环训练"><a href="#4-循环训练" class="headerlink" title="4. 循环训练"></a>4. <strong>循环训练</strong></h4><ul>
<li><strong>重复上述步骤</strong>，直到完成所有的训练迭代。</li>
</ul>
<p><strong>Demo:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 基础模块 ### </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line">        cnt = torch.tensor(<span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">'cnt'</span>, cnt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="variable language_">self</span>.cnt += <span class="number">1</span></span><br><span class="line">        <span class="comment"># print("In forward: ", self.cnt, "Rank: ", self.fc.weight.device)</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.fc(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, target</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx], <span class="variable language_">self</span>.target[idx]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 2. 初始化我们的模型、数据、各种配置  ####</span></span><br><span class="line"><span class="comment">## DDP：从外部得到local_rank参数。从外面得到local_rank参数，在调用DDP的时候，其会自动给出这个参数</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">"--local_rank"</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">FLAGS = parser.parse_args()</span><br><span class="line">local_rank = FLAGS.local_rank</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP：DDP backend初始化</span></span><br><span class="line">torch.cuda.set_device(local_rank)</span><br><span class="line">dist.init_process_group(backend=<span class="string">'nccl'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 假设我们有一些数据</span></span><br><span class="line">n_sample = <span class="number">100</span></span><br><span class="line">n_dim = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">25</span></span><br><span class="line">X = torch.randn(n_sample, n_dim)  <span class="comment"># 100个样本，每个样本有10个特征</span></span><br><span class="line">Y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (n_sample, )).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">dataset = SimpleDataset(X, Y)</span><br><span class="line">sampler = torch.utils.data.distributed.DistributedSampler(dataset)</span><br><span class="line">data_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 构造模型</span></span><br><span class="line">model = SimpleModel(n_dim).to(local_rank)</span><br><span class="line"><span class="comment">## DDP: Load模型要在构造DDP模型之前，且只需要在master上加载就行了。</span></span><br><span class="line">ckpt_path = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    model.load_state_dict(torch.load(ckpt_path))</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP: 构造DDP model —————— 必须在 init_process_group 之后才可以调用 DDP</span></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment">## DDP: 要在构造DDP model之后，才能用model初始化optimizer。</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">loss_func = nn.BCELoss().to(local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 网络训练  ###</span></span><br><span class="line">model.train()</span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line">iterator = tqdm(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> iterator:</span><br><span class="line">    <span class="comment"># DDP：设置sampler的epoch，</span></span><br><span class="line">    <span class="comment"># DistributedSampler需要这个来指定shuffle方式，</span></span><br><span class="line">    <span class="comment"># 通过维持各个进程之间的相同随机数种子使不同进程能获得同样的shuffle效果。</span></span><br><span class="line">    data_loader.sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># 后面这部分，则与原来完全一致了。</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_loader:</span><br><span class="line">        data, label = data.to(local_rank), label.to(local_rank)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        prediction = model(data)</span><br><span class="line">        loss = loss_func(prediction, label.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        loss.backward()</span><br><span class="line">        iterator.desc = <span class="string">"loss = %0.3f"</span> % loss</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DDP:</span></span><br><span class="line">    <span class="comment"># 1. save模型的时候，和DP模式一样，有一个需要注意的点：保存的是model.module而不是model。</span></span><br><span class="line">    <span class="comment">#    因为model其实是DDP model，参数是被`model=DDP(model)`包起来的。</span></span><br><span class="line">    <span class="comment"># 2. 只需要在进程0上保存一次就行了，避免多次保存重复的东西。</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> epoch == num_epoch - <span class="number">1</span>:</span><br><span class="line">        torch.save(model.module.state_dict(), <span class="string">"%d.ckpt"</span> % epoch)</span><br></pre></td></tr></table></figure>
<p>结合上面的代码，一个简化版的DDP流程：</p>
<ol>
<li>读取DDP相关的配置，其中最关键的就是：<code>local_rank</code>；</li>
<li>DDP后端初始化：<code>dist.init_process_group</code>；</li>
<li>创建DDP模型，以及数据加载器。注意要为加载器创建分布式采样器（<code>DistributedSampler</code>）；</li>
<li>训练。</li>
</ol>
<p>DDP的通常启动方式：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">CUDA_VISIBLE_DEVICES</span>=<span class="string">"0,1"</span> python -m torch.distributed.launch --nproc_per_node <span class="number">2</span> ddp.py</span><br></pre></td></tr></table></figure>
<h3 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h3><p>以上过程中涉及到一些陌生的概念，其实走一遍DDP的过程就会很好理解：每个进程是一个独立的训练流程，不同进程之间共享同一份数据。为了避免不同进程使用重复的数据训练，以及训练后同步梯度，进程间需要同步。因此，其中一个重点就是每个进程序号，或者说使用的GPU的序号。</p>
<ul>
<li><code>node</code>：节点，可以是物理主机，也可以是容器；</li>
<li><code>rank</code>和<code>local_rank</code>：都表示进程在整个分布式任务中的编号。<code>rank</code>是进程在全局的编号，<code>local_rank</code>是进程在所在节点上的编号。显然，如果只有一个节点，那么二者是相等的。在启动脚本中的<code>--nproc_per_node</code>即指定一个节点上有多少进程；</li>
<li><code>world_size</code>：即整个分布式任务中进程的数量。</li>
</ul>
<p>你好！你对 <strong>DataParallel（DP）</strong> 和 <strong>DistributedDataParallel（DDP）</strong> 的区别做了一个很好的总结。确实，DP 和 DDP 在实现方式、性能和适用场景上都有显著的不同。在分布式训练的实际应用中，涉及到很多复杂的细节，例如梯度的同步方式、数据采样策略以及进程间的通信等。</p>
<p>让我进一步深入探讨你提到的几个关键点，以帮助你更全面地理解 DP 和 DDP 的工作机制。</p>
<hr>
<h2 id="DP-与-DDP-的区别"><a href="#DP-与-DDP-的区别" class="headerlink" title="DP 与 DDP 的区别"></a>DP 与 DDP 的区别</h2><h3 id="1-并行方式"><a href="#1-并行方式" class="headerlink" title="1. 并行方式"></a><strong>1. 并行方式</strong></h3><ul>
<li><p><strong>DataParallel（DP）</strong>：</p>
<ul>
<li><strong>单进程多线程</strong>：在一个进程中使用多线程实现并行计算。</li>
<li><strong>模型复制</strong>：在每个前向传播中，将模型复制到多个 GPU 上。</li>
<li><strong>数据划分</strong>：将输入数据划分成多个子批次，分别送入不同的 GPU。</li>
</ul>
</li>
<li><p><strong>DistributedDataParallel（DDP）</strong>：</p>
<ul>
<li><strong>多进程多线程</strong>：为每个 GPU 启动一个独立的进程。</li>
<li><strong>进程间通信</strong>：通过进程间通信（如 NCCL）来同步梯度和参数。</li>
<li><strong>更高的并行效率</strong>：避免了 Python 全局解释器锁（GIL）的影响，提升了计算效率。</li>
</ul>
</li>
</ul>
<h3 id="2-性能差异的原因"><a href="#2-性能差异的原因" class="headerlink" title="2. 性能差异的原因"></a><strong>2. 性能差异的原因</strong></h3><ul>
<li><p><strong>DP 通常比 DDP 慢，主要原因有：</strong></p>
<ol>
<li><p><strong>单进程的 GIL 限制</strong>：DP 使用多线程并行计算，但由于 Python 的 GIL，无法真正实现并行计算，特别是在计算密集型任务中。</p>
</li>
<li><p><strong>模型复制和数据划分的开销</strong>：DP 在每次前向传播时都需要将模型复制到各个 GPU，并划分数据，这会增加额外的开销。</p>
</li>
<li><p><strong>梯度汇总的瓶颈</strong>：DP 在反向传播时需要将各个 GPU 的梯度汇总到主 GPU，这可能导致通信瓶颈。</p>
</li>
</ol>
</li>
<li><p><strong>DDP 的优势：</strong></p>
<ul>
<li><p><strong>多进程并行，避免 GIL</strong>：每个进程独立运行，GIL 不再成为瓶颈。</p>
</li>
<li><p><strong>高效的梯度同步</strong>：使用 All-Reduce 操作，同步梯度更高效。</p>
</li>
<li><p><strong>通信开销更低</strong>：DDP 支持 Ring-AllReduce，通信成本随着 GPU 数量的增加而 <strong>相对固定</strong>，而 DP 的通信成本则随着 GPU 数量线性增长。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-适用性"><a href="#3-适用性" class="headerlink" title="3. 适用性"></a><strong>3. 适用性</strong></h3><ul>
<li><p><strong>DP 只能在单机上工作</strong>，适用于小规模的多 GPU 训练。</p>
</li>
<li><p><strong>DDP 可以在多机多卡上工作</strong>，适用于大规模的分布式训练。</p>
</li>
</ul>
<h3 id="4-模型并行的结合"><a href="#4-模型并行的结合" class="headerlink" title="4. 模型并行的结合"></a><strong>4. 模型并行的结合</strong></h3><ul>
<li><strong>DDP 可以与模型并行相结合</strong>：在需要模型并行的场景下，可以将模型的不同部分分配到不同的 GPU 上，同时使用 DDP 进行数据并行。</li>
</ul>
<hr>
<h2 id="二、DP-与-DDP-中梯度的回收方式"><a href="#二、DP-与-DDP-中梯度的回收方式" class="headerlink" title="二、DP 与 DDP 中梯度的回收方式"></a><strong>二、DP 与 DDP 中梯度的回收方式</strong></h2><h3 id="1-DP-中的梯度回收"><a href="#1-DP-中的梯度回收" class="headerlink" title="1. DP 中的梯度回收"></a><strong>1. DP 中的梯度回收</strong></h3><ul>
<li><p><strong>梯度计算</strong>：在每个 GPU 上，模型副本计算其子批次数据的梯度。</p>
</li>
<li><p><strong>梯度汇总</strong>：所有 GPU 的梯度会被收集到主 GPU（<code>device_ids[0]</code>）上，进行汇总。</p>
</li>
<li><p><strong>参数更新</strong>：在主 GPU 上更新模型参数。</p>
</li>
<li><p><strong>问题</strong>：</p>
<ul>
<li><p><strong>通信瓶颈</strong>：所有梯度都需要传输到主 GPU，通信量大。</p>
</li>
<li><p><strong>主 GPU 的负载过重</strong>：主 GPU 需要负责梯度汇总和参数更新，可能成为性能瓶颈。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-DDP-中的梯度回收"><a href="#2-DDP-中的梯度回收" class="headerlink" title="2. DDP 中的梯度回收"></a><strong>2. DDP 中的梯度回收</strong></h3><ul>
<li><p><strong>梯度计算</strong>：每个进程独立计算其负责的数据的梯度。</p>
</li>
<li><p><strong>梯度同步（All-Reduce 操作）</strong>：</p>
<ul>
<li><p><strong>All-Reduce</strong>：将所有进程的梯度进行求和，然后平均分发回每个进程。</p>
</li>
<li><p><strong>异步通信</strong>：DDP 采用异步的 All-Reduce 操作，可以与计算重叠，减少等待时间。</p>
</li>
</ul>
</li>
<li><p><strong>参数更新</strong>：每个进程使用同步后的平均梯度，更新本地的模型参数。</p>
</li>
<li><p><strong>优势</strong>：</p>
<ul>
<li><p><strong>通信效率高</strong>：All-Reduce 操作的通信开销相对固定，不会随着 GPU 数量的增加而线性增长。</p>
</li>
<li><p><strong>没有单点瓶颈</strong>：所有进程同时参与通信和计算，避免了主 GPU 的瓶颈。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-通信成本对比"><a href="#3-通信成本对比" class="headerlink" title="3. 通信成本对比"></a><strong>3. 通信成本对比</strong></h3><ul>
<li><p><strong>DP 的通信成本</strong>：</p>
<ul>
<li><p>随着 GPU 数量的增加，通信成本 <strong>线性增长</strong>。</p>
</li>
<li><p>主 GPU 需要收集和广播梯度，通信量大。</p>
</li>
</ul>
</li>
<li><p><strong>DDP 的通信成本</strong>：</p>
<ul>
<li><p>使用 Ring-AllReduce，通信成本 <strong>相对固定</strong>。</p>
</li>
<li><p>通信效率随着 GPU 数量的增加而 <strong>更高效</strong>。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="三、DDP-中数据采样的细节"><a href="#三、DDP-中数据采样的细节" class="headerlink" title="三、DDP 中数据采样的细节"></a><strong>三、DDP 中数据采样的细节</strong></h2><h3 id="1-为什么需要-DistributedSampler"><a href="#1-为什么需要-DistributedSampler" class="headerlink" title="1. 为什么需要 DistributedSampler"></a><strong>1. 为什么需要 <code>DistributedSampler</code></strong></h3><ul>
<li><p><strong>数据划分的必要性</strong>：在 DDP 中，每个进程都独立运行，为了避免不同进程处理相同的数据（数据重叠），需要确保每个进程处理的数据是互不重叠的子集。</p>
</li>
<li><p><strong><code>DistributedSampler</code> 的作用</strong>：</p>
<ul>
<li><p><strong>划分数据集</strong>：将数据集划分为若干份，每个进程处理其中一份。</p>
</li>
<li><p><strong>确保随机性一致</strong>：在每个 epoch 开始时，通过设置相同的随机种子，确保各进程的数据划分方式一致。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-DistributedSampler-的工作机制"><a href="#2-DistributedSampler-的工作机制" class="headerlink" title="2. DistributedSampler 的工作机制"></a><strong>2. <code>DistributedSampler</code> 的工作机制</strong></h3><ul>
<li><p><strong>分割数据集</strong>：根据 <code>world_size</code>（总进程数）和 <code>rank</code>（当前进程编号），计算当前进程应该处理的数据索引范围。</p>
</li>
<li><p><strong>处理数据不重叠</strong>：不同进程处理的数据索引范围不重叠，确保了数据并行。</p>
</li>
<li><p><strong>支持数据随机打乱</strong>：在每个 epoch，可以通过设置不同的随机种子，实现数据的随机打乱。</p>
</li>
</ul>
<h3 id="3-设置-sampler-set-epoch-epoch-的必要性"><a href="#3-设置-sampler-set-epoch-epoch-的必要性" class="headerlink" title="3. 设置 sampler.set_epoch(epoch) 的必要性"></a><strong>3. 设置 <code>sampler.set_epoch(epoch)</code> 的必要性</strong></h3><ul>
<li><p><strong>原因</strong>：</p>
<ul>
<li><p><strong>确保数据乱序的一致性</strong>：在每个 epoch 开始时，需要为 <code>DistributedSampler</code> 设置 epoch，以确保所有进程的数据乱序方式一致。</p>
</li>
<li><p><strong>避免数据重复或遗漏</strong>：不同进程在数据乱序时，如果不设置相同的种子，可能导致数据重复或遗漏，影响模型训练的正确性。</p>
</li>
</ul>
</li>
<li><p><strong>使用方法</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_loader.sampler.set_epoch(epoch)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<h2 id="四、DDP-中的数据同步操作"><a href="#四、DDP-中的数据同步操作" class="headerlink" title="四、DDP 中的数据同步操作"></a><strong>四、DDP 中的数据同步操作</strong></h2><h3 id="1-模型参数和缓冲区的同步"><a href="#1-模型参数和缓冲区的同步" class="headerlink" title="1. 模型参数和缓冲区的同步"></a><strong>1. 模型参数和缓冲区的同步</strong></h3><ul>
<li><p><strong>初始同步</strong>：</p>
<ul>
<li><strong>参数广播</strong>：在 DDP 初始化时，自动将主进程（<code>rank == 0</code>）的模型参数和缓冲区广播到其他进程，确保所有进程的模型状态一致。</li>
</ul>
</li>
<li><p><strong>缓冲区的同步</strong>：</p>
<ul>
<li><p><strong>自动同步</strong>：在前向和反向传播过程中，DDP 会自动同步模型的缓冲区（如 BatchNorm 的 <code>running_mean</code> 和 <code>running_var</code>）。</p>
</li>
<li><p><strong>确保一致性</strong>：使得模型在所有进程中的缓冲区状态保持一致。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-梯度的同步（All-Reduce-操作）"><a href="#2-梯度的同步（All-Reduce-操作）" class="headerlink" title="2. 梯度的同步（All-Reduce 操作）"></a><strong>2. 梯度的同步（All-Reduce 操作）</strong></h3><ul>
<li><p><strong>注册梯度钩子</strong>：DDP 为每个模型参数注册了梯度钩子，当参数的梯度计算完成后，自动触发 All-Reduce 操作。</p>
</li>
<li><p><strong>All-Reduce 的过程</strong>：</p>
<ul>
<li><p><strong>梯度求和</strong>：将所有进程的对应参数的梯度相加。</p>
</li>
<li><p><strong>梯度平均</strong>：将总和除以进程数，得到平均梯度。</p>
</li>
<li><p><strong>同步更新</strong>：将平均梯度分发回各个进程，更新模型参数。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-通信操作的处理"><a href="#3-通信操作的处理" class="headerlink" title="3. 通信操作的处理"></a><strong>3. 通信操作的处理</strong></h3><ul>
<li><p><strong>通信后端</strong>：通常使用高效的通信库（如 NCCL）进行进程间通信。</p>
</li>
<li><p><strong>通信模式</strong>：</p>
<ul>
<li><p><strong>Broadcast（广播）</strong>：用于初始参数和缓冲区的同步。</p>
</li>
<li><p><strong>All-Reduce</strong>：用于梯度的同步。</p>
</li>
<li><p><strong>异步通信</strong>：DDP 采用异步通信机制，通信和计算可以重叠，减少等待时间。</p>
</li>
</ul>
</li>
<li><p><strong>用户无需干预</strong>：这些通信操作都由 DDP 在后台自动处理，用户不需要手动编写通信代码。</p>
</li>
</ul>
<hr>
<h2 id="五、基于真实需求的实践体会"><a href="#五、基于真实需求的实践体会" class="headerlink" title="五、基于真实需求的实践体会"></a><strong>五、基于真实需求的实践体会</strong></h2><ul>
<li><p><strong>复杂性与细节</strong>：在分布式训练中，涉及到很多复杂的细节，包括通信机制、数据同步、随机性控制等。</p>
</li>
<li><p><strong>实践的重要性</strong>：只有在真实的项目中，面对具体的需求和挑战，才能深入理解并解决分布式训练中的各种问题。</p>
</li>
<li><p><strong>建议</strong>：</p>
<ul>
<li><p><strong>深入学习 PyTorch 官方文档和示例</strong>：了解 DDP 的详细使用方法和注意事项。</p>
</li>
<li><p><strong>从小规模实验开始</strong>：先在单机多 GPU 环境下实践 DDP，熟悉其工作机制。</p>
</li>
<li><p><strong>逐步扩展到多机环境</strong>：在熟悉基本原理后，可以尝试在多机多卡的环境下进行训练，处理更多的实际问题。</p>
</li>
<li><p><strong>关注性能优化</strong>：在实践中，可以针对通信开销、数据加载效率、模型并行等方面进行优化，提升训练性能。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a><strong>六、总结</strong></h2><ul>
<li><p><strong>DP 与 DDP 的主要区别</strong>在于并行方式、通信机制和性能表现。</p>
</li>
<li><p><strong>DP 的局限性</strong>：</p>
<ul>
<li><p>受限于 GIL，无法充分利用多核 CPU 和多 GPU 的计算能力。</p>
</li>
<li><p>通信开销随着 GPU 数量线性增长，主 GPU 可能成为瓶颈。</p>
</li>
</ul>
</li>
<li><p><strong>DDP 的优势</strong>：</p>
<ul>
<li><p>采用多进程并行，避开 GIL 限制，充分利用硬件资源。</p>
</li>
<li><p>使用高效的 All-Reduce 操作，同步梯度和参数，通信开销低。</p>
</li>
<li><p>支持多机多卡，具有良好的扩展性。</p>
</li>
</ul>
</li>
<li><p><strong>实践中需要注意的细节</strong>：</p>
<ul>
<li><p>正确设置数据采样器，确保数据不重叠。</p>
</li>
<li><p>理解梯度同步和参数更新的机制。</p>
</li>
<li><p>熟悉 DDP 的启动和配置方法。</p>
</li>
</ul>
</li>
</ul>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/DP%E5%92%8CDDP/" data-id="cm5ar5vhb0000ciwi3yd85gcy" data-title="DP和DDP" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-千问相关" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T14:19:29.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/">千问相关</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="Qwen2-1-5B-模型结构"><a href="#Qwen2-1-5B-模型结构" class="headerlink" title="Qwen2-1.5B 模型结构"></a><code>Qwen2-1.5B</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2ForCausalLM</span>(</span><br><span class="line">  (model): <span class="built_in">Qwen2Model</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2DecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2SdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2RotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>Qwen2ForCausalLM</code> 模型主要由两大核心组件构成：</p>
<ol>
<li><strong>模型（<code>model</code>）</strong>：基于 Transformer 的核心架构，负责处理输入 Token 并生成上下文嵌入。</li>
<li><strong>语言建模头（<code>lm_head</code>）</strong>：将模型输出的嵌入转换为对应词汇的 Logits，支持 Token 预测。</li>
</ol>
<h3 id="值得注意的地方"><a href="#值得注意的地方" class="headerlink" title="值得注意的地方"></a>值得注意的地方</h3><ol>
<li><code>Qwen2RotaryEmbedding</code>: 在注意力机制中使用<strong>旋转位置编码</strong></li>
<li><code>Qwen2MLP</code>: 在MLP中使用了 <strong>门控 MLP（Gated MLP）</strong> 架构 <ul>
<li><code>up_proj</code> 主要负责扩展特征空间，使模型能够学习更复杂的表示</li>
<li><code>gate_proj</code> 主要生成控制信息流的门控信号</li>
<li>升维过程中的</li>
<li>生成门控信号，用于调节 MLP 内部的信息流</li>
</ul>
</li>
<li><code>Qwen2RMSNorm</code>: 归一化使用 <strong>RMSNorm（均方根归一化）</strong> <ul>
<li>RMSNorm 仅基于均方根（Root Mean Square, RMS）来进行归一化，而不计算均值。</li>
<li>对于输入向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>，RMSNorm 的计算公式为：<script type="math/tex; mode=display">
RMSNorm(x)=γ(xRMS(x)+ϵ)+β\text{RMSNorm}(\mathbf{x}) = \gamma \left( \frac{\mathbf{x}}{\text{RMS}(\mathbf{x}) + \epsilon} \right) + \beta</script>  其中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.304ex;" xmlns="http://www.w3.org/2000/svg" width="23.621ex" height="4.208ex" role="img" focusable="false" viewBox="0 -1283.6 10440.3 1860"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(736,0)"></path><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z" transform="translate(1653,0)"></path></g><g data-mml-node="mo" transform="translate(2209,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2598,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(3205,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3871.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msqrt" transform="translate(4927.6,0)"><g transform="translate(1020,0)"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(255.4,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(1030.9,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msubsup" transform="translate(3484.2,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,353.6) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(605,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,73.6)"><path data-c="221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"></path></g><rect width="4492.8" height="60" x="1020" y="1163.6"></rect></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container> 同样是可学习参数，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> 防止分母为零。</li>
<li><strong>特点</strong>：<ul>
<li><strong>计算效率</strong>：RMSNorm 省去了均值的计算，降低了计算复杂度，特别是在大规模模型中，这种优化可以显著减少训练和推理时间。</li>
<li><strong>性能表现</strong>：尽管 RMSNorm 忽略了均值，但在许多实践中，它能够提供与 LayerNorm 相近甚至更好的性能，尤其是在某些特定任务或架构中。</li>
<li><strong>稳定性</strong>：RMSNorm 通过仅依赖 RMS 进行规范化，可能在某些情况下提供更稳定的梯度流动，有助于训练过程的稳定性。</li>
</ul>
</li>
</ul>
</li>
<li><code>预归一化(Pre-Norm)</code> 和 <code>后归一化(Post-Norm)</code>: <code>Qwen2DecoderLayer</code> 中有两个 RMSNorm 层: <ul>
<li>input_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制和 MLP 之前</li>
<li>post_attention_layernorm(<code>Qwen2RMSNorm</code>): 位于自注意力机制之后，进入 MLP 之前</li>
<li>Transformer 架构中的归一化层可以放置在不同的位置，主要有两种常见的设计：<ul>
<li><strong>后归一化（Post-Norm）</strong>：<ul>
<li>归一化层位于子层（如自注意力层或 MLP 层）之后。</li>
<li>典型的 Transformer 论文如 “Attention is All You Need” 中采用此设计。</li>
<li>缺点：在非常深的模型中，可能导致梯度消失或梯度爆炸，影响训练稳定性。</li>
</ul>
</li>
<li><strong>预归一化（Pre-Norm）</strong>：<ul>
<li>归一化层位于子层之前。</li>
<li>这种设计有助于缓解深层模型中的梯度问题，提高训练的稳定性和效率。</li>
<li>近年来，越来越多的研究和实践表明，预归一化在深层模型中表现更佳。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>强化位置信息</code>: 在进入 <code>lm_head</code> 之前再次应用 <code>rotary_emb</code></li>
<li><code>SiLU (Sigmoid Linear Unit)</code> 激活函数</li>
</ol>
<h3 id="示例流程"><a href="#示例流程" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>文本输入</strong>：提示或部分文本通过 <code>embed_tokens</code> 层进行标记化并转化为嵌入。</li>
</ul>
</li>
<li><strong>模型推理</strong>：<ul>
<li>嵌入传递到堆叠的解码层，逐层应用自注意力、前馈网络和归一化，生成上下文嵌入。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li>最终嵌入通过 <code>lm_head</code> 转换为词汇表的 Logits。</li>
<li>对 Logits 应用 Softmax 获取下一个 Token 的概率分布。</li>
<li>选择概率最高的 Token（或使用采样策略如 top-k 或 nucleus 采样）生成下一个词。</li>
</ul>
</li>
<li><strong>迭代生成</strong>：<ul>
<li>将新生成的 Token 添加到输入序列，重复该过程，直到达到终止条件（例如序列结束 Token 或最大长度）。</li>
</ul>
</li>
</ol>
<h2 id="Qwen2-VL-2B-Instruct-模型结构"><a href="#Qwen2-VL-2B-Instruct-模型结构" class="headerlink" title="Qwen2-VL-2B-Instruct 模型结构"></a><code>Qwen2-VL-2B-Instruct</code> 模型结构</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Qwen2VLForConditionalGeneration</span>(</span><br><span class="line">  (visual): <span class="built_in">Qwen2VisionTransformerPretrainedModel</span>(</span><br><span class="line">    (patch_embed): <span class="built_in">PatchEmbed</span>(</span><br><span class="line">      (proj): <span class="built_in">Conv3d</span>(<span class="number">3</span>, <span class="number">1280</span>, kernel_size=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), stride=(<span class="number">2</span>, <span class="number">14</span>, <span class="number">14</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (rotary_pos_emb): <span class="built_in">VisionRotaryEmbedding</span>()</span><br><span class="line">    (blocks): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">31</span>): <span class="number">32</span> x <span class="built_in">Qwen2VLVisionBlock</span>(</span><br><span class="line">        (norm1): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (norm2): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">        (attn): <span class="built_in">VisionSdpaAttention</span>(</span><br><span class="line">          (qkv): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">3840</span>, bias=True)</span><br><span class="line">          (proj): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">VisionMlp</span>(</span><br><span class="line">          (fc1): <span class="built_in">Linear</span>(in_features=<span class="number">1280</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">          (act): <span class="built_in">QuickGELUActivation</span>()</span><br><span class="line">          (fc2): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1280</span>, bias=True)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (merger): <span class="built_in">PatchMerger</span>(</span><br><span class="line">      (ln_q): <span class="built_in">LayerNorm</span>((<span class="number">1280</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>, elementwise_affine=True)</span><br><span class="line">      (mlp): <span class="built_in">Sequential</span>(</span><br><span class="line">        (<span class="number">0</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">5120</span>, bias=True)</span><br><span class="line">        (<span class="number">1</span>): <span class="built_in">GELU</span>(approximate=<span class="string">'none'</span>)</span><br><span class="line">        (<span class="number">2</span>): <span class="built_in">Linear</span>(in_features=<span class="number">5120</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (model): <span class="built_in">Qwen2VLModel</span>(</span><br><span class="line">    (embed_tokens): <span class="built_in">Embedding</span>(<span class="number">151936</span>, <span class="number">1536</span>)</span><br><span class="line">    (layers): <span class="built_in">ModuleList</span>(</span><br><span class="line">      (<span class="number">0</span>-<span class="number">27</span>): <span class="number">28</span> x <span class="built_in">Qwen2VLDecoderLayer</span>(</span><br><span class="line">        (self_attn): <span class="built_in">Qwen2VLSdpaAttention</span>(</span><br><span class="line">          (q_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=True)</span><br><span class="line">          (k_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (v_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">256</span>, bias=True)</span><br><span class="line">          (o_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">        )</span><br><span class="line">        (mlp): <span class="built_in">Qwen2MLP</span>(</span><br><span class="line">          (gate_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (up_proj): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">8960</span>, bias=False)</span><br><span class="line">          (down_proj): <span class="built_in">Linear</span>(in_features=<span class="number">8960</span>, out_features=<span class="number">1536</span>, bias=False)</span><br><span class="line">          (act_fn): <span class="built_in">SiLU</span>()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">        (post_attention_layernorm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): <span class="built_in">Qwen2RMSNorm</span>((<span class="number">1536</span>,), eps=<span class="number">1</span>e-<span class="number">06</span>)</span><br><span class="line">    (rotary_emb): <span class="built_in">Qwen2VLRotaryEmbedding</span>()</span><br><span class="line">  )</span><br><span class="line">  (lm_head): <span class="built_in">Linear</span>(in_features=<span class="number">1536</span>, out_features=<span class="number">151936</span>, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由上 <code>Qwen2-VL-2B-Instruct</code> 模型主要由三个核心组件组成：</p>
<ol>
<li><strong>视觉模块（<code>visual</code>）</strong>：通过基于视觉 Transformer 的架构处理并编码视觉输入。</li>
<li><strong>语言模块（<code>model</code>）</strong>：通过堆叠的解码层处理文本输入并生成输出。</li>
<li><strong>条件生成头（<code>lm_head</code>）</strong>：将语言模块的输出转化为文本生成的词概率。</li>
</ol>
<h3 id="1-视觉模块-值得注意的地方"><a href="#1-视觉模块-值得注意的地方" class="headerlink" title="1. 视觉模块-值得注意的地方"></a>1. 视觉模块-值得注意的地方</h3><ol>
<li><code>Conv3d</code>: 使用 3D 卷积覆盖非重叠的区域实现Patchify<ul>
<li>将输入的3个通道映射到1280个特征通道</li>
<li>卷积核大小(kernel_size): <code>(2, 14, 14)</code>; 步幅(stride): <code>(2, 14, 14)</code></li>
<li><code>(N, 3, D, H, W) -&gt; (N, 1280, D_out, H_out, W_out)</code><ul>
<li>1280 是每个 Token 的嵌入维度（<code>embedding dimension</code>）。</li>
<li><strong><code>D_out * H_out * W_out</code></strong> 表示生成的 Token 数量</li>
<li>每个 Token 对应于输入图像中的一个 Patch</li>
</ul>
</li>
</ul>
</li>
<li><code>VisionRotaryEmbedding</code>: 视觉特征添加位置信息</li>
<li><code>LayerNorm</code>: 图像部分使用的是LN而非RMS, 但同样是Attn前后各一个(MLP 之前)</li>
<li><code>QuickGELUActivation</code> 激活函数: 位于两个线性层之间，作为 MLP 的非线性激活函数</li>
<li><code>PatchMerger</code>: 进行视觉token数的压缩与进一步提取特征(两层MLP):<ul>
<li><strong>减少 Patch 数量</strong>：合并相邻的 Patch，减少整体的 Token 数量</li>
<li><strong>增强特征表达和对齐维度</strong>：两层MLP提取特征, 同时对齐语言模型维度</li>
<li><strong>GELU激活函数</strong></li>
</ul>
</li>
</ol>
<h3 id="2-语言模块-值得注意的地方"><a href="#2-语言模块-值得注意的地方" class="headerlink" title="2. 语言模块-值得注意的地方"></a>2. 语言模块-值得注意的地方</h3><ol>
<li><code>词表大小</code>: 151657</li>
<li><code>embed_matrix维度</code>: [151,936, 1536]</li>
<li>其余注意事项 <code>同Qwen语言模型</code></li>
</ol>
<h3 id="示例流程-1"><a href="#示例流程-1" class="headerlink" title="示例流程"></a>示例流程</h3><ol>
<li><strong>输入处理</strong>：<ul>
<li><strong>视觉输入</strong>：通过视觉模块处理，将其转化为 Patch 嵌入并编码空间信息。</li>
<li><strong>文本输入</strong>：通过语言模块的嵌入层将文本转化为特征向量。</li>
</ul>
</li>
<li><strong>条件生成</strong>：<ul>
<li>将视觉和文本信息整合到模型中，生成连贯且相关的输出。</li>
</ul>
</li>
<li><strong>输出生成</strong>：<ul>
<li><code>lm_head</code> 将语言模块的输出转化为词概率，生成最终文本。</li>
</ul>
</li>
</ol>
<p>qwen2vl 的一大创新就来源于对 <code>Patch</code> 的处理</p>
<h3 id="详解一下-PatchEmbed"><a href="#详解一下-PatchEmbed" class="headerlink" title="详解一下 PatchEmbed"></a>详解一下 <code>PatchEmbed</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		patch_size: <span class="built_in">int</span> = <span class="number">14</span>,</span></span><br><span class="line"><span class="params">		temporal_patch_size: <span class="built_in">int</span> = <span class="number">2</span>,</span></span><br><span class="line"><span class="params">		in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span><br><span class="line"><span class="params">		embed_dim: <span class="built_in">int</span> = <span class="number">1152</span>,</span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">		<span class="variable language_">self</span>.temporal_patch_size = temporal_patch_size</span><br><span class="line">		<span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">		<span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">		  </span><br><span class="line">		kernel_size = [temporal_patch_size, patch_size, patch_size]</span><br><span class="line">		<span class="variable language_">self</span>.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=<span class="literal">False</span>)</span><br><span class="line">	  </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">		target_dtype = <span class="variable language_">self</span>.proj.weight.dtype</span><br><span class="line">		hidden_states = hidden_states.view(</span><br><span class="line">		-<span class="number">1</span>, <span class="variable language_">self</span>.in_channels, <span class="variable language_">self</span>.temporal_patch_size, <span class="variable language_">self</span>.patch_size, <span class="variable language_">self</span>.patch_size</span><br><span class="line">		)</span><br><span class="line">		hidden_states = <span class="variable language_">self</span>.proj(hidden_states.to(dtype=target_dtype)).view(-<span class="number">1</span>, <span class="variable language_">self</span>.embed_dim)</span><br><span class="line">		<span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<ul>
<li><p>输入时 <code>hidden_states</code> 维度为: [tokens=5704, dim=1176]</p>
<ul>
<li>想读懂qwen2vl是怎么处理图像视频数据的, 必须搞明白 <code>processor</code> 源码是如何处理的, 尤其是这个 <code>hidden_states</code> 维度</li>
<li>维度详情为: <code>(grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size)</code></li>
<li>可以视作确定了 tokens个数, 并且确定了后续 3D卷积 处理patch<ul>
<li>相当于后面的 3D卷积 只针对一个 patch 进行, 卷出来之后 <code>时间步</code>, <code>长</code>, <code>宽</code> 维度直接为降为1</li>
</ul>
</li>
</ul>
</li>
<li><p><code>hidden_states = hidden_states.view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)</code></p>
<ul>
<li>又将 <code>hidden_states</code> 后面一个维度拆回去</li>
</ul>
</li>
<li><p><code>hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)</code></p>
<ul>
<li>这是一个 <code>dim=1176 -&gt; self.embed_dim=1280</code> 过程</li>
<li>其中 <code>self.proj</code> 是一个 3D 卷积: <ul>
<li>in_channels=3</li>
<li>embed_dim=1280</li>
<li>kernel_size=[temporal_patch_size, patch_size, patch_size]</li>
<li>stride=[temporal_patch_size, patch_size, patch_size]</li>
<li>bias=False</li>
</ul>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>hidden_states.to(dtype=target_dtype).shape</p>
<ul>
<li>torch.Size([5704, 3, 2, 14, 14])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).shape</p>
<ul>
<li>torch.Size([5704, 1280, 1, 1, 1])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><blockquote>
<blockquote>
<blockquote>
<p>self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim).shape</p>
<ul>
<li>torch.Size([5704, 1280])</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="详解一下-PatchMerger"><a href="#详解一下-PatchMerger" class="headerlink" title="详解一下 PatchMerger"></a>详解一下 <code>PatchMerger</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, context_dim: <span class="built_in">int</span>, spatial_merge_size: <span class="built_in">int</span> = <span class="number">2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	<span class="variable language_">self</span>.hidden_size = context_dim * (spatial_merge_size**<span class="number">2</span>)</span><br><span class="line">	<span class="variable language_">self</span>.ln_q = LayerNorm(context_dim, eps=<span class="number">1e-6</span>)</span><br><span class="line">	<span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, <span class="variable language_">self</span>.hidden_size),</span><br><span class="line">		nn.GELU(),</span><br><span class="line">		nn.Linear(<span class="variable language_">self</span>.hidden_size, dim),</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">	x = <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_q(x).view(-<span class="number">1</span>, <span class="variable language_">self</span>.hidden_size))</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>问题: 如果仔细阅读Qwen2VL的autoprocessor部分源码的话, 你会发现:</p>
<ul>
<li>tokenizer: 正常对文本部分进行分词, 使用”&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;”来进行初步的视频tokens记录</li>
<li>ImageProcessor: 按照 <code>时间步: 2, 长宽: 14x14</code> patchify</li>
<li>最终输出的inputs_id: 会将 “&lt;|image_pad|&gt;”等视觉pad变长, 但是实际长度却是 patchify 之后的 1/4, 这个原因就是来自于 <code>PatchMerger</code> 模块</li>
</ul>
<p>解答: <code>PatchMerger</code> 类用于将多个 patch 合并成一个更高维度的表示。这种合并操作会显著减少 patch 的数量。具体来说，<code>PatchMerger</code> 通过 <code>spatial_merge_size</code>（默认为 2）将相邻的 patch 合并(<code>十字相邻</code>)。例如，<code>spatial_merge_size=2</code> 表示每 2x2 的 patch 会被合并为一个新的 patch。因此，原本的 patch 数量会减少为原来的 1/4。</p>
<ul>
<li>重点在 <code>self.ln_q(x).view(-1, self.hidden_size)</code> 这行代码</li>
</ul>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><h2 id="视觉-语言-Vision-Language-VL-模型中有多个配置文件"><a href="#视觉-语言-Vision-Language-VL-模型中有多个配置文件" class="headerlink" title="视觉-语言(Vision-Language, VL)模型中有多个配置文件"></a>视觉-语言(Vision-Language, VL)模型中有多个配置文件</h2><ol>
<li><code>config.json</code><br> <code>config.json</code> 在模型初始化时被加载, 模型的主要配置文件，用于定义模型的架构和参数。它包含了模型的结构信息，使得模型能够根据这些配置正确地初始化和运行。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">	<span class="attr">"architectures"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="string">"Qwen2VLForConditionalGeneration"</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"attention_dropout"</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"bos_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"eos_token_id"</span><span class="punctuation">:</span> <span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_start_token_id"</span><span class="punctuation">:</span> <span class="number">151652</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_end_token_id"</span><span class="punctuation">:</span> <span class="number">151653</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_token_id"</span><span class="punctuation">:</span> <span class="number">151654</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"image_token_id"</span><span class="punctuation">:</span> <span class="number">151655</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"video_token_id"</span><span class="punctuation">:</span> <span class="number">151656</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"hidden_act"</span><span class="punctuation">:</span> <span class="string">"silu"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"hidden_size"</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"initializer_range"</span><span class="punctuation">:</span> <span class="number">0.02</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"intermediate_size"</span><span class="punctuation">:</span> <span class="number">8960</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"max_position_embeddings"</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"max_window_layers"</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"model_type"</span><span class="punctuation">:</span> <span class="string">"qwen2_vl"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_attention_heads"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_hidden_layers"</span><span class="punctuation">:</span> <span class="number">28</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"num_key_value_heads"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rms_norm_eps"</span><span class="punctuation">:</span> <span class="number">1e-06</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rope_theta"</span><span class="punctuation">:</span> <span class="number">1000000.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"sliding_window"</span><span class="punctuation">:</span> <span class="number">32768</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"tie_word_embeddings"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"torch_dtype"</span><span class="punctuation">:</span> <span class="string">"bfloat16"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"transformers_version"</span><span class="punctuation">:</span> <span class="string">"4.41.2"</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"use_cache"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"use_sliding_window"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vision_config"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">		<span class="attr">"depth"</span><span class="punctuation">:</span> <span class="number">32</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"embed_dim"</span><span class="punctuation">:</span> <span class="number">1280</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"mlp_ratio"</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"num_heads"</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"in_chans"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"hidden_size"</span><span class="punctuation">:</span> <span class="number">1536</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"patch_size"</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"spatial_merge_size"</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"spatial_patch_size"</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"temporal_patch_size"</span><span class="punctuation">:</span> <span class="number">2</span></span><br><span class="line">	<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"rope_scaling"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">		<span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"mrope"</span><span class="punctuation">,</span></span><br><span class="line">		<span class="attr">"mrope_section"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">24</span></span><br><span class="line">		<span class="punctuation">]</span></span><br><span class="line">	<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"vocab_size"</span><span class="punctuation">:</span> <span class="number">151936</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><code>Qwen2VLConfig</code> 与 <code>LlavaConfig</code> 的初始化配置类略有不同</p>
<ul>
<li><code>LlavaConfig</code> 类可以单独接收 <code>vision_config</code>, <code>text_config</code> </li>
<li><code>Qwen2VLConfig</code> 类主要接受语言模型的配置参数，并通过 <code>vision_config</code> 参数嵌套包含视觉模型的配置, 可以直接传入json</li>
</ul>
<p><code>Qwen2VLConfig</code> 接收参数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Qwen2VLConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">	model_type = <span class="string">"qwen2_vl"</span></span><br><span class="line">	keys_to_ignore_at_inference = [<span class="string">"past_key_values"</span>]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self,</span></span><br><span class="line"><span class="params">		vocab_size=<span class="number">152064</span>,</span></span><br><span class="line"><span class="params">		hidden_size=<span class="number">8192</span>,</span></span><br><span class="line"><span class="params">		intermediate_size=<span class="number">29568</span>,</span></span><br><span class="line"><span class="params">		num_hidden_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		num_attention_heads=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">		num_key_value_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">		hidden_act=<span class="string">"silu"</span>,</span></span><br><span class="line"><span class="params">		max_position_embeddings=<span class="number">32768</span>,</span></span><br><span class="line"><span class="params">		initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">		rms_norm_eps=<span class="number">1e-05</span>,</span></span><br><span class="line"><span class="params">		use_cache=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">		tie_word_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		rope_theta=<span class="number">1000000.0</span>,</span></span><br><span class="line"><span class="params">		use_sliding_window=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">		sliding_window=<span class="number">4096</span>,</span></span><br><span class="line"><span class="params">		max_window_layers=<span class="number">80</span>,</span></span><br><span class="line"><span class="params">		attention_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">		vision_config=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		rope_scaling=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">		**kwargs,</span></span><br><span class="line"><span class="params">	</span>):</span><br></pre></td></tr></table></figure></p>
<p><code>Qwen2VLConfig</code> 官方示例:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLForConditionalGeneration, Qwen2VLConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a Qwen2VL style configuration</span></span><br><span class="line">configuration = Qwen2VLConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing a model from the Qwen2-VL-7B style configuration</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(configuration)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accessing the model configuration</span></span><br><span class="line">configuration = model.config</span><br></pre></td></tr></table></figure></p>
<p>自定义配置:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Qwen2VLConfig, Qwen2VLForConditionalGeneration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 配置文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"path/to/your/config.json"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    config_dict = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Qwen2VLConfig 实例</span></span><br><span class="line">qwen2vl_config = Qwen2VLConfig(**config_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化并加载 Qwen2VL 模型</span></span><br><span class="line">model = Qwen2VLForConditionalGeneration(qwen2vl_config)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<ol>
<li><code>generation_config.json</code><br> <code>generation_config.json</code> 在调用生成方法（如 <code>generate()</code>）时被加载, 专门用于定义文本生成过程中的超参数和策略。这些配置项控制生成文本的行为，如生成长度、采样策略、温度、束搜索等, 例如:<ul>
<li><strong>生成长度</strong>：如最大生成长度（<code>max_length</code>）、最小生成长度（<code>min_length</code>）等。</li>
<li><strong>生成策略</strong>：<ul>
<li><strong>采样相关</strong>：如温度（<code>temperature</code>）、顶部K采样（<code>top_k</code>）、顶部P采样（<code>top_p</code>）等。</li>
<li><strong>束搜索</strong>：束宽度（<code>num_beams</code>）、束惩罚因子（<code>repetition_penalty</code>）等。</li>
</ul>
</li>
<li><strong>其他生成参数</strong>：如是否使用核采样（<code>do_sample</code>）、停止标记（<code>eos_token_id</code>）等。<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">	<span class="attr">"bos_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"pad_token_id"</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"do_sample"</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"eos_token_id"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">		<span class="number">151643</span></span><br><span class="line">	<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"repetition_penalty"</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"temperature"</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"top_p"</span><span class="punctuation">:</span> <span class="number">0.001</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"top_k"</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">	<span class="attr">"transformers_version"</span><span class="punctuation">:</span> <span class="string">"4.37.0"</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<p>值得注意的一个地方是, 在仅使用 <code>qwen2vl_config = Qwen2VLConfig(**config_dict)</code> 也就是 config.json 初始化模型的时候, 模型也会有推理参数, 这是 huggingface 源码中 PretrainedConfig 类初始化的时候会给一个默认的参数字典:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_generation_defaults</span>() -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]:</span><br><span class="line">	<span class="keyword">return</span> {</span><br><span class="line">		<span class="string">"max_length"</span>: <span class="number">20</span>,</span><br><span class="line">		<span class="string">"min_length"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"do_sample"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"early_stopping"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"num_beams"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"num_beam_groups"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"diversity_penalty"</span>: <span class="number">0.0</span>,</span><br><span class="line">		<span class="string">"temperature"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"top_k"</span>: <span class="number">50</span>,</span><br><span class="line">		<span class="string">"top_p"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"typical_p"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"repetition_penalty"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"length_penalty"</span>: <span class="number">1.0</span>,</span><br><span class="line">		<span class="string">"no_repeat_ngram_size"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"encoder_no_repeat_ngram_size"</span>: <span class="number">0</span>,</span><br><span class="line">		<span class="string">"bad_words_ids"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"num_return_sequences"</span>: <span class="number">1</span>,</span><br><span class="line">		<span class="string">"output_scores"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"return_dict_in_generate"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"forced_bos_token_id"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"forced_eos_token_id"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"remove_invalid_values"</span>: <span class="literal">False</span>,</span><br><span class="line">		<span class="string">"exponential_decay_length_penalty"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"suppress_tokens"</span>: <span class="literal">None</span>,</span><br><span class="line">		<span class="string">"begin_suppress_tokens"</span>: <span class="literal">None</span>,</span><br><span class="line">	}</span><br></pre></td></tr></table></figure></p>
<ol>
<li><p><code>vocab.json</code><br> <code>vocab.json</code> 文件主要用于定义分词器的词汇表。它包含了模型可以识别和处理的所有词汇（tokens）及其对应的唯一标识符（IDs）。一些特殊tokens标记一般不会出现在这里    </p>
</li>
<li><p><code>tokenizer_config.json</code><br> <code>tokenizer_config.json</code> 文件用于存储分词器的高层配置参数。这些参数影响分词器的行为和处理方式, 如填充方式(<code>padding_side</code>)、添加特殊标记(<code>add_special_tokens</code>)、最大序列长度(<code>model_max_length</code>)等. 但不涉及具体的词汇映射或分词逻辑:</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">	"add_prefix_space": false,</span><br><span class="line">	<span class="string">"added_tokens_decoder"</span>: {</span><br><span class="line">		"<span class="number">151643</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|endoftext|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151644</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|im_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151645</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|im_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151646</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|object_ref_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151647</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|object_ref_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151648</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|box_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151649</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|box_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151650</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|quad_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151651</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|quad_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151652</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_start|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151653</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_end|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151654</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|vision_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151655</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|image_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		},</span><br><span class="line">		"<span class="number">151656</span>": {</span><br><span class="line">			"<span class="attribute">content</span>": <span class="string">"&lt;|video_pad|&gt;"</span>,</span><br><span class="line">			<span class="string">"lstrip"</span>: false,</span><br><span class="line">			<span class="string">"normalized"</span>: false,</span><br><span class="line">			<span class="string">"rstrip"</span>: false,</span><br><span class="line">			<span class="string">"single_word"</span>: false,</span><br><span class="line">			<span class="string">"special"</span>: true</span><br><span class="line">		}</span><br><span class="line">	},</span><br><span class="line">	"additional_special_tokens": [<span class="string">"&lt;|im_start|&gt;"</span>, <span class="string">"&lt;|im_end|&gt;"</span>, <span class="string">"&lt;|object_ref_start|&gt;"</span>,<span class="string">"&lt;|object_ref_end|&gt;"</span>,<span class="string">"&lt;|box_start|&gt;"</span>,<span class="string">"&lt;|box_end|&gt;"</span>,<span class="string">"&lt;|quad_start|&gt;"</span>,<span class="string">"&lt;|quad_end|&gt;"</span>,<span class="string">"&lt;|vision_start|&gt;"</span>,<span class="string">"&lt;|vision_end|&gt;"</span>,<span class="string">"&lt;|vision_pad|&gt;"</span>,<span class="string">"&lt;|image_pad|&gt;"</span>,<span class="string">"&lt;|video_pad|&gt;"</span>],</span><br><span class="line">	<span class="string">"bos_token"</span>: null,</span><br><span class="line">	<span class="string">"chat_template"</span>: <span class="string">"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n{% endif %}&lt;|im_start|&gt;{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}&lt;|im_end|&gt;\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}&lt;|vision_start|&gt;&lt;|video_pad|&gt;&lt;|vision_end|&gt;{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}&lt;|im_end|&gt;\n{% endif %}{% endfor %}{% if add_generation_prompt %}&lt;|im_start|&gt;assistant\n{% endif %}"</span>,</span><br><span class="line">	<span class="string">"clean_up_tokenization_spaces"</span>: false,</span><br><span class="line">	<span class="string">"eos_token"</span>: <span class="string">"&lt;|im_end|&gt;"</span>,</span><br><span class="line">	<span class="string">"padding_side"</span>: <span class="string">"left"</span>,</span><br><span class="line">	<span class="string">"errors"</span>: <span class="string">"replace"</span>,</span><br><span class="line">	<span class="string">"model_max_length"</span>: <span class="number">32768</span>,</span><br><span class="line">	<span class="string">"pad_token"</span>: <span class="string">"&lt;|endoftext|&gt;"</span>,</span><br><span class="line">	<span class="string">"split_special_tokens"</span>: false,</span><br><span class="line">	<span class="string">"tokenizer_class"</span>: <span class="string">"Qwen2Tokenizer"</span>,</span><br><span class="line">	<span class="string">"unk_token"</span>: null</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tokenizer.json</code><br><code>tokenizer.json</code> 是一个综合性文件，通常包含了分词器的完整配置和分词逻辑。它不仅包含 <code>vocab.json</code> 和 <code>tokenizer_config.json</code> 的内容(但<code>tokenizer.json</code>中的add_tokens可能没有<code>tokenizer_config.json</code>中全)，还包括分词器的具体实现细节，如分词合并规则、正则表达式等, 结合 <code>tokenizer_config.json</code> 的内容，提供完整的分词器配置</p>
</li>
</ol>
<h3 id="vocab-json-tokenizer-json-和-tokenizer-config-json"><a href="#vocab-json-tokenizer-json-和-tokenizer-config-json" class="headerlink" title="vocab.json, tokenizer.json 和 tokenizer_config.json"></a><code>vocab.json</code>, <code>tokenizer.json</code> 和 <code>tokenizer_config.json</code></h3><ul>
<li><strong><code>vocab.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>vocab.json</code> 提供了词汇到ID的基础映射，是分词器不可或缺的一部分。</li>
<li><code>tokenizer.json</code> 将 <code>vocab.json</code> 嵌入其中，并结合分词规则（如BPE的合并规则）和行为参数，形成一个完整的分词器定义。</li>
</ul>
</li>
<li><strong><code>tokenizer_config.json</code> 与 <code>tokenizer.json</code></strong>：<ul>
<li><code>tokenizer_config.json</code> 专注于高层次的分词器配置参数，控制分词器的整体行为。</li>
<li><code>tokenizer.json</code> 不仅包含 <code>tokenizer_config.json</code> 的内容，还包括具体的分词逻辑和词汇表，是一个更全面的配置文件。</li>
</ul>
</li>
</ul>
<h2 id="Qwen2-1-5B-config"><a href="#Qwen2-1-5B-config" class="headerlink" title="Qwen2-1.5B config"></a><code>Qwen2-1.5B</code> config</h2><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Qwen2Config {</span><br><span class="line">  "_attn_implementation_autoset": true,</span><br><span class="line">  <span class="string">"_name_or_path"</span>: <span class="string">"/mnt/nas/ianli/models/Qwen2-1.5B"</span>,</span><br><span class="line">  <span class="string">"architectures"</span>: [</span><br><span class="line">    <span class="string">"Qwen2ForCausalLM"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"attention_dropout"</span>: <span class="number">0.0</span>,</span><br><span class="line">  <span class="string">"bos_token_id"</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">"eos_token_id"</span>: <span class="number">151643</span>,</span><br><span class="line">  <span class="string">"hidden_act"</span>: <span class="string">"silu"</span>,</span><br><span class="line">  <span class="string">"hidden_size"</span>: <span class="number">1536</span>,</span><br><span class="line">  <span class="string">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">"intermediate_size"</span>: <span class="number">8960</span>,</span><br><span class="line">  <span class="string">"max_position_embeddings"</span>: <span class="number">131072</span>,</span><br><span class="line">  <span class="string">"max_window_layers"</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">"model_type"</span>: <span class="string">"qwen2"</span>,</span><br><span class="line">  <span class="string">"num_attention_heads"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"num_hidden_layers"</span>: <span class="number">28</span>,</span><br><span class="line">  <span class="string">"num_key_value_heads"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">"rms_norm_eps"</span>: <span class="number">1</span>e-<span class="number">06</span>,</span><br><span class="line">  <span class="string">"rope_scaling"</span>: null,</span><br><span class="line">  <span class="string">"rope_theta"</span>: <span class="number">1000000.0</span>,</span><br><span class="line">  <span class="string">"sliding_window"</span>: null,</span><br><span class="line">  <span class="string">"tie_word_embeddings"</span>: true,</span><br><span class="line">  <span class="string">"torch_dtype"</span>: <span class="string">"bfloat16"</span>,</span><br><span class="line">  <span class="string">"transformers_version"</span>: <span class="string">"4.46.3"</span>,</span><br><span class="line">  <span class="string">"use_cache"</span>: true,</span><br><span class="line">  <span class="string">"use_sliding_window"</span>: false,</span><br><span class="line">  <span class="string">"vocab_size"</span>: <span class="number">151936</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>
<h1 id="关于Qwen2VL的训练构想与需求"><a href="#关于Qwen2VL的训练构想与需求" class="headerlink" title="关于Qwen2VL的训练构想与需求"></a>关于Qwen2VL的训练构想与需求</h1><ol>
<li><strong>网络结构</strong>: 继承 Qwen2VL 的 <code>visual部分</code> 网络结构, 修改Qwen2VL的 <code>语言部分</code> 结构与 <code>陈老师的天文语言模型</code> 结构相一致</li>
<li><strong>Tokenizer</strong>: 注意修改语言模型 <code>tokenizer</code> 中的 <code>special_tokens</code>, 尤其是添加相关视觉的 <code>special_tokens</code> 与 Qwen2VL 保持一致</li>
<li><strong>模型权重</strong>:  <code>visual部分</code> 加载所继承的 Qwen2VL 的视觉权重, <code>语言部分</code> 加载 <code>陈老师的天文语言模型</code> </li>
<li><strong>训练框架</strong>: <ul>
<li>方案一: 将修改完 网络结构 和 参数权重 的模型保存好, 基于现有微调框架进行训练<ul>
<li>问题: 可能存在兼容性的问题, 现在能想到的主要兼容问题: <ul>
<li>现有微调框架对于网络结构的继承</li>
</ul>
</li>
</ul>
</li>
<li>方案二: 训练对于直接使用现有框架而言灵活度要求更高, 同时为了后续可拓展性:<ul>
<li>计划基于 Huggingface 直接搭建训练框架(正在进行)</li>
</ul>
</li>
</ul>
</li>
<li><strong>数据要求</strong>: <ol>
<li>通用图文对<ul>
<li>因为重构了原VL模型的结构和权重, 故而该模型训练还有 视觉 和 语言 对齐的任务存在, 故而需要有大规模高质量的图文数据对支撑</li>
<li>需求(正在调研)</li>
</ul>
</li>
<li>天文图文对<ul>
<li>Apod网站爬虫数据集提供了较高质量了天文图文对</li>
<li>天文微调图文对, 使用目前已有的填空、选择、简答</li>
</ul>
</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/%E5%8D%83%E9%97%AE%E7%9B%B8%E5%85%B3/" data-id="cm5ar5vhg0009ciwicu0xehv5" data-title="千问相关" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-LLaMa系列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2024-12-05T12:10:20.000Z" itemprop="datePublished">2024-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/">LLaMa系列</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-LLaMa系列"><a href="#1-LLaMa系列" class="headerlink" title="1. LLaMa系列"></a>1. LLaMa系列</h1><h2 id="1-1-原始llama"><a href="#1-1-原始llama" class="headerlink" title="1.1 原始llama"></a>1.1 原始llama</h2><h3 id="1-1-1-简介"><a href="#1-1-1-简介" class="headerlink" title="1.1.1 简介"></a>1.1.1 简介</h3><ul>
<li><p><strong>LLama 模型</strong>集合由 <strong>Meta AI</strong> 于 2023 年 2 月推出， 包括四种尺寸(7B 、13B 、30B 和 65B)。上下文长度<strong>2048</strong>.</p>
</li>
<li><p>Meta在2023年7月发布了免费可商用版本 <strong>Llama-2</strong>，有7B、13B、34B和70B四个参数量版本，除了34B模型均已开源。模型的上下文长度从<strong>2048</strong>翻倍到了<strong>4096</strong>.</p>
</li>
<li><p>进入2024年4月18日，Meta发布 <strong>LLama 3</strong>。LLama 3在技术层面实现了重大突破，其最大版本的参数量飙升至超过<strong>405B</strong></p>
</li>
<li><p>7月23日, <strong>LLama3.1</strong>发布, 上下文长度可达128k, 几乎就是一本书的长度了</p>
</li>
</ul>
<h3 id="1-1-2-沿革"><a href="#1-1-2-沿革" class="headerlink" title="1.1.2 沿革"></a>1.1.2 沿革</h3><h4 id="1-1-2-1-模型结构沿革"><a href="#1-1-2-1-模型结构沿革" class="headerlink" title="1.1.2.1 模型结构沿革"></a>1.1.2.1 模型结构沿革</h4><ul>
<li><p><strong>llama1</strong>, 先上图:</p>

<p>值得一讲的模块有<code>RMSNorm</code>, <code>Causal Mask</code>, <code>RoPE</code>, 以及上面的<code>Swi-GLU</code>. 那咱就结合代码一点点搞吧</p>
<ul>
<li><p><strong><code>RMSNorm</code></strong> 其主要基于<code>LayerNorm</code>移除了re-center操作, 并且通过实验证明之做到了“降本不降效”(计算时间减少7%-64%不等). 具体公式如下, RMS就是均方误差根:<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.654ex;" xmlns="http://www.w3.org/2000/svg" width="20.343ex" height="4.023ex" role="img" focusable="false" viewBox="0 -1047.1 8991.6 1778.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(1823.6,0)"><g data-mml-node="mrow" transform="translate(866.6,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1350,0)"><g data-mml-node="mi"><path data-c="1D53C" d="M12 666Q12 675 24 683H582Q590 680 593 672V588Q593 514 591 502T575 490Q567 490 563 495T555 517Q552 556 517 590Q486 623 445 634T340 648H282Q266 636 264 620T260 492V370H277Q329 375 358 391T404 439Q420 480 420 506Q420 529 436 529Q445 529 451 521Q455 517 455 361Q455 333 455 298T456 253Q456 217 453 207T437 197Q420 196 420 217Q420 240 406 270Q377 328 284 335H260V201Q261 174 261 134Q262 73 264 61T278 38Q281 36 282 35H331Q400 35 449 50Q571 93 602 179Q605 203 622 203Q629 203 634 197T640 183Q638 181 624 95T604 3L600 -1H24Q12 5 12 16Q12 35 51 35Q92 38 97 52Q102 60 102 341T97 632Q91 645 51 648Q12 648 12 666ZM137 341Q137 131 136 89T130 37Q129 36 129 35H235Q233 41 231 48L226 61V623L231 635L235 648H129Q132 641 133 638T135 603T137 517T137 341ZM557 603V648H504Q504 646 515 639Q527 634 542 619L557 603ZM420 317V397L406 383Q394 370 380 363L366 355Q373 350 382 346Q400 333 409 328L420 317ZM582 61L586 88Q585 88 582 83Q557 61 526 46L511 37L542 35H577Q577 36 578 39T580 49T582 61Z"></path></g></g><g data-mml-node="mo" transform="translate(2017,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2406,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2978,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="msqrt" transform="translate(220,-531.2) scale(0.707)"><g transform="translate(1020,0)"><g data-mml-node="mtext"><path data-c="56" d="M114 620Q113 621 110 624T107 627T103 630T98 632T91 634T80 635T67 636T48 637H19V683H28Q46 680 152 680Q273 680 294 683H305V637H284Q223 634 223 620Q223 618 313 372T404 126L490 358Q575 588 575 597Q575 616 554 626T508 637H503V683H512Q527 680 627 680Q718 680 724 683H730V637H723Q648 637 627 596Q627 595 515 291T401 -14Q396 -22 382 -22H374H367Q353 -22 348 -14Q346 -12 231 303Q114 617 114 620Z"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(750,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1250,0)"></path></g><g data-mml-node="mo" transform="translate(1642,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2031,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2603,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2992,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3770,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g><g data-mml-node="mo" transform="translate(0,49.9)"><path data-c="221A" d="M263 249Q264 249 315 130T417 -108T470 -228L725 302Q981 837 982 839Q989 850 1001 850Q1008 850 1013 844T1020 832V826L741 243Q645 43 540 -176Q479 -303 469 -324T453 -348Q449 -350 436 -350L424 -349L315 -96Q206 156 205 156L171 130Q138 104 137 104L111 130L263 249Z"></path></g><rect width="4176" height="42.4" x="1020" y="857.5"></rect></g><rect width="3874.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6159.9,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(6660.1,0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mo" transform="translate(7425.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(8425.6,0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.304ex;" xmlns="http://www.w3.org/2000/svg" width="40.008ex" height="4.208ex" role="img" focusable="false" viewBox="0 -1283.6 17683.7 1860"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(264.5,3) translate(-250 0)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1133.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2189.5,0)"><g data-mml-node="msub" transform="translate(1160.5,451.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-370.3) scale(0.707)"><g data-mml-node="mtext"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(736,0)"></path><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z" transform="translate(1653,0)"></path></g><g data-mml-node="mo" transform="translate(2209,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2598,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(3127,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="2686.2" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(5115.7,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5919.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mstyle" transform="translate(6197.6,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mtext" transform="translate(7364.3,0)"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path data-c="4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z" transform="translate(736,0)"></path><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z" transform="translate(1653,0)"></path></g><g data-mml-node="mo" transform="translate(9573.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9962.3,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(10491.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(11158.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msqrt" transform="translate(12213.9,0)"><g transform="translate(1020,0)"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(255.4,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(1030.9,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msubsup" transform="translate(3484.2,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,353.6) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(562,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,73.6)"><path data-c="221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"></path></g><rect width="4449.8" height="60" x="1020" y="1163.6"></rect></g></g></g></svg></mjx-container></p>
<p>现在来看看代码, <code>LN</code>就不看了, 只看<code>RMSNorm</code>的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d, p=-<span class="number">1.</span>, eps=<span class="number">1e-8</span>, bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Root Mean Square Layer Normalization</span></span><br><span class="line"><span class="string">        :param d: model size</span></span><br><span class="line"><span class="string">        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)</span></span><br><span class="line"><span class="string">        :param eps:  epsilon value, default 1e-8</span></span><br><span class="line"><span class="string">        :param bias: whether use bias term for RMSNorm, disabled by</span></span><br><span class="line"><span class="string">            default because RMSNorm doesn't enforce re-centering invariance.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>(RMSNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.d = d</span><br><span class="line">        <span class="variable language_">self</span>.p = p</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(d))</span><br><span class="line">        <span class="variable language_">self</span>.register_parameter(<span class="string">"scale"</span>, <span class="variable language_">self</span>.scale)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="variable language_">self</span>.offset = nn.Parameter(torch.zeros(d))</span><br><span class="line">            <span class="variable language_">self</span>.register_parameter(<span class="string">"offset"</span>, <span class="variable language_">self</span>.offset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.p &lt; <span class="number">0.</span> <span class="keyword">or</span> <span class="variable language_">self</span>.p &gt; <span class="number">1.</span>:</span><br><span class="line">            norm_x = x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = <span class="variable language_">self</span>.d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            partial_size = <span class="built_in">int</span>(<span class="variable language_">self</span>.d * <span class="variable language_">self</span>.p)</span><br><span class="line">            partial_x, _ = torch.split(x, [partial_size, <span class="variable language_">self</span>.d - partial_size], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            norm_x = partial_x.norm(<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            d_x = partial_size</span><br><span class="line"></span><br><span class="line">        rms_x = norm_x * d_x ** (-<span class="number">1.</span> / <span class="number">2</span>)</span><br><span class="line">        x_normed = x / (rms_x + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bias:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.scale * x_normed + <span class="variable language_">self</span>.offset</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.scale * x_normed</span><br></pre></td></tr></table></figure>
<p>值得注意在实际应用中还多了一个<code>partial</code>选项, 另一半被partial掉的就不参与计算了, 怀疑一个深意是实验结果表明RMS没必要在token的d维度全部(换言之所有头)做计算. <strong>下去琢磨琢磨</strong> </p>
</li>
<li><p><code>causal_mask</code>(顺道提一嘴BERT里的<code>label=-100</code>就是不参与交叉熵损失的意思)</p>
<ul>
<li>上图中<code>causal_mask</code>实际上是一个shape为<code>num_tokens_per_sample</code>*<code>num_tokens_per_sample</code>的下三角矩阵, 下三角的值你既可以填<code>-10000</code>也可以填<code>-inf</code>.</li>
<li>这种 <strong>causal mask</strong>真的十分巧妙, 每个token只能看到它后面的token了, 让decoder-only结构的模型训练如此高效!</li>
<li>具体而言,</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">q = <span class="variable language_">self</span>.query(x)</span><br><span class="line">k = <span class="variable language_">self</span>.key(x)</span><br><span class="line">v = <span class="variable language_">self</span>.value(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头</span></span><br><span class="line">q = rearrange(q, <span class="string">'b p (n_he d_h) -&gt; b n_he p d_h'</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line">k = rearrange(k, <span class="string">'b p (n_he d_h) -&gt; b n_he p d_h'</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line">v = rearrange(v, <span class="string">'b p (n_he d_h) -&gt; b n_he p d_h'</span>, n_he = <span class="variable language_">self</span>.num_heads, d_h = <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">qk = torch.matmul(q, k.transpose(-<span class="number">1</span>,-<span class="number">2</span>))*mask</span><br><span class="line"></span><br><span class="line">qk = F.softmax(qk, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">qkv = torch.matmul(qk, v.transpose(-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rearrange(qkv, <span class="string">'b n_he p d_h -&gt; b p (n_he d_h)'</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>RoPE</code>, 一图胜千言:</p>


</li>
<li><p><code>Swi-GLU</code>, 顺便复习下glu</p>

<blockquote>
<p>GLU(Gated Linear Units,门控线性单元)引入了两个不同的线性层，其中一个首先经过sigmoid函数，其结果将和另一个线性层的输出进行逐元素相乘作为最终的输出<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="42.802ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 18918.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="47" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q401 658 376 654T316 633T254 592T205 519T177 411Q173 369 173 335Q173 259 192 201T238 111T302 58T370 31T431 24Q478 24 513 45T559 100Q562 110 562 160V212Q561 213 557 216T551 220T542 223T526 225T502 226T463 227H437V273H449L609 270Q715 270 727 273H735V227H721Q674 227 668 215Q666 211 666 108V6Q660 0 657 0Q653 0 639 10Q617 25 600 42L587 54Q571 27 524 3T406 -22Q317 -22 238 22T108 151T56 342Z"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(785,0)"></path><path data-c="55" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H302Q262 636 251 634T233 622L232 418V291Q232 189 240 145T280 67Q325 24 389 24Q454 24 506 64T571 183Q575 206 575 410V598Q569 608 565 613T541 627T489 637H472V683H481Q496 680 598 680T715 683H724V637H707Q634 633 622 598L621 399Q620 194 617 180Q617 179 615 171Q595 83 531 31T389 -22Q304 -22 226 33T130 192Q129 201 128 412V622Z" transform="translate(1410,0)"></path></g><g data-mml-node="mo" transform="translate(2160,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2549,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3121,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3565.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(4613.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5058.3,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(5827.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(6272,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(6701,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7145.7,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(7578.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8245.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(9301.2,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(9872.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10261.2,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(10833.2,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(12103.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(13103.7,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(13532.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(14143.9,0)"><path data-c="2297" d="M56 250Q56 394 156 488T384 583Q530 583 626 485T722 250Q722 110 625 14T390 -83Q249 -83 153 14T56 250ZM582 471Q531 510 496 523Q446 542 381 542Q324 542 272 519T196 471L389 278L485 375L582 471ZM167 442Q95 362 95 250Q95 137 167 58L359 250L167 442ZM610 58Q682 138 682 250Q682 363 610 442L418 250L610 58ZM196 29Q209 16 230 2T295 -27T388 -42Q409 -42 429 -40T465 -33T496 -23T522 -11T544 1T561 13T574 22T582 29L388 222L196 29Z"></path></g><g data-mml-node="mo" transform="translate(15144.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(15533.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(16105.1,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(17096.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(18096.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(18529.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
</blockquote>
<p>SiLU就不用说了,<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.058ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4003.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(794.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(1516.4,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2087.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2476.4,0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mi" transform="translate(3042.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3614.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>, 经典激活函数</p>
<p><code>up</code>和<code>down</code>一般是四倍的上下采样线形层. </p>
</li>
</ul>
</li>
<li><p><strong>llama2</strong><br>  核心就是引入了<strong>GQA</strong>,见下图:</p>
  
<p>  虽然现在的算力, 一个sample 4096个token根本算不上什么, 但还是一讲, 先上图:</p>
  
<ul>
<li>GQA-1：一个单独的组，等同于 Multi-Query Attention (MQA)。</li>
<li>GQA-H：组数等于头数，基本上与 Multi-Head Attention (MHA) 相同。</li>
<li><p>GQA-G：一个中间配置，具有G个组，平衡了效率和表达能力。</p>
<p>代码也没必要放了, 已加入torch全家桶, 就是把<code>v</code>,<code>k</code>用<code>torch.chunk</code> 了一下, 不过我们有必要顺带讲一嘴滑动窗口注意力:</p>
<blockquote>
<p>每一个token只和包含其本身在内的前W个token做Attention。最简单的实现其实就是给不需要计算attention的其它token都加上一个mask就可以了</p>
</blockquote>
<p>一图胜千言:</p>

</li>
</ul>
</li>
<li><p><strong>llama3</strong><br>似乎在结构上没有大的创新(这就是scaling的威力吗…), 不过引入了“<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fairscale/blob/main/fairscale/nn/model_parallel/layers.py">parallel layers from Fairscale</a>”去并行加速矩阵运算, <strong>有时间看看,并加到这或者单开一章</strong></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/05/LLaMa%E7%B3%BB%E5%88%97/" data-id="cm5ar5vhe0002ciwi62iv95sy" data-title="LLaMa系列" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/llm/" rel="tag">llm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Magvit系列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2024-11-23T04:38:26.000Z" itemprop="datePublished">2024-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/11/23/Magvit%E7%B3%BB%E5%88%97/">Open-MAGVIT-2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="MAGVIT1-amp-2-Google-amp-腾讯的实现"><a href="#MAGVIT1-amp-2-Google-amp-腾讯的实现" class="headerlink" title="MAGVIT1&2(Google&腾讯的实现)"></a>MAGVIT1&amp;2(Google&amp;腾讯的实现)</h1><h2 id="MAGVIT-v1-Google"><a href="#MAGVIT-v1-Google" class="headerlink" title="MAGVIT-v1 (Google)"></a>MAGVIT-v1 (Google)</h2><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a target="_blank" rel="noopener" href="https://github.com/google-research/magvit">源码地址</a></p>
<p>这是个20年的框架,作者主要用了jax. 所以还是得调研下jax有什么好处, 以及现在生态怎么样了. (<strong>先码住, 以后填坑</strong>)</p>
<h4 id="VQ-tokenizer部分"><a href="#VQ-tokenizer部分" class="headerlink" title="VQ-tokenizer部分"></a>VQ-tokenizer部分</h4>
<p>其结构图如上, 下面上代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQVAE</span>(nn.Module):</span><br><span class="line">  <span class="string">"""VQ-VAE model."""</span></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  precision: <span class="type">Any</span> = jax.lax.Precision.DEFAULT</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">"""VQ-VAE setup."""</span></span><br><span class="line">    quantizer_str = <span class="variable language_">self</span>.config.vqvae.get(</span><br><span class="line">        <span class="string">'vector_quantizer_class'</span>, <span class="string">'VectorQuantizer'</span>)</span><br><span class="line">    <span class="keyword">if</span> quantizer_str == <span class="string">'VectorQuantizer'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.quantizer = VectorQuantizer(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, precision=<span class="variable language_">self</span>.precision, dtype=<span class="variable language_">self</span>.dtype</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(quantizer_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Encoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = model_utils.vmap_t_dim(enc_dec_2dcnn.Decoder)(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'3dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_3dcnn.Encoder(config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_3dcnn.Decoder(config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.architecture == <span class="string">'2plus1dcnn'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.encoder = enc_dec_2plus1dcnn.Encoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">      <span class="variable language_">self</span>.decoder = enc_dec_2plus1dcnn.Decoder(</span><br><span class="line">          config=<span class="variable language_">self</span>.config, output_dim=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">          <span class="string">f'Architecture <span class="subst">{self.config.vqvae.architecture}</span>'</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>先分析<code>enc_dec_3dcnn.Encoder</code>和<code>enc_dec_3dcnn.Decoder</code>这俩:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Encoder Blocks."""</span></span><br><span class="line"></span><br><span class="line">  config: ml_collections.ConfigDict</span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="variable language_">self</span>.filters = <span class="variable language_">self</span>.config.vqvae.filters</span><br><span class="line">    <span class="variable language_">self</span>.num_res_blocks = <span class="variable language_">self</span>.config.vqvae.num_enc_res_blocks</span><br><span class="line">    <span class="variable language_">self</span>.channel_multipliers = <span class="variable language_">self</span>.config.vqvae.channel_multipliers</span><br><span class="line">    <span class="variable language_">self</span>.temporal_downsample = <span class="variable language_">self</span>.config.vqvae.temporal_downsample</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.temporal_downsample, <span class="built_in">int</span>):</span><br><span class="line">      <span class="variable language_">self</span>.temporal_downsample = _get_selected_flags(</span><br><span class="line">          <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers) - <span class="number">1</span>, <span class="variable language_">self</span>.temporal_downsample, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>.embedding_dim = <span class="variable language_">self</span>.config.vqvae.embedding_dim</span><br><span class="line">    <span class="variable language_">self</span>.conv_downsample = <span class="variable language_">self</span>.config.vqvae.conv_downsample</span><br><span class="line">    <span class="variable language_">self</span>.custom_conv_padding = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'custom_conv_padding'</span>)</span><br><span class="line">    <span class="variable language_">self</span>.norm_type = <span class="variable language_">self</span>.config.vqvae.norm_type</span><br><span class="line">    <span class="variable language_">self</span>.num_remat_block = <span class="variable language_">self</span>.config.vqvae.get(<span class="string">'num_enc_remat_blocks'</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'relu'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.relu</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.config.vqvae.activation_fn == <span class="string">'swish'</span>:</span><br><span class="line">      <span class="variable language_">self</span>.activation_fn = nn.swish</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>从其setup来看, 也是resnet作encoder-decoder那一套. 从下述代码, 我们可以大概归纳出一个层级结构:</p>
<pre><code>Encoder
|---conv_fn
|    |
|---Block (Before)
|    |----ResBlock * N
|    |----conv_downsample
|---Block (last)
|    |----ResBlock * N
|---norm-act-conv_fn
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@nn.compact</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, *, is_train=<span class="literal">False</span></span>):</span><br><span class="line">  conv_fn = functools.partial(</span><br><span class="line">      model_utils.Conv,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">      custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line">  norm_fn = model_utils.get_norm_layer(</span><br><span class="line">      norm_type=<span class="variable language_">self</span>.norm_type, dtype=<span class="variable language_">self</span>.dtype)</span><br><span class="line">  block_args = <span class="built_in">dict</span>(</span><br><span class="line">      norm_fn=norm_fn,</span><br><span class="line">      conv_fn=conv_fn,</span><br><span class="line">      dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">      activation_fn=<span class="variable language_">self</span>.activation_fn,</span><br><span class="line">      use_conv_shortcut=<span class="literal">False</span>,</span><br><span class="line">  )</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">  filters = <span class="variable language_">self</span>.filters</span><br><span class="line">  num_blocks = <span class="built_in">len</span>(<span class="variable language_">self</span>.channel_multipliers)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks):</span><br><span class="line">    filters = <span class="variable language_">self</span>.filters * <span class="variable language_">self</span>.channel_multipliers[i]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">      <span class="keyword">if</span> i &lt; <span class="variable language_">self</span>.num_remat_block <span class="keyword">and</span> is_train:</span><br><span class="line">        x = ResBlock(filters, **block_args).remat_call(x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = ResBlock(filters, **block_args)(x)</span><br><span class="line">    <span class="keyword">if</span> i &lt; num_blocks - <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.conv_downsample:</span><br><span class="line">        t_stride = <span class="number">2</span> <span class="keyword">if</span> <span class="variable language_">self</span>.temporal_downsample[i] <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        x = conv_fn(</span><br><span class="line">            filters, kernel_size=(<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>), strides=(t_stride, <span class="number">2</span>, <span class="number">2</span>))(</span><br><span class="line">                x)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x = model_utils.downsample(x, <span class="variable language_">self</span>.temporal_downsample[i])</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">    x = ResBlock(filters, **block_args)(x)</span><br><span class="line">  x = norm_fn()(x)</span><br><span class="line">  x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">  x = conv_fn(<span class="variable language_">self</span>.embedding_dim, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>核心就在于它的ResBlock怎么写的,不过应该大差不差.</p>
<pre><code>ResBlock
|
|---norm-act-conv_fn(3x3x3, stride=1)
|    |
|---norm-act-conv_fn(3x3x3, stride=1)
</code></pre><p>在细看<code>ResBlock</code>前,紧急插播一下<code>conv_fn</code>的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">conv_fn = functools.partial(</span><br><span class="line">    model_utils.Conv,</span><br><span class="line">    dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">    padding=<span class="string">'VALID'</span> <span class="keyword">if</span> <span class="variable language_">self</span>.custom_conv_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'SAME'</span>,</span><br><span class="line">    custom_padding=<span class="variable language_">self</span>.custom_conv_padding)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(nn.Conv):</span><br><span class="line">  <span class="string">"""Convolution with custom padding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Attributes:</span></span><br><span class="line"><span class="string">    custom_padding: padding mode accepted by jnp.pad. When using this, must set</span></span><br><span class="line"><span class="string">      padding=VALID to disable padding in nn.Conv.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  custom_padding: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.padding == <span class="string">'VALID'</span>, <span class="string">'Must use VALID padding for raw Conv.'</span></span><br><span class="line">      <span class="keyword">assert</span> <span class="variable language_">self</span>.kernel_dilation <span class="keyword">in</span> (<span class="number">1</span>, <span class="literal">None</span>), <span class="string">'Kernel dilation not supported.'</span></span><br><span class="line">      pads = [((k - <span class="number">1</span>) // <span class="number">2</span>, k // <span class="number">2</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="variable language_">self</span>.kernel_size]</span><br><span class="line">      pads = [(<span class="number">0</span>, <span class="number">0</span>)] + pads + [(<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.custom_padding.startswith(</span><br><span class="line">          <span class="string">'reflect_'</span>) <span class="keyword">or</span> <span class="variable language_">self</span>.custom_padding.startswith(<span class="string">'symmetric_'</span>):</span><br><span class="line">        custom_padding, reflect_type = <span class="variable language_">self</span>.custom_padding.split(<span class="string">'_'</span>)</span><br><span class="line">        pad_kwargs = {<span class="string">'reflect_type'</span>: reflect_type}</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        custom_padding = <span class="variable language_">self</span>.custom_padding</span><br><span class="line">        pad_kwargs = {}</span><br><span class="line">      x = jnp.pad(x, pads, mode=custom_padding, **pad_kwargs)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">super</span>(Conv, <span class="variable language_">self</span>).__call__(x)</span><br></pre></td></tr></table></figure>
<p>下面我们来看ResBlock的定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">  <span class="string">"""Basic Residual Block."""</span></span><br><span class="line">  filters: <span class="built_in">int</span></span><br><span class="line">  norm_fn: <span class="type">Any</span></span><br><span class="line">  conv_fn: <span class="type">Any</span></span><br><span class="line">  dtype: <span class="built_in">int</span> = jnp.float32</span><br><span class="line">  activation_fn: <span class="type">Any</span> = nn.relu</span><br><span class="line">  use_conv_shortcut: <span class="built_in">bool</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">    input_dim = x.shape[-<span class="number">1</span>]</span><br><span class="line">    residual = x</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.norm_fn()(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.activation_fn(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.conv_fn(<span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> input_dim != <span class="variable language_">self</span>.filters:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.use_conv_shortcut:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        residual = <span class="variable language_">self</span>.conv_fn(</span><br><span class="line">            <span class="variable language_">self</span>.filters, kernel_size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), use_bias=<span class="literal">False</span>)(</span><br><span class="line">                residual)</span><br><span class="line">    <span class="keyword">return</span> x + residual</span><br></pre></td></tr></table></figure>
<p>由上可见,似乎没有用瓶颈层, 而且有个问题: <strong>input_dim != self.filters:</strong>时不应该直接报错的吗? 答案是不会的, <code>self.filters</code>是out_channel, 至于in_channel, 会自动适应的.</p>
<h4 id="MLM-Masked-Language-Model-部分"><a href="#MLM-Masked-Language-Model-部分" class="headerlink" title="MLM(Masked Language Model)部分"></a>MLM(Masked Language Model)部分</h4><p>很惭愧,我找了半天没找到他们的template command line, 因此只能猜 <code>videogvt/trainers/maskgvt_trainer.py</code> 和 <code>videogvt/trainers/lmgvt_trainer.py</code>是训练脚本.</p>
<p>先看<code>maskgvt_trainer.py</code> </p>
<p>太复杂了,咱就记住<code>flax_model</code>是bert, 最重要的是<code>train_step</code>就行了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_loss_fn</span>(<span class="params">params</span>):</span><br><span class="line">  variables = {<span class="string">'params'</span>: params, **train_state.model_state}</span><br><span class="line">  logits, new_model_state = flax_model.apply(</span><br><span class="line">      variables,</span><br><span class="line">      batch_tokens[<span class="string">'masked_inputs'</span>],</span><br><span class="line">      batch_tokens[<span class="string">'segment_ids'</span>],</span><br><span class="line">      deterministic=<span class="literal">False</span>,</span><br><span class="line">      mutable=mutable,</span><br><span class="line">      rngs={<span class="string">'dropout'</span>: dropout_rng})</span><br><span class="line">  <span class="comment"># logits shape [bs, 1 + (l_cond) + l_t * l_h * l_w,</span></span><br><span class="line">  <span class="comment">#               vq_codebook_size + num_classes + num_special_tokens]</span></span><br><span class="line">  logits = logits[:, :, :vq_codebook_size]</span><br><span class="line">  <span class="comment"># TODO(roadjiang): add classification loss. Use batch_tokens['weights'][0]</span></span><br><span class="line">  one_hot_targets = jax.nn.one_hot(batch_tokens[<span class="string">'targets'</span>], logits.shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  sof_ce_loss = (</span><br><span class="line">      scenic_model_utils.weighted_unnormalized_softmax_cross_entropy(</span><br><span class="line">          logits,</span><br><span class="line">          one_hot_targets,</span><br><span class="line">          weights=batch.get(<span class="string">'batch_mask'</span>),</span><br><span class="line">          label_smoothing=config.get(<span class="string">'label_smoothing'</span>),</span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  weights = batch_tokens[<span class="string">'weights'</span>]</span><br><span class="line">  masked_sof_ce_loss = jnp.<span class="built_in">sum</span>(</span><br><span class="line">      sof_ce_loss * weights, axis=-<span class="number">1</span>) / (</span><br><span class="line">          jnp.<span class="built_in">sum</span>(weights, axis=-<span class="number">1</span>) + <span class="number">1e-8</span>)</span><br><span class="line">  token_loss = jnp.mean(masked_sof_ce_loss)</span><br><span class="line"></span><br><span class="line">  l2_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> config.get(<span class="string">'l2_decay_factor'</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    total_loss = token_loss</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    l2_loss = scenic_model_utils.l2_regularization(params)</span><br><span class="line">    total_loss = token_loss + <span class="number">0.5</span> * config.l2_decay_factor * l2_loss</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> total_loss, (new_model_state, token_loss, l2_loss)</span><br></pre></td></tr></table></figure>
<p>LM那就是BERT, 看一看<code>videogvt/models/simplified_bert.py</code>就行了. 注意,人家的logits不是分类头分出来的,而是和词表乘出来的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertMlmLayer</span>(nn.Module):</span><br><span class="line">  <span class="string">"""BERT layer for masked token prediction."""</span></span><br><span class="line"></span><br><span class="line">  hidden_size: <span class="built_in">int</span></span><br><span class="line">  initializer_fn: InitializerType</span><br><span class="line"></span><br><span class="line"><span class="meta">  @nn.compact</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params"></span></span><br><span class="line"><span class="params">      self, last_layer: jnp.ndarray, embeddings: jnp.ndarray</span></span><br><span class="line"><span class="params">  </span>) -&gt; jnp.ndarray:</span><br><span class="line">    mlm_hidden = nn.Dense(</span><br><span class="line">        features=<span class="variable language_">self</span>.hidden_size,</span><br><span class="line">        kernel_init=<span class="variable language_">self</span>.initializer_fn,</span><br><span class="line">        name=<span class="string">'mlm_dense'</span>,</span><br><span class="line">    )(last_layer)</span><br><span class="line">    mlm_hidden = jax.nn.gelu(mlm_hidden)</span><br><span class="line">    mlm_hidden = nn.LayerNorm(epsilon=TF_LAYERNORM_EPSILON, name=<span class="string">'mlm_ln'</span>)(</span><br><span class="line">        mlm_hidden</span><br><span class="line">    )</span><br><span class="line">    output_weights = jnp.transpose(embeddings)</span><br><span class="line">    logits = jnp.matmul(mlm_hidden, output_weights)</span><br><span class="line">    logits = Bias(name=<span class="string">'mlm_bias'</span>)(logits)</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h2 id="MAGVIT-v2-Google-proposed-Tencent-implementation"><a href="#MAGVIT-v2-Google-proposed-Tencent-implementation" class="headerlink" title="MAGVIT-v2 (Google proposed Tencent implementation)"></a>MAGVIT-v2 (Google proposed Tencent implementation)</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><h4 id="VQ部分"><a href="#VQ部分" class="headerlink" title="VQ部分"></a>VQ部分</h4><p>咱就是说,这代码都大差不差. 不过<code>use_gan</code>设为<code>True</code>后, 很容易<code>disc_loss</code>loss就是<code>nan</code>, 目前正在排查</p>
<h4 id="MLM部分"><a href="#MLM部分" class="headerlink" title="MLM部分"></a>MLM部分</h4><p>他们尚未发布</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/23/Magvit%E7%B3%BB%E5%88%97/" data-id="cm5ar5vhg0007ciwi29ruge1x" data-title="Open-MAGVIT-2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LFQ探究" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/" class="article-date">
  <time class="dt-published" datetime="2024-11-23T01:21:21.000Z" itemprop="datePublished">2024-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/">LFQ探究</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="LFQ-Look-up-free-quantization-探究"><a href="#LFQ-Look-up-free-quantization-探究" class="headerlink" title="LFQ(Look up free quantization)探究"></a>LFQ(Look up free quantization)探究</h1><ul>
<li><p><strong>怎么quantize的, token是啥</strong></p>
<p>原文中: </p>
<blockquote>
<p>Specifically, the latent space of LFQ is decomposed as the Cartesian product of single-dimensional variables, as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.804ex;" xmlns="http://www.w3.org/2000/svg" width="12.962ex" height="3.1ex" role="img" focusable="false" viewBox="0 -1015 5729.3 1370.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2102" d="M684 131Q684 125 672 109T633 71T573 29T489 -5T386 -19Q330 -19 276 -3T174 46T91 134T44 261Q39 283 39 341T44 421Q66 538 143 611T341 699Q344 699 364 700T395 701Q449 698 503 677T585 655Q603 655 611 662T620 678T625 694T639 702Q650 702 657 690V481L653 474Q640 467 628 472Q624 476 618 496T595 541Q562 587 507 625T390 663H381Q337 663 299 625Q212 547 212 336Q212 249 233 179Q274 30 405 30Q533 30 641 130Q658 147 666 147Q671 147 677 143T684 131ZM250 625Q264 643 261 643Q238 635 214 620T161 579T110 510T79 414Q74 384 74 341T79 268Q89 213 113 169T164 101T217 61T260 39L277 34Q270 41 264 48Q199 111 181 254Q178 281 178 344T181 434Q200 559 250 625ZM621 565V625Q617 623 613 623Q603 619 590 619H575L588 605Q608 583 610 579L621 565Z"></path></g></g><g data-mml-node="mo" transform="translate(999.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msubsup" transform="translate(2055.6,0)"><g data-mml-node="mo"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="TeXAtom" transform="translate(811,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(811,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4875.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"></path></g></g><g data-mml-node="mi" transform="translate(560,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p>
</blockquote>
<p>  笛卡尔积啥意思呢, 就是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.014ex;" xmlns="http://www.w3.org/2000/svg" width="24.847ex" height="2.711ex" role="img" focusable="false" viewBox="0 -750 10982.5 1198.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(849,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1314,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(1980.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3036.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3425.6,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(748,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(4577.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5021.8,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(748,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6173.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(6618,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(7956.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(8401.3,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(748,-237.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msubsup" transform="translate(783,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,353.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mn" transform="translate(510,-297.3) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(10593.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>,其中 K 是词表大小.</p>
<p>  进一步做简化,我们只让C从{-1,1}里取值,即原文中:</p>
<blockquote>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="41.282ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 18246.5 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(849,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1641,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2307.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mtext" transform="translate(3363.5,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(394,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(672,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1172,0)"></path></g><g data-mml-node="mo" transform="translate(5091.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(5480.5,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6272.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6939.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(7995,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(8773,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(9273,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(9773,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(10842.7,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mn" transform="translate(11898.5,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(12398.5,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g><g data-mml-node="mo" transform="translate(13120.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(14121,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(14621,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(15121,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(16190.7,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(17246.5,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(17746.5,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.804ex;" xmlns="http://www.w3.org/2000/svg" width="70.208ex" height="3.1ex" role="img" focusable="false" viewBox="0 -1015 31032 1370.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(361,0)"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(917,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1473,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1917,0)"></path></g><g data-mml-node="mo" transform="translate(2445,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2834,0)"><g data-mml-node="mi"><path data-c="1D433" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"></path></g></g><g data-mml-node="mo" transform="translate(3345,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4011.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(5067.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(8332.1,0)"><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(500,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(892,0)"></path></g><g data-mml-node="mo" transform="translate(9724.1,0)"><path data-c="2061" d=""></path></g><g data-mml-node="munder" transform="translate(9890.7,0)"><g data-mml-node="mo"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)"></path></g><g data-mml-node="mi" transform="translate(1700,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(12175.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(12453.8,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(13468,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(14468.2,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(748,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(16075.1,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="munderover" transform="translate(16519.8,0)"><g data-mml-node="mo"><path data-c="220F" d="M158 656Q147 684 131 694Q110 707 69 710H55V750H888V710H874Q840 708 820 698T795 678T786 656V-155Q798 -206 874 -210H888V-250H570V-210H584Q618 -208 638 -197T663 -178T673 -155V710H270V277L271 -155Q283 -206 359 -210H373V-250H55V-210H69Q103 -208 123 -197T148 -178T158 -155V656Z"></path></g><g data-mml-node="TeXAtom" transform="translate(977,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(977,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(429,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1207,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g><g data-mml-node="mo" transform="translate(18920.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(19198.5,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g><g data-mml-node="mo" transform="translate(20299.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(20855.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(21911.4,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,524.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mn" transform="translate(1311,-241.4) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1714.6,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1881.2,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msup" transform="translate(25175.9,0)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(533,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mn" transform="translate(26906.5,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(27406.5,0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(27906.5,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(28976.3,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(30032,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(30532,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></p>
</blockquote>
<p>  这就是look up free, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.853ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1703 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(849,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1314,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>并非我的“embedding”, 我从codebook查值的时候也无需像以前一样“一一”比对了.</p>
<p>  <strong>以上我们解决了怎么quantize的</strong></p>
<p>  至于<strong>token是啥</strong>, 我们就得结合源代码了, 毕竟文章中说的不清不楚, 总不可能纬度真的就只是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.651ex;" xmlns="http://www.w3.org/2000/svg" width="4.461ex" height="2.565ex" role="img" focusable="false" viewBox="0 -846 1971.6 1133.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msubsup" transform="translate(783,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mn" transform="translate(510,-287.9) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>, 不然哪来的词表?</p>
<p>  一个合理的猜测是, codebook是像nlp里的vocabulary一样是单独初始化的, 但是这样其实面临个实践中我们遇到的问题: 一旦将learnable_codebook设置为True, 且code_dim设置为百维以上时, 模型很难收敛……</p>
<p>  所以,<strong>上源码</strong>(主要看forward部分):</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 源码, from https://github.com/lucidrains/vector-quantize-pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    x,</span></span><br><span class="line"><span class="params">    inv_temperature = <span class="number">100.</span>,</span></span><br><span class="line"><span class="params">    return_loss_breakdown = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    mask = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    einstein notation</span></span><br><span class="line"><span class="string">    b - batch</span></span><br><span class="line"><span class="string">    n - sequence (or flattened spatial dimensions)</span></span><br><span class="line"><span class="string">    d - feature dimension, which is also log2(codebook size)</span></span><br><span class="line"><span class="string">    c - number of codebook dim</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    is_img_or_video = x.ndim &gt;= <span class="number">4</span></span><br><span class="line">    should_transpose = default(<span class="variable language_">self</span>.channel_first, is_img_or_video)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># standardize image or video into (batch, seq, dimension)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> should_transpose:</span><br><span class="line">        x = rearrange(x, <span class="string">'b d ... -&gt; b ... d'</span>)</span><br><span class="line">        x, ps = pack_one(x, <span class="string">'b * d'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> x.shape[-<span class="number">1</span>] == <span class="variable language_">self</span>.dim, <span class="string">f'expected dimension of <span class="subst">{self.dim}</span> but received <span class="subst">{x.shape[-<span class="number">1</span>]}</span>'</span></span><br><span class="line"></span><br><span class="line">    x = <span class="variable language_">self</span>.project_in(x)</span><br></pre></td></tr></table></figure>
<p>  这里我们插播下project_in 的定义</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cosine_sim_project_in:</span><br><span class="line">    cosine_sim_project_in = default(cosine_sim_project_in_scale, codebook_scale)</span><br><span class="line">    project_in_klass = partial(CosineSimLinear, scale = cosine_sim_project_in)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    project_in_klass = partial(nn.Linear, bias = projection_has_bias)</span><br><span class="line"></span><br><span class="line"><span class="variable language_">self</span>.project_in = project_in_klass(dim, codebook_dims) <span class="keyword">if</span> has_projections <span class="keyword">else</span> nn.Identity()</span><br></pre></td></tr></table></figure>
<p>  以及dim和codebook_dims的定义</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">codebook_dims = codebook_dim * num_codebooks</span><br><span class="line">dim = default(dim, codebook_dims)</span><br></pre></td></tr></table></figure>
<p>  从上述代码块可见,仅仅只是一个线性变换,从<code>dim</code>到<code>codebook_dim</code>. 且默认是<code>has_projections = default(has_projections, dim != codebook_dims)</code>, 那在默认传入<code>has_projections = None</code>下, 会返回<code>dim != codebook_dims</code>, 那同样的,在默认传入<code>dim = None</code>下, 会返回<code>false</code>.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># maybe soft clamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> exists(<span class="variable language_">self</span>.soft_clamp_input_value):</span><br><span class="line">    clamp_value = <span class="variable language_">self</span>.soft_clamp_input_value</span><br><span class="line">    x = (x / clamp_value).tanh() * clamp_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># split out number of codebooks</span></span><br><span class="line"></span><br><span class="line">x = rearrange(x, <span class="string">'b n (c d) -&gt; b n c d'</span>, c = <span class="variable language_">self</span>.num_codebooks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># maybe l2norm</span></span><br><span class="line"></span><br><span class="line">x = <span class="variable language_">self</span>.maybe_l2norm(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to force quantization step to be full precision or not</span></span><br><span class="line"></span><br><span class="line">force_f32 = <span class="variable language_">self</span>.force_quantization_f32</span><br><span class="line"></span><br><span class="line">quantization_context = partial(autocast, <span class="string">'cuda'</span>, enabled = <span class="literal">False</span>) <span class="keyword">if</span> force_f32 <span class="keyword">else</span> nullcontext</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> quantization_context():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> force_f32:</span><br><span class="line">        orig_dtype = x.dtype</span><br><span class="line">        x = x.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># quantize by eq 3.</span></span><br><span class="line"></span><br><span class="line">    original_input = x</span><br><span class="line"></span><br><span class="line">    codebook_value = torch.ones_like(x) * <span class="variable language_">self</span>.codebook_scale</span><br><span class="line">    quantized = torch.where(x &gt; <span class="number">0</span>, codebook_value, -codebook_value)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate indices</span></span><br><span class="line"></span><br><span class="line">    indices = reduce((quantized &gt; <span class="number">0</span>).<span class="built_in">int</span>() * <span class="variable language_">self</span>.mask.<span class="built_in">int</span>(), <span class="string">'b n c d -&gt; b n c'</span>, <span class="string">'sum'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># maybe l2norm</span></span><br><span class="line"></span><br><span class="line">    quantized = <span class="variable language_">self</span>.maybe_l2norm(quantized)</span><br></pre></td></tr></table></figure>
<p>  以上是quantize部分, 那么自然来了个疑问: 原始输入的<code>x</code>是什么shape的?<br>  进入到本代码块时,<code>x</code>无疑最后一维是<code>codebook_dims</code>, 就是说<code>codebook_dims=(c d)</code>, 根据之前的定义,确实就没问题了.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use straight-through gradients (optionally with custom activation fn) if training</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">    x = <span class="variable language_">self</span>.activation(x)</span><br><span class="line">    x = x + (quantized - x).detach()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x = quantized</span><br><span class="line"></span><br><span class="line"><span class="comment"># entropy aux loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> force_f32:</span><br><span class="line">        codebook = <span class="variable language_">self</span>.codebook.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    codebook = <span class="variable language_">self</span>.maybe_l2norm(codebook)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the same as euclidean distance up to a constant</span></span><br><span class="line">    distance = -<span class="number">2</span> * einsum(<span class="string">'... i d, j d -&gt; ... i j'</span>, original_input, codebook)</span><br><span class="line"></span><br><span class="line">    prob = (-distance * inv_temperature).softmax(dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># account for mask</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exists(mask):</span><br><span class="line">        prob = prob[mask]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob = rearrange(prob, <span class="string">'b n ... -&gt; (b n) ...'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># whether to only use a fraction of probs, for reducing memory</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.frac_per_sample_entropy &lt; <span class="number">1.</span>:</span><br><span class="line">        num_tokens = prob.shape[<span class="number">0</span>]</span><br><span class="line">        num_sampled_tokens = <span class="built_in">int</span>(num_tokens * <span class="variable language_">self</span>.frac_per_sample_entropy)</span><br><span class="line">        rand_mask = torch.randn(num_tokens).argsort(dim = -<span class="number">1</span>) &lt; num_sampled_tokens</span><br><span class="line">        per_sample_probs = prob[rand_mask]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        per_sample_probs = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate per sample entropy</span></span><br><span class="line"></span><br><span class="line">    per_sample_entropy = entropy(per_sample_probs).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distribution over all available tokens in the batch</span></span><br><span class="line"></span><br><span class="line">    avg_prob = reduce(per_sample_probs, <span class="string">'... c d -&gt; c d'</span>, <span class="string">'mean'</span>)</span><br><span class="line"></span><br><span class="line">    avg_prob = maybe_distributed_mean(avg_prob)</span><br><span class="line"></span><br><span class="line">    codebook_entropy = entropy(avg_prob).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. entropy will be nudged to be low for each code, to encourage the network to output confident predictions</span></span><br><span class="line">    <span class="comment"># 2. codebook entropy will be nudged to be high, to encourage all codes to be uniformly used within the batch</span></span><br><span class="line"></span><br><span class="line">    entropy_aux_loss = per_sample_entropy - <span class="variable language_">self</span>.diversity_gamma * codebook_entropy</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># if not training, just return dummy 0</span></span><br><span class="line">    entropy_aux_loss = per_sample_entropy = codebook_entropy = <span class="variable language_">self</span>.zero</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to make the entropy loss positive or not through a (shifted) softplus</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.experimental_softplus_entropy_loss:</span><br><span class="line">    entropy_aux_loss = F.softplus(entropy_aux_loss + <span class="variable language_">self</span>.entropy_loss_offset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit loss</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.commitment_loss_weight &gt; <span class="number">0.</span>:</span><br><span class="line"></span><br><span class="line">    commit_loss = F.mse_loss(original_input, quantized.detach(), reduction = <span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exists(mask):</span><br><span class="line">        commit_loss = commit_loss[mask]</span><br><span class="line"></span><br><span class="line">    commit_loss = commit_loss.mean()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    commit_loss = <span class="variable language_">self</span>.zero</span><br><span class="line"></span><br><span class="line"><span class="comment"># input back to original dtype if needed</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> force_f32:</span><br><span class="line">    x = x.<span class="built_in">type</span>(orig_dtype)</span><br></pre></td></tr></table></figure>
<p>  仔细一看上述计算loss确实完完全照着文章里来的</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># merge back codebook dim</span></span><br><span class="line"></span><br><span class="line">x = rearrange(x, <span class="string">'b n c d -&gt; b n (c d)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># project out to feature dimension if needed</span></span><br><span class="line"></span><br><span class="line">x = <span class="variable language_">self</span>.project_out(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># reconstitute image or video dimensions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> should_transpose:</span><br><span class="line">    x = unpack_one(x, ps, <span class="string">'b * d'</span>)</span><br><span class="line">    x = rearrange(x, <span class="string">'b ... d -&gt; b d ...'</span>)</span><br><span class="line"></span><br><span class="line">    indices = unpack_one(indices, ps, <span class="string">'b * c'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># whether to remove single codebook dim</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.keep_num_codebooks_dim:</span><br><span class="line">    indices = rearrange(indices, <span class="string">'... 1 -&gt; ...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># complete aux loss</span></span><br><span class="line"></span><br><span class="line">aux_loss = entropy_aux_loss * <span class="variable language_">self</span>.entropy_loss_weight + commit_loss * <span class="variable language_">self</span>.commitment_loss_weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># returns</span></span><br><span class="line"></span><br><span class="line">ret = Return(x, indices, aux_loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> return_loss_breakdown:</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> ret, LossBreakdown(per_sample_entropy, codebook_entropy, commit_loss)</span><br></pre></td></tr></table></figure>
<p>  最后一块也没什么可说的,和其他的VQ也一样</p>
<p>  至此, 我们可以回答了: token维度就是encoder出来的维度, 不过我们用了 c(codebook nums)*d(codebook dim) 个去quantify它. 而且出乎意料, token里的元素确实是由-1,1组成的,和之前的<code>learnable_codebook = false</code>有异曲同工之处.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/23/LFQ%E6%8E%A2%E7%A9%B6/" data-id="cm5ar5vhf0003ciwi0qrc0zjt" data-title="LFQ探究" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basical-network/" rel="tag">basical-network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm-securaty/" rel="tag">llm-securaty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/object-detection/" rel="tag">object-detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-supervised/" rel="tag">self-supervised</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag">多模态</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/basical-network/" style="font-size: 10px;">basical-network</a> <a href="/tags/llm/" style="font-size: 20px;">llm</a> <a href="/tags/llm-securaty/" style="font-size: 10px;">llm-securaty</a> <a href="/tags/object-detection/" style="font-size: 15px;">object-detection</a> <a href="/tags/self-supervised/" style="font-size: 10px;">self-supervised</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 15px;">多模态</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/04/torch%E4%B8%AD%E7%9A%84forward-hook/">torch中的forward hook</a>
          </li>
        
          <li>
            <a href="/2025/01/01/ObjectDetectionRecent20Years/">ObjectDetectionRecent20Years</a>
          </li>
        
          <li>
            <a href="/2024/12/30/playingDINOv2/">playingDINOv2</a>
          </li>
        
          <li>
            <a href="/2024/12/30/CNN-surveys/">CNN-surveys</a>
          </li>
        
          <li>
            <a href="/2024/12/10/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%92%8Cllm/">小样本和llm</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>